cell_id,cell_type,classes,code,comments,display_data_keys,error_names,error_values,execute_result_keys,execution_count,file,functions,headings,imports,lines_of_code,links,markdown,num_classes,num_comments,num_display_data,num_error,num_execute_result,num_functions,num_imports,num_stream,num_words,parsed_ast
0,code,[],"# create folder to store data

!mkdir midi-data",['create folder to store data'],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,0,0,True
1,code,[],%cd midi_data,[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
2,code,[],"# upload data to google colab



from google.colab import files



files.upload()",['upload data to google colab'],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],5,[],[],0,1,0,0,0,0,0.0,0,0,True
3,code,[],%cd ..,[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
4,code,[],!ls,[],[],[],[],[],1.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
5,code,[],"from music21 import converter, instrument, note, chord, stream

from glob import glob",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
6,code,[],len(glob('midi_data/*.mid')),[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
7,code,[],"# define a method to get notes' list

def get_notes(path='midi-data/*.mid'):

  

  note_list = []

  midi_song_file_list = glob(path)

  for file in midi_song_file_list[:40]: # 40 used bcz of memory limitis :/ 

    # converting .mid file to stream object

    midi = converter.parse(file)

    notes_to_parse = None

    

    # Given a single stream, partition into a part for each unique instrument

    parts = instrument.partitionByInstrument(midi)

    

    if parts:  # if parts has instrument parts 

      notes_to_parse = parts.parts[0].recurse()

    else:      # file has notes in a flat structure

      notes_to_parse = midi.flat.notes

      

    for element in notes_to_parse:

      if isinstance(element, note.Note):

        # if element is a note, extract pitch

        note_list.append(str(element.pitch))

      elif isinstance(element, chord.Chord):

        # if element is a chord, append the normal form of the 

        # chord (a list of integers) to the list of notes.

        note_list.append('.'.join([str(n) for n in element.normalOrder]))

        

  

  return note_list","[""define a method to get notes' list"", '40 used bcz of memory limitis :/', 'converting .mid file to stream object', 'Given a single stream, partition into a part for each unique instrument', 'if parts has instrument parts', 'file has notes in a flat structure', 'if element is a note, extract pitch', 'if element is a chord, append the normal form of the', 'chord (a list of integers) to the list of notes.']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],29,[],[],0,9,0,0,0,0,0.0,0,0,True
8,code,[],"# Defining method to make mini-batches for training

def get_batches(x, y, batch_size, train, val_frac=0.1):

  # create training and validation data

  # x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)

  val_start_indx = int(x.shape[0] * (1 - val_frac))

  x_train, y_train, x_test, y_test = x[:val_start_indx, :, : ], y[:val_start_indx, :], x[val_start_indx:, :, : ], y[val_start_indx:, :]

  

  # Reshape into batch_size rows

  if train:

    # x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)

    # total number of batches we can make

    n_batches = x_train.shape[0] // batch_size

    # Keep only enough characters to make full batches

    x = x_train[:n_batches * batch_size, :, :]

    y = y_train[:n_batches * batch_size, :]

    

    for batch in range(n_batches):

      x_ = x[batch * batch_size : batch_size + batch * batch_size, :, :]

      y_ = y[batch * batch_size : batch_size + batch * batch_size, :]

      yield x_, y_

  

  else:

    # x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)

    # total number of batches we can make

    n_batches = x_test.shape[0] // batch_size

    # Keep only enough characters to make full batches

    x = x_test[:n_batches * batch_size, :, :]

    y = y_test[:n_batches * batch_size, :]

    

    for batch in range(n_batches):

      x_ = x[batch * batch_size : batch_size + batch * batch_size, :, :]

      y_ = y[batch * batch_size : batch_size + batch * batch_size, :]

      yield x_, y_","['Defining method to make mini-batches for training', 'create training and validation data', 'x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)', 'Reshape into batch_size rows', 'x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)', 'total number of batches we can make', 'Keep only enough characters to make full batches', 'x and y must have the same first dim (x.shape[0] == y.shape[0] => True!)', 'total number of batches we can make', 'Keep only enough characters to make full batches']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],33,[],[],0,10,0,0,0,0,0.0,0,0,True
9,code,[],"from keras.utils import to_categorical

import numpy as np



def data_loader(data, batch_size, sequence_length=100, train=True, val_frac=0.1):

  # Extract the unique pitches in the list of notes

  unique_pitches = sorted(set(data))

  

  # Create a dictionary to map pitches to integers

  note_to_int = {note:i for i, note in enumerate(unique_pitches)}

  

  network_input = []

  network_output = []

  

  # create input sequences and the corresponding outputs

  for i in range(0, len(data) - sequence_length, 1):

    sequence_in = data[i : i + sequence_length]   # I want to make network to learn predict next note looking to last 100 ones

    sequence_out = data[i + sequence_length]      # here sequence_out will be a note (str)

    network_input.append([note_to_int[note] for note in sequence_in])

    network_output.append(note_to_int[sequence_out])   # here sequence_out will be a note (str)

    

  rows = len(network_input)

  # reshape the input / output into a format compatible with LSTM layers 

  network_input = np.reshape(network_input, (rows, sequence_length))

  network_output = np.reshape(network_output, (rows, 1))

  # one hot encode input

  network_input = to_categorical(network_input) # the func will count classes and add 'one hot vectors' to the last dim

  

  return get_batches(network_input, network_output, batch_size, train, val_frac=val_frac)","['Extract the unique pitches in the list of notes', 'Create a dictionary to map pitches to integers', 'create input sequences and the corresponding outputs', 'I want to make network to learn predict next note looking to last 100 ones', 'here sequence_out will be a note (str)', 'here sequence_out will be a note (str)', 'reshape the input / output into a format compatible with LSTM layers', 'one hot encode input', ""the func will count classes and add 'one hot vectors' to the last dim""]",[],[],[],[],5.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],28,[],[],0,9,0,0,0,0,0.0,1,0,True
10,markdown,[],,[],[],[],[],[],,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],"[[2, 'Define the LSTM model']]",[],0,[],['## Define the LSTM model'],0,0,0,0,0,0,,0,5,False
11,code,[],"import torch

from torch import nn

from torch.autograd import Variable

import torch.nn.functional as F",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
12,code,[],"# Check if GPU is available

train_on_gpu = torch.cuda.is_available()

if(train_on_gpu):

    print('Training on GPU!')

else: 

    print('No GPU available, training on CPU; consider making n_epochs very small.')",['Check if GPU is available'],[],[],[],[],7.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],6,[],[],0,1,0,0,0,0,0.0,1,0,True
13,code,[],"# Declaring the model



class Note_RNN(nn.Module):

  def __init__(self, input_dim, hidden_dim=128, n_layer=1, drop_prob=0.3):

    super().__init__()

    

    self.input_dim = input_dim

    self.hidden_dim = hidden_dim

    self.layer_dim = n_layer

    

    # define the LSTM

    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.layer_dim, batch_first=True)

    

    # define a dropout layer

    self.dropout = nn.Dropout(drop_prob)

    

    # fully-connected output layers

    self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dim)

    

    # define the final, fully-connected output layer

    self.fc2 = nn.Linear(self.hidden_dim, self.input_dim)

    

  def forward(self, x, hidden_state_tuple):

    # one time step

    out, hidden_state_tuple = self.lstm(x, hidden_state_tuple)

    

    # Stack up LSTM outputs using view

    # now out size is -> (butch_size, time_steps (sequence_length), hidden_dim)

    # so, let's resize out to -> (butch_size, last_time_step, hidden_dim) to make prediction after 100 steps

    out = out[:, -1, :]

    

    # pass through a dropout layer

    out = self.dropout(out)

    

    # put x through the fully-connected layers

    out = self.fc1(out)

    

    # pass through a dropout layer

    out = self.dropout(out)

    

    out = self.fc2(out)

    

    # return the final output and the hidden state

    return out, hidden_state_tuple

  

  def init_hidden(self, batch_size):

    ''' 

    Initializes hidden state 

    '''

    # Create two new tensors with sizes (n_layers x batch_size x n_hidden),

    # initialized to zero, for hidden state and cell state of LSTM

    # shape - (layer_dim, batch_dim, hidden_dim)

    weight = next(self.parameters()).data # next used bcz the second item in param list are params for hidden states

    if train_on_gpu:

      h0 = weight.new(self.layer_dim, batch_size, self.hidden_dim).zero_().cuda()

      c0 = weight.new(self.layer_dim, batch_size, self.hidden_dim).zero_().cuda()

    else:

      h0 = weight.new(self.layer_dim, batch_size, self.hidden_dim).zero_()

      c0 = weight.new(self.layer_dim, batch_size, self.hidden_dim).zero_()

            

    return (h0, c0)","['Declaring the model', 'define the LSTM', 'define a dropout layer', 'fully-connected output layers', 'define the final, fully-connected output layer', 'one time step', 'Stack up LSTM outputs using view', 'now out size is -> (butch_size, time_steps (sequence_length), hidden_dim)', ""so, let's resize out to -> (butch_size, last_time_step, hidden_dim) to make prediction after 100 steps"", 'pass through a dropout layer', 'put x through the fully-connected layers', 'pass through a dropout layer', 'return the final output and the hidden state', 'Create two new tensors with sizes (n_layers x batch_size x n_hidden),', 'initialized to zero, for hidden state and cell state of LSTM', 'shape - (layer_dim, batch_dim, hidden_dim)', 'next used bcz the second item in param list are params for hidden states']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],61,[],[],0,17,0,0,0,0,0.0,0,0,True
14,markdown,[],,[],[],[],[],[],,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],"[[2, 'Create Method for Training']]",[],0,[],['## Create Method for Training'],0,0,0,0,0,0,,0,5,False
15,code,[],"# Declaring the train method

def train(net, data, epochs=10, batch_size=64, sequence_length=100, val_frac=0.1, clip=10, print_every=100):

  net.train()

  

  optimizer = torch.optim.Adam(net.parameters())

  criterion = nn.CrossEntropyLoss()

  

  if train_on_gpu:

    net.cuda()

  

  counter = 0

  unique_chars = net.input_dim

  for e in range(epochs):

    # initialize hidden state

    hidden_state_tuple = net.init_hidden(batch_size)

    

    for x, y in data_loader(data, batch_size, sequence_length=sequence_length, train=True, val_frac=val_frac):

        counter += 1

        

        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

        if train_on_gpu:

          inputs, targets = inputs.cuda(), targets.cuda()

          

        # make zero accumulated gradients

        optimizer.zero_grad()

        

        # make grads of hidden state zero between batches, but keep thier values

        # 'tensor.data' return a new tensor that shares storage with tensor, but it always has requires_grad=False

        hidden_state_tuple = tuple(Variable(hidden.data, requires_grad=True) for hidden in hidden_state_tuple)

        

        # get the output from the model

        output, hidden_state_tuple = net(inputs, hidden_state_tuple)

        

        # calculate the loss and perform backprop

        loss = criterion(output, targets.squeeze())

        loss.backward()

        

        # ""clip_grad_norm"" helps prevent the exploding gradient problem in RNNs / LSTMs

        nn.utils.clip_grad_norm_(net.parameters(), clip)

        optimizer.step()

        

        # print loss stats

        if counter % print_every == 0:

          net.eval()

          with torch.set_grad_enabled(False):

            

            # Get validation loss

            val_hidden_state_tuple = net.init_hidden(batch_size)

            val_losses = []

            

            for x_test, y_test in data_loader(data, batch_size, sequence_length=sequence_length, train=False, val_frac=val_frac):

              inputs, targets = torch.from_numpy(x_test), torch.from_numpy(y_test)



              if train_on_gpu:

                inputs, targets = inputs.cuda(), targets.cuda()

              

              # get the output from the model

              output, val_hidden_state_tuple = net(inputs, val_hidden_state_tuple)

              val_loss = criterion(output, targets.squeeze())

              val_losses.append(val_loss.item())

              

            print(""Epoch: {}/{}..."".format(e+1, epochs),

                  ""Step: {}..."".format(counter),

                  ""Loss: {:.4f}..."".format(loss.item()),

                  ""Val Loss: {:.4f}"".format(np.mean(val_losses)))

            

          net.train()   # reset to train mode after iterationg through validation data","['Declaring the train method', 'initialize hidden state', 'make zero accumulated gradients', 'make grads of hidden state zero between batches, but keep thier values', ""'tensor.data' return a new tensor that shares storage with tensor, but it always has requires_grad=False"", 'get the output from the model', 'calculate the loss and perform backprop', '""clip_grad_norm"" helps prevent the exploding gradient problem in RNNs / LSTMs', 'print loss stats', 'Get validation loss', 'get the output from the model', 'reset to train mode after iterationg through validation data']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],67,[],[],0,12,0,0,0,0,0.0,0,0,True
16,code,[],notes = get_notes(),[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
17,code,[],"unique_notes = set(notes)

unique_notes_num = len(unique_notes)

print('total list size ->', len(notes))

print(unique_notes_num)",[],[],[],[],[],11.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,1,0,True
18,code,[],"model = Note_RNN(unique_notes_num)

checkpoint = torch.load('Music_LSTM.pth.tar')

model.load_state_dict(checkpoint['state_dict'])",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
19,code,[],"# net = Note_RNN(unique_notes_num)

print(model)",['net = Note_RNN(unique_notes_num)'],[],[],[],[],13.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,1,0,True
20,code,[],"# Declaring the hyperparameters

batch_size = 32

seq_length = 100

n_epochs = 100",['Declaring the hyperparameters'],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],4,[],[],0,1,0,0,0,0,0.0,0,0,True
21,code,[],"# train the model

train(model, notes, epochs=n_epochs, batch_size=batch_size, sequence_length=seq_length)",['train the model'],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,0,0,True
22,code,[],"torch.save({'state_dict': model.state_dict()}, 'Music_LSTM.pth.tar')",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
23,code,[],"from google.colab import files

files.download('Music_LSTM.pth.tar') ",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
24,code,[],"# upload data to google colab



from google.colab import files



files.upload()",['upload data to google colab'],"['text/html', 'text/plain']",[],[],['text/plain'],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],5,[],[],0,1,1,0,1,0,0.0,1,0,True
25,markdown,[],,[],[],[],[],[],,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],"[[2, 'Now Generate Music']]",[],0,[],['## Now Generate Music'],0,0,0,0,0,0,,0,4,False
26,code,[],"def get_inputSequences(data, unique_pitches, sequence_length=100):

  """""" 

  Prepare the sequences used by the Neural Network

  """"""

  # Create a dictionary to map pitches to integers

  note_to_int = {note:i for i, note in enumerate(unique_pitches)}

  

  network_input = []

  # create input sequences and the corresponding outputs

  for i in range(0, len(data) - sequence_length, 1): # 50 less bcz memory limits

    sequence_in = data[i : i + sequence_length]   # I want to make network to learn predict next note looking to last 100 ones

    network_input.append([note_to_int[note] for note in sequence_in])

    

  rows = len(network_input)

  # reshape the input / output into a format compatible with LSTM layers 

  network_input = np.reshape(network_input, (rows, sequence_length))

  # one hot encode input

  network_input = to_categorical(network_input) # the func will count classes and add 'one hot vectors' to the last dim

  

  return network_input","['Create a dictionary to map pitches to integers', 'create input sequences and the corresponding outputs', '50 less bcz memory limits', 'I want to make network to learn predict next note looking to last 100 ones', 'reshape the input / output into a format compatible with LSTM layers', 'one hot encode input', ""the func will count classes and add 'one hot vectors' to the last dim""]",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],20,[],[],0,7,0,0,0,0,0.0,0,0,True
27,code,[],"def generate_notes(model, network_input, unique_pitches):

  """"""

  Generate notes from the neural network based on a sequence of notes 

  """"""

  model.eval()

  # Pick a random integer

  start = np.random.randint(0, network_input.shape[0]) # [low, high)

  

  int_to_note = {i:note for i, note in enumerate(unique_pitches)}

  # pick a random sequence from the input as a starting point for the prediction

  pattern = np.expand_dims(network_input[start, :, :], axis=0)  # add first, ""batch dim"", bcz it was removed by indexing above

    

  prediction_output = []

  

  print('Generating Notes...')  

  with torch.set_grad_enabled(False):

    

    inputs = torch.from_numpy(pattern)

    if train_on_gpu:

      model.cuda()

      inputs = inputs.cuda()

    # init hidden

    hidden_state_tuple = model.init_hidden(1) # 1 batch size

    # generate 500 notes

    for note_index in range(250):

      output, hidden_state_tuple = model(inputs, hidden_state_tuple)  

      _, index = torch.max(output, 1)

      # Mapping the predicted interger back to the corresponding note

      result = int_to_note[index.item()]

      # Storing the predicted output

      prediction_output.append(result)

      # move to the next inputs

      np_input = np.zeros((1, 1, inputs.shape[-1]))

      np_input[0, 0, index.item()] = 1

      input_tensor = torch.from_numpy(np_input)

      if train_on_gpu:

        input_tensor = input_tensor.cuda()

      inputs = torch.cat((inputs, input_tensor.float()), 1)

      inputs = inputs[:,  1:, :] 

    

  print('Notes Generated')

  return prediction_output    ","['Pick a random integer', '[low, high)', 'pick a random sequence from the input as a starting point for the prediction', 'add first, ""batch dim"", bcz it was removed by indexing above', 'init hidden', '1 batch size', 'generate 500 notes', 'Mapping the predicted interger back to the corresponding note', 'Storing the predicted output', 'move to the next inputs']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],42,[],[],0,10,0,0,0,0,0.0,0,0,True
28,code,[],"def create_midi(prediction_output):

  """"""

  convert the output from the prediction to notes and create a midi file from the notes

  """"""

  offset = 0

  output_notes = []

  

  # create note and chord objects based on the values generated by the model

  for pattern in prediction_output:

    

    if ('.' in pattern) or pattern.isdigit():

      notes_in_chord = pattern.split('.')

      notes = []

      for current_note in notes_in_chord:

        new_note = note.Note(int(current_note))

        new_note.storedInstrument = instrument.Piano()

        notes.append(new_note)

      new_chord = chord.Chord(notes)

      new_chord.offset = offset

      output_notes.append(new_chord)

    # pattern is a note

    else:

      new_note = note.Note(pattern)

      new_note.offset = offset

      output_notes.append(new_note)

    # increase offset each iteration so that notes do not stack

    offset += 0.5

    

  midi_stream = stream.Stream(output_notes)

  print('Saving Output file as midi...')

  midi_stream.write('midi', fp='test_output4.mid')

  print('Done!')","['create note and chord objects based on the values generated by the model', 'pattern is a note', 'increase offset each iteration so that notes do not stack']",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],32,[],[],0,3,0,0,0,0,0.0,0,0,True
29,code,[],"import pickle



def generate(note_list, unique_notes_num):

  """""" 

  Generate a piano midi file 

  """"""

  unique_pitches = sorted(set(note_list))

  unique_notes_num = len(unique_pitches)

  network_input = get_inputSequences(note_list, unique_pitches)

  

  print('Loading Model Weights...')

  model = Note_RNN(unique_notes_num)

  checkpoint = torch.load('Music_LSTM.pth.tar')

  model.load_state_dict(checkpoint['state_dict'])

  print('Model Loaded')

  prediction_output = generate_notes(model, network_input, unique_pitches)

  create_midi(prediction_output)",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],17,[],[],0,0,0,0,0,0,0.0,0,0,True
30,code,[],"#### Generate a new music 

generate(notes, unique_notes_num)",['Generate a new music'],[],[],[],[],21.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,1,0,True
31,code,[],"from google.colab import files

files.download('test_output4.mid') ",[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
32,code,[],"# # play generated song

# import play



# play.play_midi('test_output4.mid')","['play generated song', 'import play', ""play.play_midi('test_output4.mid')""]",[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],4,[],[],0,3,0,0,0,0,0.0,0,0,True
33,code,[],,[],[],[],[],[],0.0,Soso-Sukhitashvili..Pytorch_Homeworks..Generate_Music_LSTM.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
0,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],"[[1, '<h2><font color=""Crimson"">Importing all required packages</font></h2>']]",[],0,[],"['# <h2><font color=""Crimson"">Importing all required packages</font></h2>']",0,0,0,0,0,0,,0,6,False
1,code,[],"import matplotlib.pyplot as plt

import matplotlib.image as mpimg

import math

import csv

import numpy as np

from sklearn.decomposition import PCA

from sklearn.neighbors import KNeighborsClassifier

import scipy.special

import cv2 as cv

import os

import random",[],[],[],[],[],1.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],11,[],[],0,0,0,0,0,0,0.0,0,0,True
2,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['<h2> <font color=""Chocolate ""> Setting up the Environment Variables and Reading the image files </font></h2>']",0,0,0,0,0,0,,0,15,False
3,code,[],"IMG_FOLDER='.'



band1_img=mpimg.imread(os.path.join(IMG_FOLDER,""band1.gif""))

band2_img=mpimg.imread(os.path.join(IMG_FOLDER,""band2.gif""))

band3_img=mpimg.imread(os.path.join(IMG_FOLDER,""band3.gif""))

band4_img=mpimg.imread(os.path.join(IMG_FOLDER,""band4.gif""))



band1_img_norm=band1_img/255

band2_img_norm=band2_img/255

band3_img_norm=band3_img/255

band4_img_norm=band4_img/255





band1_img_norm_avg=np.mean(band1_img_norm,axis=2)

band2_img_norm_avg=np.mean(band2_img_norm,axis=2)

band3_img_norm_avg=np.mean(band3_img_norm,axis=2)

band4_img_norm_avg=np.mean(band4_img_norm,axis=2)





plt.figure(dpi=256)

plt.axis(""off"")

plt.imshow(band4_img_norm_avg)

plt.show()



#band1_img_avg=np.



plt.figure(dpi=256)

plt.subplot(241), plt.axis(""off""), plt.imshow(band1_img_norm,plt.cm.gray)

plt.subplot(242), plt.axis(""off""), plt.imshow(band2_img_norm,plt.cm.gray)

plt.subplot(243), plt.axis(""off""), plt.imshow(band3_img_norm,plt.cm.gray)

plt.subplot(244), plt.axis(""off""), plt.imshow(band4_img_norm,plt.cm.gray)





plt.subplot(245), plt.axis(""off""), plt.imshow(band1_img_norm_avg,plt.cm.gray)

plt.subplot(246), plt.axis(""off""), plt.imshow(band2_img_norm_avg,plt.cm.gray)

plt.subplot(247), plt.axis(""off""), plt.imshow(band3_img_norm_avg,plt.cm.gray)

plt.subplot(248), plt.axis(""off""), plt.imshow(band4_img_norm_avg,plt.cm.gray)





plt.show()",['band1_img_avg=np.'],"['image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],3.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],40,[],[],0,1,2,0,0,0,0.0,0,0,True
4,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['\n', '<h2> <font color=Tomato > Reading river position coordinates from txt file </font></h2>']",0,0,0,0,0,0,,0,12,False
5,code,[],"river_pos=list()

with open('river_position.txt',""r"") as river_file:

    for i in river_file:

        river_pos.append(str.split(i))

        ",[],[],[],[],[],5.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
6,code,[],"river_pos_x=[int(i[0]) for i in river_pos]

river_pos_y=[int(i[1]) for i in river_pos]",[],[],[],[],[],6.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
7,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['<h2> <font color=""SandyBrown ""> Reading non-river position coordinates from txt file </font></h2>']",0,0,0,0,0,0,,0,12,False
8,code,[],"non_river_pos=list()

with open('nonriver_position.txt',""r"") as nonriver_file:

    for i in nonriver_file:

        non_river_pos.append(str.split(i))",[],[],[],[],[],8.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
9,code,[],"non_river_pos_x=[int(i[0]) for i in non_river_pos]

non_river_pos_y=[int(i[1]) for i in non_river_pos]",[],[],[],[],[],9.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
10,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],['<h2><font color=Orchid > Displaying input images</font></h2>'],0,0,0,0,0,0,,0,6,False
11,code,[],"plt.figure(dpi=256)

#plt.imshow(band4_img_norm_avg)

plt.axis(""off"")

plt.title(""Points which are taken as training data points"")

plt.scatter(non_river_pos_y,non_river_pos_x,marker=""p"", color=""red"", s=2, label=""non-river"")

plt.scatter(river_pos_y,river_pos_x,marker=""o"", color=""blue"", s=2, label=""river"")

plt.legend()

plt.imshow(band4_img_norm_avg, plt.cm.gray)

plt.title(""Points which are taken as training data points"")

plt.show()",['plt.imshow(band4_img_norm_avg)'],[],[],[],[],33.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],10,[],[],0,1,0,0,0,0,0.0,0,0,True
12,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],['<h2> <font color=PeachPuff > Training Data set is created </font></h2>'],0,0,0,0,0,0,,0,10,False
13,code,[],"training_dataset=list()

for i in range(len(river_pos)):

    training_dataset.append([band1_img_norm_avg[river_pos_x[i],river_pos_y[i]],\

                             band2_img_norm_avg[river_pos_x[i],river_pos_y[i]],\

                             band3_img_norm_avg[river_pos_x[i],river_pos_y[i]],\

                             band4_img_norm_avg[river_pos_x[i],river_pos_y[i]],1,1])

    

for i in range(len(non_river_pos)):

    training_dataset.append([band1_img_norm_avg[non_river_pos_x[i],non_river_pos_y[i]],\

                             band2_img_norm_avg[non_river_pos_x[i],non_river_pos_y[i]],\

                             band3_img_norm_avg[non_river_pos_x[i],non_river_pos_y[i]],\

                             band4_img_norm_avg[non_river_pos_x[i],non_river_pos_y[i]],1,0])

    

    ",[],[],[],[],[],11.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],14,[],[],0,0,0,0,0,0,0.0,0,0,True
14,code,[],"training_dataset=np.array(training_dataset,dtype=np.float64)

np.random.shuffle(training_dataset)",[],[],[],[],[],12.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
15,code,[],training_dataset.shape,[],[],[],[],['text/plain'],28.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
16,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],"[[1, '<font color=Chocolate>Reading the training labels</font>']]",[],0,[],['# <font color=Chocolate>Reading the training labels</font>'],0,0,0,0,0,0,,0,6,False
17,code,[],"train_features=list()

train_label=list()

for i in range(len(training_dataset)):

    train_features.append(training_dataset[i][0:training_dataset.shape[1]-1])

    train_label.append(training_dataset[i][training_dataset.shape[1]-1])



train_features=np.array(train_features)",[],[],[],[],[],14.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],7,[],[],0,0,0,0,0,0,0.0,0,0,True
18,code,[],train_features[0:10],[],[],[],[],['text/plain'],15.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
19,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['<font color=""BluePurple"">Principle Component analysis</font>']",0,0,0,0,0,0,,0,4,False
20,code,[],"#######Comment whole section if PCA is not required

#train_features_pca=np.array(train_features[:,0:train_features.shape[1]-1])

#pca = PCA(n_components=2)

#train_features_pca=pca.fit_transform(X = train_features_pca)



#bias_ones=np.ones((len(train_features_pca),1),dtype=np.float64)

#train_features=np.append(train_features_pca,bias_ones, axis=1)","['Comment whole section if PCA is not required', 'train_features_pca=np.array(train_features[:,0:train_features.shape[1]-1])', 'pca = PCA(n_components=2)', 'train_features_pca=pca.fit_transform(X = train_features_pca)', 'bias_ones=np.ones((len(train_features_pca),1),dtype=np.float64)', 'train_features=np.append(train_features_pca,bias_ones, axis=1)']",[],[],[],[],16.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],7,[],[],0,6,0,0,0,0,0.0,0,0,True
21,code,[],"#######Comment whole section if PCA is not required

#plt.scatter(train_features[:,0],train_features[:,1],c=train_label)

#plt.xlabel(""PC1"")

#plt.ylabel(""PC2"")

#plt.show()","['Comment whole section if PCA is not required', 'plt.scatter(train_features[:,0],train_features[:,1],c=train_label)', 'plt.xlabel(""PC1"")', 'plt.ylabel(""PC2"")', 'plt.show()']",[],[],[],[],17.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],5,[],[],0,5,0,0,0,0,0.0,0,0,True
22,code,[],#train_features[0:10],['train_features[0:10]'],[],[],[],[],18.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,1,0,0,0,0,0.0,0,0,True
23,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],"[[1, 'Setting the hyper parameters']]",[],0,[],['# Setting the hyper parameters'],0,0,0,0,0,0,,0,5,False
24,code,[],"input_no_of_nodes=len(train_features[0])

hidden_no_of_nodes=10

output_no_of_nodes=2

no_of_interation=10000

learning_rate=0.05",[],[],[],[],[],19.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
25,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['<font color=""brown""> Setting initial matrix </font>']",0,0,0,0,0,0,,0,6,False
26,code,[],"#weight_matrix_input_to_hidden=np.random.normal(0,1,(input_no_of_nodes,hidden_no_of_nodes))

weight_matrix_input_to_hidden=np.random.rand(input_no_of_nodes,hidden_no_of_nodes)

hidden_induced_value=np.zeros(hidden_no_of_nodes,dtype=np.float64)

hidden_sigmoid_value=np.zeros(hidden_no_of_nodes,dtype=np.float64)

print(weight_matrix_input_to_hidden)","['weight_matrix_input_to_hidden=np.random.normal(0,1,(input_no_of_nodes,hidden_no_of_nodes))']",[],[],[],[],20.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],5,[],[],0,1,0,0,0,0,0.0,1,0,True
27,code,[],"weight_matrix_hidden_to_output=np.random.normal(0,1,(hidden_no_of_nodes,output_no_of_nodes))

output_induced_value=dtype=np.zeros(output_no_of_nodes,np.float64)

output_sigmoid_value=np.zeros(output_no_of_nodes,dtype=np.float64)

print(weight_matrix_hidden_to_output)",[],[],[],[],[],21.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,1,0,True
28,code,[],"#total_error=list()

for iter in range(no_of_interation):

#    error_in_epoch=0

    for i in range(len(train_features)):

        

        error_for_an_instance=np.zeros(output_no_of_nodes,dtype=np.float64)

        

        #Sum value in hidden layer

        hidden_induced_value=np.matmul(weight_matrix_input_to_hidden.transpose(),train_features[i])

        

        #Applying the sigmoid function to induced value

        hidden_sigmoid_value=scipy.special.expit(hidden_induced_value)

        

        #Sigmoid derivative of hidden layer, will be used while back propagation

        hidden_sigmoid_derivative=hidden_sigmoid_value*(1-hidden_sigmoid_value)

        

        #Output induced value 

        output_induced_value=np.matmul(weight_matrix_hidden_to_output.transpose(),hidden_sigmoid_value)

        

        #Output Sigmoid

        output_sigmoid_value=scipy.special.expit(output_induced_value)

        

        #Output Sigmoid Derivative

        output_sigmoid_derivative=output_sigmoid_value*(1-output_sigmoid_value)

        

        # One Hot Encoding is done here

        actual_output=np.zeros(output_no_of_nodes)

        actual_output[int(train_label[i])]=1

        

        

        if i < 2 :

            print(""\n\nInstance Serial:"",i)

            print(""Hidden Sigmoid Value in vector form:\n"",hidden_sigmoid_value)

            print(""Hidden Sigmoid Derivative:\n"",hidden_sigmoid_derivative)

            print(""Output Sigmoid Derivative:\n"",output_sigmoid_derivative)

            print(""**Output Predicted Value:\n"",output_sigmoid_value)

            print(""**Actual output:"",actual_output)

            print(""Class Label:"",train_label[i])



            entropy_in_output=0

            for p in  output_sigmoid_value:

                entropy_in_output=entropy_in_output+(-p*math.log2(p))



            print(""**Entropy Value of output sigmoid values:"",entropy_in_output)

        

        #

        

        #back propagation starts here

        error_for_an_instance=actual_output - output_sigmoid_value

        #print(""Error or difference:"",error_for_an_instance)

        

        #

        temp_weight_matrix_hidden_to_output=weight_matrix_hidden_to_output



        delta_output=error_for_an_instance*output_sigmoid_derivative

        

        #print(delta_output)



        for j in range(output_no_of_nodes):

            for k in range(hidden_no_of_nodes):

                temp_weight_matrix_hidden_to_output[k,j]=\

                weight_matrix_hidden_to_output[k,j] + learning_rate*delta_output[j]*\

                                                    hidden_sigmoid_value[k]

        delta_hidden=np.zeros(hidden_no_of_nodes,dtype=np.float64)



        for k in range(hidden_no_of_nodes):

            for j in range(output_no_of_nodes):

                delta_hidden[k]=delta_hidden[k] + delta_output[j]*weight_matrix_hidden_to_output[k,j]*\

                                                hidden_sigmoid_derivative[k]

        

        

        for k in range(hidden_no_of_nodes):

            for l in range(input_no_of_nodes):

                weight_matrix_input_to_hidden[l,k]=weight_matrix_input_to_hidden[l,k] + learning_rate*\

                                                    delta_hidden[k]*train_features[i][l]



                #print(weight_matrix_input_to_hidden[l,k] , learning_rate, hidden_sigmoid_derivative[k],error_hidden[k],normalized_image[l], learning_rate*hidden_sigmoid_derivative[k]*\

                                                    #error_hidden[k]*normalized_image[l])



        weight_matrix_hidden_to_output=temp_weight_matrix_hidden_to_output

    #print(""\n\n***********Epoch >>"",iter,""is completed.\n\n\n"")

    print(""Epoch"",iter,""\n"")



    ","['total_error=list()', 'error_in_epoch=0', 'Sum value in hidden layer', 'Applying the sigmoid function to induced value', 'Sigmoid derivative of hidden layer, will be used while back propagation', 'Output induced value', 'Output Sigmoid', 'Output Sigmoid Derivative', 'One Hot Encoding is done here', '', 'back propagation starts here', 'print(""Error or difference:"",error_for_an_instance)', '', 'print(delta_output)', 'print(weight_matrix_input_to_hidden[l,k] , learning_rate, hidden_sigmoid_derivative[k],error_hidden[k],normalized_image[l], learning_rate*hidden_sigmoid_derivative[k]*\\', 'error_hidden[k]*normalized_image[l])', 'print(""\\n\\n***********Epoch >>"",iter,""is completed.\\n\\n\\n"")']",[],[],[],[],26.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],84,[],[],0,17,0,0,0,0,0.0,769,0,True
29,code,[],weight_matrix_input_to_hidden,[],[],[],[],['text/plain'],25.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
30,code,[],weight_matrix_hidden_to_output,[],[],[],[],['text/plain'],24.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
31,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],"[[1, '<font color=LightSalmon>Test image generation</font>']]",[],0,[],['# <font color=LightSalmon>Test image generation</font>'],0,0,0,0,0,0,,0,5,False
32,code,[],"test_output=np.zeros(band1_img_norm_avg.shape,dtype=np.uint8)



for i in range(band1_img_norm_avg.shape[0]):

    for j in range(band1_img_norm_avg.shape[1]):

        test_input=np.append(

                            #Uncomment below line for PCA

                            #pca.transform(\

                            np.array([band1_img_norm_avg[i,j],\

                            band2_img_norm_avg[i,j],\

                            band3_img_norm_avg[i,j],\

                            band4_img_norm_avg[i,j]])\

                            #Uncomment below line for PCA

                            #.reshape(1,-1)\

                            #Uncomment below line for PCA\ 

                            #)\

                             ,1)

        #print(test_input)

        hidden_induced_value=np.matmul(weight_matrix_input_to_hidden.transpose(),test_input)

        hidden_sigmoid_value=scipy.special.expit(hidden_induced_value)

       # hidden_sigmoid_derivative=hidden_sigmoid_value*(1-hidden_sigmoid_value)

        output_induced_value=np.matmul(weight_matrix_hidden_to_output.transpose(),hidden_sigmoid_value)

        output_sigmoid_value=scipy.special.expit(output_induced_value)

        index=np.argmax(output_sigmoid_value)

        if index ==1 :

            test_output[i,j]=255

        else:

            test_output[i,j]=0

            ","['Uncomment below line for PCA', 'pca.transform(\\', 'Uncomment below line for PCA', '.reshape(1,-1)\\', 'Uncomment below line for PCA\\', ')\\', 'print(test_input)', 'hidden_sigmoid_derivative=hidden_sigmoid_value*(1-hidden_sigmoid_value)']",[],[],[],[],29.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],28,[],[],0,8,0,0,0,0,0.0,0,0,True
33,code,[],"plt.figure(dpi=256)

plt.title(""Generated Test Image"")

plt.imshow(test_output,plt.cm.gray)

plt.axis(""off"")

plt.show()

pca_string=""PCA""

mpimg.imsave (""Segmented""+\

            #Uncomment below line it if PCA is required

           #""PCA"" +\

           "".png"",test_output,cmap=plt.cm.gray)","['Uncomment below line it if PCA is required', '""PCA"" +\\']","['image/png', 'text/plain']",[],[],[],25.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],10,[],[],0,2,1,0,0,0,0.0,0,0,True
34,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],"[[1, 'Further Processing can be done to reduce the error']]",[],0,[],['# Further Processing can be done to reduce the error'],0,0,0,0,0,0,,0,10,False
35,markdown,[],,[],[],[],[],[],,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],0,[],"['<h2><font color=""DarkMagenta"">Using same Training points, segmented image for k-NN</font></h2> ']",0,0,0,0,0,0,,0,9,False
36,code,[],"test_input=np.concatenate((\

                            band1_img_norm_avg.flatten().reshape(-1,1),\

                            band2_img_norm_avg.flatten().reshape(-1,1),\

                            band3_img_norm_avg.flatten().reshape(-1,1),\

                            band4_img_norm_avg.flatten().reshape(-1,1),\

                          ),axis=1)



#Uncomment below line if PCA is required 

#test_input=pca.transform(test_input) #Uncomment for PCA ","['Uncomment below line if PCA is required', 'test_input=pca.transform(test_input)  Uncomment for PCA']",[],[],[],[],26.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],9,[],[],0,2,0,0,0,0,0.0,0,0,True
37,code,[],test_input.shape,[],[],[],[],['text/plain'],27.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
38,code,[],"knn = KNeighborsClassifier(n_neighbors=7) 

# You can change and test for different values of neighbors

# Fit the model on the training data.

knn.fit(train_features[:,0:train_features.shape[1]-1],train_label )

# Make point predictions on the test set using the fit model.

predictions = knn.predict(test_input)","['You can change and test for different values of neighbors', 'Fit the model on the training data.', 'Make point predictions on the test set using the fit model.']",[],[],[],[],28.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],6,[],[],0,3,0,0,0,0,0.0,0,0,True
39,code,[],"test_output_knn=predictions.reshape(band1_img_norm_avg.shape)

plt.figure(dpi=256)

plt.imshow(test_output_knn,plt.cm.gray)

plt.axis(""off"")

plt.show()",[],"['image/png', 'text/plain']",[],[],[],29.0,Swaraj-Mohapatra..river_detection_neural..Neural_Network_Satellite_Image.ipynb,[],[],[],5,[],[],0,0,1,0,0,0,0.0,0,0,True
0,code,[],%reset,[],[],[],[],[],56.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
1,code,[],"#!/usr/bin/python

""""""

Copyright 2019 Accenture and/or its affiliates.  All Rights Reserved.  

You may not use, copy, modify, and/or distribute this code and/or its documentation without permission from Accenture.

Please contact the Advanced Analytics-Operations Analytics team and/or Frode Huse Gjendem (lead) with any questions.



\brief This is the starter script for the Accenture's Health Datathon 2019 competition.



\version 1.0



\date $Date: 2019/05



""""""



import numpy as np

import pandas as pd

import matplotlib.pyplot as plt







'''

Function to select certain/uncertain weights

You will not need to ever create the following parameters

They are created (and this is called) within the next function ""calc_weights""

@paramters

    patient_info: vector holding every month you want to evaluate, 1 for whichever column months_survived = 1, NA otherwise

    df_aux: vector that contains the probability of survival at each month, fixed values for a test set

@return

    the vector of weights



'''

def get_weights(patient_info, df_aux):

    x = np.array(patient_info)

    

    # weight will always be 1 if we know they are dead

    if (x[~np.isnan(x)][0]==1):

        x = np.ones_like(x)

    else:

        y = np.argwhere(~np.isnan(x))

        not_zero_index = y.ravel()[0]

        

        x[:not_zero_index] = 1

        x[not_zero_index:] = df_aux['prob'][not_zero_index:]

    return (x)





'''

Function to calculate the weight matrix

@paramters

    y_df: The test set for which you are calculating the weights

        Format: Index = ID, Rows = patients, cols = ['specific_death', 'months_survival']

@ return

    the weights matrix with the weights for each patient at each time t  

'''

def calc_weights(y_df):

    

    # Create a matrix with patient id in the index and months_survival as header, specific_death as values.   

    df_y_pivot = y_df.pivot(columns=""months_survival"", values='specific_death')

    # The table changes order of rows after pivoting it, we need to reorder it again by doing reindex_axis.

    df_y_pivot= df_y_pivot.reindex(y_df.index, axis=0).reset_index()

    

    # We need to calculate the weights based on the entire time initially, then cut it off after the fact

    all_months_survival = np.arange(0,y_df.months_survival.max()+1)

    months_complementary = np.setdiff1d(all_months_survival, y_df.months_survival.unique())

    df_complementary = pd.DataFrame(np.nan, index=df_y_pivot.index, columns=months_complementary )

    df_y_pivot = pd.concat([df_y_pivot,df_complementary],axis=1)[all_months_survival]

    

    # Get aux matrix to provide in the get_weights function. 

    # Probability of being alive at each month based on patients you are certain about (excluding patients censored pior to month)

    df_aux = pd.DataFrame(data=np.arange(0,y_df.months_survival.max()+1),columns=[""months_survival""])

    df_aux['prob_num'] = df_aux['months_survival'].apply(lambda x : (y_df['months_survival'] > x).values.sum())

    df_aux['prob_den'] = df_aux['months_survival'].apply(lambda x : ((y_df['months_survival'] < x) & (y_df['specific_death']==1)).values.sum())

    df_aux['prob'] = (df_aux['prob_num']/(df_aux['prob_num']+df_aux['prob_den']))



    df_aux = df_aux[['months_survival','prob']].sort_values('months_survival').reset_index().drop('index',axis=1)



    

    #Get weights

    df_weights = df_y_pivot.apply(lambda x: get_weights(x,df_aux),axis=1)

    

    new_weights = pd.DataFrame(np.vstack(df_weights),columns=all_months_survival )



    new_weights = np.apply_along_axis(np.cumprod, 1, new_weights)

    

    new_weights = pd.DataFrame(new_weights)

    

    return new_weights





''' 

Fill up the Y_true matrix's value 

You will not need to ever creat the following parameter.

It is created (and this is called) within the next function ""populate_actual"".

@paramters

    patient_info: vector holding every month you want to evaluate, 1 for whichever column months_survived = 1, NA otherwise

'''

def apply_non_zero(patient_info):

    x = np.array(patient_info)

    if (x[~np.isnan(x)][0]==0):

        x = np.ones_like(x)

    else:

        y = np.argwhere(~np.isnan(x))

        not_zero_index = y.ravel()[0]

        x[:(not_zero_index)] = 1

        x[(not_zero_index):] = 0   



    return (pd.Series(x))



 

'''

Build the Y_true matrix

@paramters

    y_df: The test set for which you are calculating the weights

        Format: Index = ID, Rows = patients, cols = ['specific_death', 'months_survival']

    years: Number of years for which you want to evaluate your model. Default = 10.

'''

def populate_actual(y_df, years = 10):

    #Create a matrix by pivoting table

    df_y_true_pivot = y_df.pivot(columns = ""months_survival"", values='specific_death')

    #The table changes order of rows after pivoting it, we need to reorder it again by doing reindex_axis.

    df_y_true_pivot = df_y_true_pivot.reindex(y_df.index, axis=0).reset_index()





    all_months_survival = np.arange(0,y_df.months_survival.max()+1)

    #Get the month that we don't have any patient record in our dataset.

    

    months_complementary = np.setdiff1d(all_months_survival, y_df.months_survival.unique())

    df_complementary = pd.DataFrame(np.nan, index=df_y_true_pivot.index, columns=months_complementary )

    

    #Add the complementary dataframe to create a full-month dataframe

    df_y_true_pivot = pd.concat([df_y_true_pivot,df_complementary],axis=1)[all_months_survival]



    # Fill NaN value to either 0 or 1

    df_y_pro = df_y_true_pivot.apply(lambda x: apply_non_zero(x),axis=1)

    

    

    chosen_months_survival = np.arange(0,years*12+1)

    df_y_pro = df_y_pro[chosen_months_survival].values

    

    return df_y_pro





'''

Function to compute the weighted Brier score

@paramters

    pred: is the prediction matrix

        Format: Index = ID, Rows = patients, cols = months, values = probability of being alive (alive = 1, dead = 0)

    actual: is the actual value matrix.

        Format: Index = ID, Rows = patients, cols = ['specific_death', 'months_survival']

    weights: is the matrix of weights associated to the predictions

    years_cutoff: is the time for which the error is computed

@return

    the vector of actual status over time

'''

def brier_score_loss_weighted(pred, actual, weights, years_cutoff = 10):

    

    # Select the desired period

    weights = weights.iloc[:,:(12*years_cutoff)+1]

    

    # obtain the unique time values in the y data of your survival data

    unique_times = range(0, (12*years_cutoff)+1)

    

    # fill an empty matrix to hold the weights

    errors = np.empty([len(actual), len(unique_times)])

    

    # subset y_pred to be the number of years you want

    m_y_pred = pred.iloc[:,:(max(unique_times)+1)]



    try:

        m_y_true = np.matrix(populate_actual(actual, years_cutoff))

        m_y_pred = np.matrix(m_y_pred)

        m_weights = np.matrix(weights)

    except:

        print(""Matrix format is required for y_true, y_predict and weights"")

            

    errors = np.multiply(np.power(m_y_pred - m_y_true,2),m_weights)

    

    error = pd.DataFrame(errors)

    time = years_cutoff * 12

    

    # calculate the average error of all patients for each time

    all_dates = error.mean(axis=0)

    

    # subset desired dates

    desired_dates = pd.DataFrame(all_dates[0:(time+1)])

    

    # calculate the average error up until a point in time

    desired_error = np.mean(desired_dates)

    

    return desired_error","['!/usr/bin/python', 'weight will always be 1 if we know they are dead', 'Create a matrix with patient id in the index and months_survival as header, specific_death as values.', 'The table changes order of rows after pivoting it, we need to reorder it again by doing reindex_axis.', 'We need to calculate the weights based on the entire time initially, then cut it off after the fact', 'Get aux matrix to provide in the get_weights function.', 'Probability of being alive at each month based on patients you are certain about (excluding patients censored pior to month)', 'Get weights', 'Create a matrix by pivoting table', 'The table changes order of rows after pivoting it, we need to reorder it again by doing reindex_axis.', ""Get the month that we don't have any patient record in our dataset."", 'Add the complementary dataframe to create a full-month dataframe', 'Fill NaN value to either 0 or 1', 'Select the desired period', 'obtain the unique time values in the y data of your survival data', 'fill an empty matrix to hold the weights', 'subset y_pred to be the number of years you want', 'calculate the average error of all patients for each time', 'subset desired dates', 'calculate the average error up until a point in time']",[],[],[],[],57.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],190,[],[],0,20,0,0,0,0,0.0,0,0,True
2,code,[],"

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import time

from plotly import tools

import plotly.offline as py

py.init_notebook_mode(connected=True)

import plotly.graph_objs as go



import datetime

import random



import re

sns.set()



from scipy.stats.mstats import winsorize

import scipy.stats

import pandas as pd

pd.set_option('display.max_rows', 500)

pd.set_option('display.max_columns', 500)

pd.set_option('display.width', 1000)





#%matplotlib notebook

import warnings

warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder

from sksurv.preprocessing import OneHotEncoder

from sksurv.linear_model import CoxPHSurvivalAnalysis

from sksurv.ensemble import GradientBoostingSurvivalAnalysis

from sksurv.linear_model import CoxnetSurvivalAnalysis

from sklearn.preprocessing import StandardScaler",['%matplotlib notebook'],"['text/html', 'text/vnd.plotly.v1+html']",[],[],[],58.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],35,[],[],0,1,1,0,0,0,0.0,0,0,True
3,code,[],"df = pd.read_csv(""./numeric_full.csv"",  index_col='ID')


",[],[],[],[],[],59.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
4,code,[],"df.visceral_metastasis_location_ENCODEDmonths_survival.fillna(0,inplace=True)



df.visceral_metastasis_location_ENCODEDspecific_death.fillna(0,inplace=True)
",[],[],[],[],[],60.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
5,code,[],"x=df.drop(columns=['specific_death', 'months_survival'])

y=df[['specific_death','months_survival','train']]",[],[],[],[],[],61.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
6,code,[],x = x._get_numeric_data(),[],[],[],[],[],62.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
7,code,[],"sns.heatmap(x.corr(),center=True,square=True);",[],"['image/png', 'text/plain']",[],[],[],63.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,1,0,0,0,0.0,0,0,True
8,code,[],x.shape,[],[],[],[],['text/plain'],64.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
9,code,[],"x[~x.isin([np.nan, np.inf, -np.inf]).any(1)]",[],[],[],[],"['text/html', 'text/plain']",65.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
10,code,[],x.shape,[],[],[],[],['text/plain'],66.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
11,code,[],"train = x[x['train'] == 1]

test = x[x['train'] == 0]",[],[],[],[],[],67.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
12,code,[],,[],[],[],[],[],,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
13,code,[],"# Feature selection

b_x_train = train.drop(columns='train')

b_y_train = y.query('train==1')[['specific_death', 'months_survival']]",['Feature selection'],[],[],[],[],68.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],3,[],[],0,1,0,0,0,0,0.0,0,0,True
14,code,[],b_test = test.drop(columns='train'),[],[],[],[],[],69.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
15,code,[],b_y_train.isnull().sum(),[],[],[],[],['text/plain'],70.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
16,code,[],"def fit_and_prepare(x_train, y_train, test_df):

    

    # 3.1. Prepare Y-----

    y_train.specific_death = y_train.specific_death.astype(bool)

    

    # Transform it into a structured array

    y_train = y_train.to_records(index = False)

    

    # 3.2. Prepare X-----

    # obtain the x variables that are categorical

    categorical_feature_mask = x_train.dtypes==object



    # Filter categorical columns using mask and turn it into a list

    categorical_cols = x_train.columns[categorical_feature_mask].tolist()



    # Ensure categorical columns are category type

    for col in categorical_cols:

        x_train[col] = x_train[col].astype('category')

        test_df[col] = test_df[col].astype('category')

    

    # 3.3. Fit model-----

    # initiate

    encoder = OneHotEncoder()

    estimator = CoxnetSurvivalAnalysis(fit_baseline_model=True)

    

    # fit model

    estimator.fit(encoder.fit_transform(x_train), y_train)

    

    # transform the test variables to match the train

    x_test = encoder.transform(test_df)

    

    return (estimator, x_test, x_train, y_train)","['3.1. Prepare Y-----', 'Transform it into a structured array', '3.2. Prepare X-----', 'obtain the x variables that are categorical', 'Filter categorical columns using mask and turn it into a list', 'Ensure categorical columns are category type', '3.3. Fit model-----', 'initiate', 'fit model', 'transform the test variables to match the train']",[],[],[],[],71.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],32,[],[],0,10,0,0,0,0,0.0,0,0,True
17,code,[],"test.isna().sum().sum()
",[],[],[],[],['text/plain'],72.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
18,code,[],"b_x_train.isna().sum().sum()
",[],[],[],[],['text/plain'],73.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
19,code,[],b_y_train.isna().sum().sum(),[],[],[],[],['text/plain'],74.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
20,code,[],"estimator, x_test, x_train, y_train = fit_and_prepare(b_x_train,b_y_train, test)",[],[],[],[],[],75.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
21,code,[],"estimator_b, x_test_b, x_train_b, y_train_b = fit_and_prepare(b_x_train, b_y_train, b_test)",[],[],[],[],[],76.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
22,code,[],"pred_surv = estimator.predict(x_test)

pred_surv[0]",[],[],[],[],['text/plain'],77.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
23,code,[],x_test.head(),[],[],[],[],"['text/html', 'text/plain']",78.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
24,code,[],"def get_probabilities(x_test, estimator):

    

    pred_surv = estimator.predict_survival_function(x_test)



    # Get the ""X's"" or time of each data point

    times = pred_surv[0].x



    # Create an empty pandas dataframes with these times as the columns

    pred_df = pd.DataFrame(columns = times)



    # Convert each row to a pandas series row (transpose) with the index as these x times and append it to the df

    for i in range(0, len(x_test)):

        pred_df = pred_df.append(pd.DataFrame(pred_surv[i].y).set_index(times).T) 



    pred_df = pred_df.set_index(x_test.index)



    return pred_df","['Get the ""X\'s"" or time of each data point', 'Create an empty pandas dataframes with these times as the columns', 'Convert each row to a pandas series row (transpose) with the index as these x times and append it to the df']",[],[],[],[],79.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],17,[],[],0,3,0,0,0,0,0.0,0,0,True
25,code,[],"# 4.2 store the predictions

predictions_b = get_probabilities(x_test_b, estimator_b)",['4.2 store the predictions'],[],[],[],[],80.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,0,0,True
26,code,[],y_train,[],[],[],[],['text/plain'],81.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
27,code,[],predictions_b,[],[],[],[],"['text/html', 'text/plain']",82.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
28,code,[],"# 4.3 Compute estimate of the survival curves

pred_curves = estimator_b.predict_survival_function(x_test_b)",['4.3 Compute estimate of the survival curves'],[],[],[],[],83.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,1,0,0,0,0,0.0,0,0,True
29,code,[],"for curve in pred_curves[0:3]:

    plt.step(curve.x, curve.y, where=""post"")",[],"['image/png', 'text/plain']",[],[],[],84.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
30,code,[],"from sksurv.metrics import concordance_index_censored



prediction = estimator.predict(x_train)

result = concordance_index_censored(y_train['specific_death'], 

                                    y_train[""months_survival""],

                                    prediction)

result[0]",[],[],[],[],['text/plain'],85.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],7,[],[],0,0,0,0,1,0,0.0,0,0,True
31,code,[],"from sksurv.metrics import concordance_index_censored



pred_b = estimator_b.predict(x_train_b)

result = concordance_index_censored(y_train_b['specific_death'], 

                                    y_train_b[""months_survival""],

                                    pred_b)

result[0]",[],[],[],[],['text/plain'],86.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],7,[],[],0,0,0,0,1,0,0.0,0,0,True
32,code,[],predictions_df = pd.DataFrame(predictions_b),[],[],[],[],[],87.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
33,code,[],"

# Please, remember that rows NEED TO BE indexed by patient IDs and columns MUST be ordered from T0 to T120



# First subset to 10 years

predictions_10yr = predictions_df.iloc[:,:121]



#Rename columns to Time periods

columns = predictions_10yr.columns.values

new_columns = ['T' + str(s) for s in columns]

predictions_10yr.columns = new_columns



# Write the final CSV file

# Please, remember than in order to make the submission you need to create a .zip file ONLY with the csv

pd.DataFrame(predictions_10yr).to_csv('sample-submission-cox.csv')","['Please, remember that rows NEED TO BE indexed by patient IDs and columns MUST be ordered from T0 to T120', 'First subset to 10 years', 'Rename columns to Time periods', 'Write the final CSV file', 'Please, remember than in order to make the submission you need to create a .zip file ONLY with the csv']",[],[],[],[],88.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],14,[],[],0,5,0,0,0,0,0.0,0,0,True
34,code,[],"predictions_df.index = test.index

predictions_df",[],[],[],[],"['text/html', 'text/plain']",89.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
35,raw,[],,[],[],[],[],[],,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,,0,45,False
36,code,[],"def fit_and_score_features(X, y):

    n_features = X.shape[1]

    scores = np.empty(n_features)

    m = CoxPHSurvivalAnalysis()

    for j in range(n_features):

        Xj = X[:, j:j+1]

        m.fit(Xj, y)

        scores[j] = m.score(Xj, y)

    return scores



scores = fit_and_score_features(b_x_train.values, b_y_train)

#pd.Series(scores, index=data_x_numeric.columns).sort_values(ascending=False)","['pd.Series(scores, index=data_x_numeric.columns).sort_values(ascending=False)']",[],['ValueError'],['y must be a structured array with the first field being a binary class event indicator and the second field the time of the event/censoring'],[],90.0,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],12,[],[],0,1,0,1,0,0,0.0,0,0,True
37,code,[],b_y_train,[],[],[],[],[],,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
38,code,[],,[],[],[],[],[],,cmougan..AccentureMedicalDatathon2019..first_sub_2-Copy1.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
0,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'London Real Estate Pricing Analysis']]",[],0,[],['# London Real Estate Pricing Analysis'],0,0,0,0,0,0,,0,6,False
1,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Introduction']]",[],0,[],"['## Introduction\n', 'In this project I have scraped data from various data about London real estate price and its Boroughs.Then I used Foursquare API to get the common venues of the boroughs.Then using K-means clustering I clustered London Neighborhoods on the basis of the common venues.']",0,0,0,0,0,0,,0,46,False
2,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Importing Libraries']]",[],0,[],['# Importing Libraries'],0,0,0,0,0,0,,0,3,False
3,code,[],"import numpy as np # library to handle data in a vectorized manner



import pandas as pd # library for data analsysis

pd.set_option('display.max_columns', None)

pd.set_option('display.max_rows', None)



import json # library to handle JSON files



import geocoder as geocoder

from geopy.geocoders import Nominatim # convert an address into latitude and longitude values



import requests # library to handle requests

from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe



# Matplotlib and associated plotting modules

import matplotlib.cm as cm

import matplotlib.colors as colors



# import k-means from clustering stage

from sklearn.cluster import KMeans



import folium # map rendering library



print('Libraries imported.')","['library to handle data in a vectorized manner', 'library for data analsysis', 'library to handle JSON files', 'convert an address into latitude and longitude values', 'library to handle requests', 'tranform JSON file into a pandas dataframe', 'Matplotlib and associated plotting modules', 'import k-means from clustering stage', 'map rendering library']",[],[],[],[],1.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],24,[],[],0,9,0,0,0,0,0.0,1,0,True
4,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Scraping table from Wikipedia']]",[],0,[],['# Scraping table from Wikipedia'],0,0,0,0,0,0,,0,5,False
5,code,[],"import pandas as pd

import wikipedia as wp

 

#Get the html source

html = wp.page(""List of areas of London"").html().encode(""UTF-8"")

df = pd.read_html(html)[1]

df.head()",['Get the html source'],[],[],[],"['text/html', 'text/plain']",13.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],7,[],[],0,1,0,0,1,0,0.0,0,0,True
6,code,[],df.columns,[],[],[],[],['text/plain'],32.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
7,code,[],"df.columns = ['Location','London_borough','Post town','Postcode district','Dial Code','OS grid ref']",[],[],[],[],[],59.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
8,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cleaning the table']]",[],0,[],"['## Cleaning the table\n', '\n', 'The table contains hyperlinks numbers and there are more than one Postal codes so I decided to keep one.']",0,0,0,0,0,0,,0,23,False
9,code,[],"df['London_borough'] =  df['London_borough'].apply(lambda x: x.replace('[','').replace(']','')) 

df['London_borough'] =  df['London_borough'].str.replace('\d+', '')

df['London_borough'] =  df['London_borough'].str.split(',').str[0]

df['Postcode district'] =  df['Postcode district'].str.split(',').str[0]

df['Postcode district'] =  df['Postcode district'].str.split('(').str[0]

df['Post town'] =  df['Post town'].str.split(',').str[0]",[],[],[],[],[],89.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],6,[],[],0,0,0,0,0,0,0.0,0,0,True
10,code,[],df.head(),[],[],[],[],"['text/html', 'text/plain']",90.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
11,code,[],"df.to_csv('london.csv',index=False)",[],[],[],[],[],92.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
12,code,[],df.shape,[],[],[],[],['text/plain'],78.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
13,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Getting property data of London']]",[],0,[],"['# Getting property data of London\n', ' \n', 'This time it was not a wikipedia page so I use pandas read_html function to scrape data.']",0,0,0,0,0,0,,0,23,False
14,code,[],"tables = pd.read_html(""https://propertydata.co.uk/cities/london"",header=0)

df2=pd.DataFrame(data=tables[0])

df2",[],[],[],[],"['text/html', 'text/plain']",102.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],3,[],[],0,0,0,0,1,0,0.0,0,0,True
15,code,[],df2.columns,[],[],[],[],['text/plain'],103.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
16,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Cleaning Property Data']]",[],0,[],"['# Cleaning Property Data\n', '\n', ""Firstly removed unwanted columns then price was in string format so '£' and commas to be removed""]",0,0,0,0,0,0,,0,21,False
17,code,[],"df2.drop(columns=['Avg yield','£/sqft','5yr +/-','Explore data'],inplace=True)",[],[],[],[],[],105.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
18,code,[],df2.head(),[],[],[],[],"['text/html', 'text/plain']",106.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
19,code,[],"df2['Avg price'] = df2['Avg price'].str.replace(""£"","""")

df2['Avg price'] = df2['Avg price'].str.replace(',', '')

df2['Avg price'] = pd.to_numeric(df2['Avg price'])",[],[],[],[],[],124.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
20,code,[],df2.head(),[],[],[],[],"['text/html', 'text/plain']",129.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
21,code,[],"df2.columns=['Postcode district','Avg price']",[],[],[],[],[],128.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
22,code,[],"df2.to_csv('london_postcode.csv',index=False)",[],[],[],[],[],130.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
23,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Merge the table of London Boroughs and London House Prices']]",[],0,[],"['# Merge the table of London Boroughs and London House Prices\n', '\n', ""I have done inner joint because there were many postcodes that were not in the London Borough's table""]",0,0,0,0,0,0,,0,29,False
24,code,[],"data=pd.merge(df, df2, how='inner', left_on='Postcode district', right_on='Postcode district')",[],[],[],[],[],133.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
25,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Finding Longitude and Latitude of the Locations']]",[],0,[],['# Finding Longitude and Latitude of the Locations\n'],0,0,0,0,0,0,,0,8,False
26,code,[],"def getLatLong(row):

    #print('post :{}'.format(row[:]))

    #print('neigh :{}'.format(row[1]))

    # initialize your variable to None

    lat_lng_coords = None

    search_query = '{}, London,UK'.format(row)

    # loop until you get the coordinates

    try:

        while(lat_lng_coords is None):

            #g = geocoder.here(search_query,app_id=app_id,app_code=app_code)

            g = geocoder.arcgis(search_query)

            lat_lng_coords = g.latlng

            #print('FIRST')

    except IndexError:

        latitude = 0.0

        longitude = 0.0

        print('BACKUP')

        return [latitude,longitude]



    latitude = lat_lng_coords[0]

    longitude = lat_lng_coords[1]

    print(latitude, longitude)

    return [latitude, longitude]","[""print('post :{}'.format(row[:]))"", ""print('neigh :{}'.format(row[1]))"", 'initialize your variable to None', 'loop until you get the coordinates', 'g = geocoder.here(search_query,app_id=app_id,app_code=app_code)', ""print('FIRST')""]",[],[],[],[],4.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],23,[],[],0,6,0,0,0,0,0.0,0,0,True
27,code,[],coords_list = data['Postcode district'].apply(getLatLong).tolist(),[],[],[],[],[],10.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,3,0,True
28,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Merging the values in the dataframe']]",[],0,[],['## Merging the values in the dataframe'],0,0,0,0,0,0,,0,7,False
29,code,[],"data[['Latitude','Longitude']]=pd.DataFrame(coords_list,columns=['Latitude', 'Longitude'])

data.head()",[],[],[],[],"['text/html', 'text/plain']",11.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
30,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Droping unwanted columns']]",[],0,[],['## Droping unwanted columns'],0,0,0,0,0,0,,0,4,False
31,code,[],"data.drop(columns=['Post town','Dial Code','OS grid ref'],inplace=True)",[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
32,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[4, 'Use geopy library to get the latitude and longitude values of London.']]",[],0,[],['#### Use geopy library to get the latitude and longitude values of London.'],0,0,0,0,0,0,,0,13,False
33,code,[],"address = 'London'



geolocator = Nominatim(user_agent=""ldn_explorer"")

location = geolocator.geocode(address)

latitude = location.latitude

longitude = location.longitude

print('The geograpical coordinate of London are {}, {}.'.format(latitude, longitude))",[],[],[],[],[],2.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],7,[],[],0,0,0,0,0,0,0.0,1,0,True
34,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Create a map of London with neighborhoods superimposed on top.']]",[],0,[],['## Create a map of London with neighborhoods superimposed on top.'],0,0,0,0,0,0,,0,11,False
35,code,[],"map_london = folium.Map(location=[latitude, longitude], zoom_start=10)



# add markers to map

for lat, lng, borough, neighborhood in zip(data['Latitude'], data['Longitude'], data['London_borough'], data['Location']):

    label = '{}, {}'.format(neighborhood, borough)

    label = folium.Popup(label, parse_html=True)

    folium.CircleMarker(

        [lat, lng],

        radius=2,

        popup=label,

        color='blue',

        fill=True,

        fill_color='#3186cc',

        fill_opacity=0.7,

        parse_html=False).add_to(map_london)  

    

map_london","['add markers to map', ""3186cc',""]",[],[],[],"['text/html', 'text/plain']",88.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],17,[],[],0,2,0,0,1,0,0.0,0,0,True
36,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Define Foursquare Credentials and Version']]",[],0,[],"['Next, utilizing the Foursquare API to explore the neighborhoods and segment them.\n', '\n', '## Define Foursquare Credentials and Version']",0,0,0,0,0,0,,0,18,False
37,code,[],"CLIENT_ID = 'U0OWUYRP10WK5UFHIVQGDEKRQLZBSFYRALYTWAF0OBVJHIZ4' # your Foursquare ID

CLIENT_SECRET = 'R2BLV2WI204CLFJ5GTAS1OR0U2AFMKZV5UTWI20OREP30SZQ' # your Foursquare Secret

VERSION = '20180604'","['your Foursquare ID', 'your Foursquare Secret']",[],[],[],[],33.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],3,[],[],0,2,0,0,0,0,0.0,0,0,True
38,code,[],"def getBuiltUrl(neigh_lat,neigh_long,radius=1400):

    # type your answer here

    LIMIT=100

    #radius=1000

    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(

    CLIENT_ID, 

    CLIENT_SECRET, 

    VERSION, 

    neigh_lat, 

    neigh_long, 

    radius, 

    LIMIT)

    return url","['type your answer here', 'radius=1000']",[],[],[],[],34.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],13,[],[],0,2,0,0,0,0,0.0,0,0,True
39,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Exploring first Location in our dataframe']]",[],0,[],['## Exploring first Location in our dataframe'],0,0,0,0,0,0,,0,7,False
40,code,[],"neigh_name, neigh_borough, neigh_post, neigh_price, neigh_lat, neigh_long = data.iloc[0]

print('Latitude and longitude values of {} are {}, {}.'.format(neigh_name, 

                                                               neigh_lat, 

                                                               neigh_long))

results = requests.get(getBuiltUrl(neigh_lat,neigh_long)).json()

results",[],[],[],[],['text/plain'],35.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],6,[],[],0,0,0,0,1,0,0.0,1,0,True
41,code,[],"def get_category_type(row):

    try:

        categories_list = row['categories']

    except:

        categories_list = row['venue.categories']

        

    if len(categories_list) == 0:

        return None

    else:

        return categories_list[0]['name']",[],[],[],[],[],36.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],10,[],[],0,0,0,0,0,0,0.0,0,0,True
42,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[4, 'Now cleaning the json file and structure it into a *pandas* dataframe.']]",[],0,[],['#### Now cleaning the json file and structure it into a *pandas* dataframe.'],0,0,0,0,0,0,,0,13,False
43,code,[],"venues = results['response']['groups'][0]['items']

    

nearby_venues = json_normalize(venues) # flatten JSON



# filter columns

filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']

nearby_venues =nearby_venues.loc[:, filtered_columns]



# filter the category for each row

nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)



# clean columns

nearby_venues.columns = [col.split(""."")[-1] for col in nearby_venues.columns]



nearby_venues.head()","['flatten JSON', 'filter columns', 'filter the category for each row', 'clean columns']",[],[],[],"['text/html', 'text/plain']",37.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],15,[],[],0,4,0,0,1,0,0.0,0,0,True
44,code,[],print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0])),[],[],[],[],[],38.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
45,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Function to repeat the same process to all the neighborhoods in London']]",[],0,[],['# Function to repeat the same process to all the neighborhoods in London'],0,0,0,0,0,0,,0,13,False
46,code,[],"def getNearbyVenues(names, latitudes, longitudes, radius=500):

    

    venues_list=[]

    for name, lat, lng in zip(names, latitudes, longitudes):

        print(name)

            

        # create the API request URL

        LIMIT=100

        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(

            CLIENT_ID, 

            CLIENT_SECRET, 

            VERSION, 

            lat, 

            lng, 

            radius, 

            LIMIT)

            

        # make the GET request

        results = requests.get(url).json()[""response""]['groups'][0]['items']

        

        # return only relevant information for each nearby venue

        venues_list.append([(

            name, 

            lat, 

            lng, 

            v['venue']['name'], 

            v['venue']['location']['lat'], 

            v['venue']['location']['lng'],  

            v['venue']['categories'][0]['name']) for v in results])



    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])

    nearby_venues.columns = ['Neighborhood', 

                  'Neighborhood Latitude', 

                  'Neighborhood Longitude', 

                  'Venue', 

                  'Venue Latitude', 

                  'Venue Longitude', 

                  'Venue Category']

    

    return(nearby_venues)","['create the API request URL', 'make the GET request', 'return only relevant information for each nearby venue']",[],[],[],[],39.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],40,[],[],0,3,0,0,0,0,0.0,0,0,True
47,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[4, 'Running our function for all Location in the dataframe']]",[],0,[],['#### Running our function for all Location in the dataframe'],0,0,0,0,0,0,,0,10,False
48,code,[],"london_venues = getNearbyVenues(names=data['Location'], 

                                   latitudes=data['Latitude'], 

                                   longitudes=data['Longitude'], 

                                   radius=500)",[],[],[],[],[],40.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,1,0,True
49,code,[],"print(london_venues.shape)

london_venues.head()",[],[],[],[],"['text/html', 'text/plain']",41.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,1,0,True
50,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[3, 'Venues returned for each Neighborhood Location']]",[],0,[],['### Venues returned for each Neighborhood Location'],0,0,0,0,0,0,,0,7,False
51,code,[],london_venues.groupby('Neighborhood').count(),[],[],[],[],"['text/html', 'text/plain']",42.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
52,code,[],print('There are {} uniques categories.'.format(len(london_venues['Venue Category'].unique()))),[],[],[],[],[],43.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
53,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Analyzing each Neighborhood Location']]",[],0,[],['## Analyzing each Neighborhood Location'],0,0,0,0,0,0,,0,5,False
54,code,[],"# one hot encoding

london_onehot = pd.get_dummies(london_venues[['Venue Category']], prefix="""", prefix_sep="""")



# add neighborhood column back to dataframe

london_onehot['Neighborhood'] = london_venues['Neighborhood'] 



# move neighborhood column to the first column

fixed_columns = [london_onehot.columns[-1]] + list(london_onehot.columns[:-1])

london_onehot = london_onehot[fixed_columns]



london_onehot.head()","['one hot encoding', 'prefix_sep="""")', '', '# add neighborhood column back to dataframe', 'add neighborhood column back to dataframe', ""london_onehot['Neighborhood'] = london_venues['Neighborhood']"", '', '# move neighborhood column to the first column', 'move neighborhood column to the first column', 'fixed_columns = [london_onehot.columns[-1]] + list(london_onehot.columns[:-1])', 'london_onehot = london_onehot[fixed_columns]', '', 'london_onehot.head()']",[],[],[],"['text/html', 'text/plain']",44.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],11,[],[],0,4,0,0,1,0,0.0,0,0,True
55,code,[],london_onehot.shape,[],[],[],[],['text/plain'],45.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
56,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[3, 'Grouping rows by neighborhood and by taking the mean of the frequency of occurrence of each category']]",[],0,[],['###  Grouping rows by neighborhood and by taking the mean of the frequency of occurrence of each category'],0,0,0,0,0,0,,0,18,False
57,code,[],"london_grouped = london_onehot.groupby('Neighborhood').mean().reset_index()

london_grouped",[],[],[],[],"['text/html', 'text/plain']",46.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
58,code,[],"a=london_grouped

a.head()",[],[],[],[],"['text/html', 'text/plain']",33.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
59,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[3, 'Top 5 most common venues for each Neighborhood']]",[],0,[],['### Top 5 most common venues for each Neighborhood'],0,0,0,0,0,0,,0,9,False
60,code,[],"num_top_venues = 5



for hood in a['Neighborhood']:

    print(""----""+hood+""----"")

    temp = a[a['Neighborhood'] == hood].T.reset_index()

    temp.columns = ['venue','freq']

    temp = temp.iloc[1:]

    temp['freq'] = temp['freq'].astype(float)

    temp = temp.round({'freq': 2})

    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))

    print('\n')",[],[],[],[],[],34.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],11,[],[],0,0,0,0,0,0,0.0,12,0,True
61,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Put in the Dataframe']]",[],0,[],['## Put in the Dataframe'],0,0,0,0,0,0,,0,5,False
62,code,[],"def return_most_common_venues(row, num_top_venues):

    row_categories = row.iloc[1:]

    row_categories_sorted = row_categories.sort_values(ascending=False)

    

    return row_categories_sorted.index.values[0:num_top_venues]",[],[],[],[],[],35.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
63,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[4, 'Create the new dataframe and display the top 10 venues for each neighborhood.']]",[],0,[],['#### Create the new dataframe and display the top 10 venues for each neighborhood.'],0,0,0,0,0,0,,0,14,False
64,code,[],"num_top_venues = 10



indicators = ['st', 'nd', 'rd']



# create columns according to number of top venues

columns = ['Neighborhood']

for ind in np.arange(num_top_venues):

    try:

        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))

    except:

        columns.append('{}th Most Common Venue'.format(ind+1))



# create a new dataframe

neighborhoods_venues_sorted = pd.DataFrame(columns=columns)

neighborhoods_venues_sorted['Neighborhood'] = a['Neighborhood']



for ind in np.arange(a.shape[0]):

    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(a.iloc[ind, :], num_top_venues)



neighborhoods_venues_sorted.head()","['create columns according to number of top venues', 'create a new dataframe']",[],[],[],"['text/html', 'text/plain']",36.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],20,[],[],0,2,0,0,1,0,0.0,0,0,True
65,code,[],"neighborhoods_venues_sorted.rename(columns = {'Neighborhood':'Location'}, inplace = True)",[],[],[],[],[],37.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
66,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Adding Average house price of each Neighborhood in the Group'], [3, 'Normalizing the Avg price column']]",[],0,[],"['## Adding Average house price of each Neighborhood in the Group\n', '### Normalizing the Avg price column']",0,0,0,0,0,0,,0,17,False
67,code,[],"a['Price']=data['Avg price']

v= a.iloc[:, -1]

a.iloc[:,-1] = (v - v.min()) / (v.max() - v.min())",[],[],[],[],[],38.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
68,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Clustering Neighborhoods']]",[],0,[],"['\n', '\n', '\n', '# Clustering Neighborhoods\n', '\n', 'running K-means clustering for 4 clusters']",0,0,0,0,0,0,,0,9,False
69,code,[],"kclusters = 6



london_grouped_clustering = a.drop('Neighborhood', 1)



# run k-means clustering

kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(london_grouped_clustering)



# check cluster labels generated for each row in the dataframe

kmeans.labels_[0:10] ","['run k-means clustering', 'check cluster labels generated for each row in the dataframe']",[],[],[],['text/plain'],98.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],9,[],[],0,2,0,0,1,0,0.0,0,0,True
70,code,[],"from sklearn import metrics

from scipy.spatial.distance import cdist

import numpy as np

import matplotlib.pyplot as plt



# k means determine k

distortions = []

K = range(1,10)

for k in K:

    kmeanModel = KMeans(n_clusters=k).fit(london_grouped_clustering)

    kmeanModel.fit(london_grouped_clustering)

    distortions.append(sum(np.min(cdist(london_grouped_clustering, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / london_grouped_clustering.shape[0])



# Plot the elbow

plt.plot(K, distortions, 'bx-')

plt.xlabel('k')

plt.ylabel('Distortion')

plt.title('The Elbow Method showing the optimal k')

plt.show()
","['k means determine k', 'Plot the elbow']","['image/png', 'text/plain']",[],[],[],97.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],19,[],[],0,2,1,0,0,0,0.0,0,0,True
71,code,[],"neighborhoods_venues_sorted.drop(columns = 'Cluster Labels', inplace = True) ",[],[],[],[],[],99.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
72,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[4, 'Create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood.']]",[],0,[],['#### Create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood.'],0,0,0,0,0,0,,0,19,False
73,code,[],"neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)



london_merged = data



london_merged = london_merged.join(neighborhoods_venues_sorted.set_index('Location'), on='Location')



london_merged",[],[],[],[],"['text/html', 'text/plain']",100.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],7,[],[],0,0,0,0,1,0,0.0,0,0,True
74,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[5, 'There was null value in the merged data so droping the rows with null values']]",[],0,[],['##### There was null value in the merged data so droping the rows with null values'],0,0,0,0,0,0,,0,16,False
75,code,[],london_merged.dropna(inplace=True),[],[],[],[],[],101.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
76,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[5, ""Because of null values the data type of 'Cluster Labels' was changed from int to float so converting it back to int""]]",[],0,[],"[""##### Because of null values the data type of 'Cluster Labels' was changed from int to float so converting it back to int""]",0,0,0,0,0,0,,0,23,False
77,code,[],"london_merged['Cluster Labels'] = london_merged['Cluster Labels'].astype(int)

london_merged.dtypes",[],[],[],[],['text/plain'],102.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
78,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[3, 'There were many Location that were assigned the same postcodes as they were very near so droping the duplicate postcodes']]",[],0,[],['### There were many Location that were assigned the same postcodes as they were very near so droping the duplicate postcodes '],0,0,0,0,0,0,,0,21,False
79,code,[],"london_merged.drop_duplicates(subset='Postcode district',inplace=True)",[],[],[],[],[],103.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
80,code,[],"london_merged.reset_index(inplace=True)

london_merged.drop(columns='index',inplace=True)",[],[],[],[],[],104.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
81,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Visualize the clusters']]",[],0,[],['## Visualize the clusters'],0,0,0,0,0,0,,0,4,False
82,code,[],"map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)



# set color scheme for the clusters

x = np.arange(kclusters)

ys = [i + x + (i*x)**2 for i in range(kclusters)]

colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))

rainbow = [colors.rgb2hex(i) for i in colors_array]



# add markers to the map

markers_colors = []

for lat, lon, poi, cluster in zip(london_merged['Latitude'],london_merged['Longitude'], london_merged['Location'], london_merged['Cluster Labels']):

    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)

    folium.CircleMarker(

        [lat, lon],

        radius=3,

        popup=label,

        color=rainbow[cluster-1],

        fill=True,

        fill_color=rainbow[cluster-1],

        fill_opacity=0.7).add_to(map_clusters)

       

map_clusters","['set color scheme for the clusters', 'add markers to the map']",[],[],[],"['text/html', 'text/plain']",106.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],22,[],[],0,2,0,0,1,0,0.0,0,0,True
83,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Binning']]",[],0,[],['# Binning'],0,0,0,0,0,0,,0,2,False
84,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[3, 'There is the range of Avg price so binned the price into 7 distinct values'], [5, ""('Low level 1', 'Low level 2', 'Average level 1', 'Average level 2','Above Average','High level 1','High level 2')""]]",[],0,[],"['### There is the range of Avg price so binned the price into 7 distinct values\n', ""##### ('Low level 1', 'Low level 2', 'Average level 1', 'Average level 2','Above Average','High level 1','High level 2')\n"", '\n', 'Visualizing the bins']",0,0,0,0,0,0,,0,37,False
85,code,[],"%matplotlib inline

import matplotlib as plt

from matplotlib import pyplot

plt.pyplot.hist(london_merged[""Avg price""],bins=7)



# set x/y labels and plot title

plt.pyplot.xlabel(""Avg price"")

plt.pyplot.ylabel(""count"")

plt.pyplot.title(""Price bins"")",['set x/y labels and plot title'],"['image/png', 'text/plain']",[],[],['text/plain'],46.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],9,[],[],0,1,1,0,1,0,0.0,0,0,True
86,code,[],"bins = np.linspace(min(london_merged[""Avg price""]), max(london_merged[""Avg price""]), 8)

bins",[],[],[],[],['text/plain'],119.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
87,code,[],"group_names = ['Low level 1', 'Low level 2', 'Average level 1', 'Average level 2','Above Average','High level 1','High level 2']",[],[],[],[],[],120.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
88,code,[],"london_merged['Price-Categories'] = pd.cut(london_merged['Avg price'], bins, labels=group_names, include_lowest=True )

london_merged[['Avg price','Price-Categories']].head()",[],[],[],[],"['text/html', 'text/plain']",121.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
89,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster bins'], [3, 'Creating 4 bins for clusters']]",[],0,[],"['## Cluster bins\n', '### Creating 4 bins for clusters']",0,0,0,0,0,0,,0,9,False
90,code,[],"plt.pyplot.hist(london_merged[""Cluster Labels""],bins=4)



# set x/y labels and plot title

plt.pyplot.xlabel(""Cluster Labels"")

plt.pyplot.ylabel(""count"")

plt.pyplot.title(""Cluster Labels"")",['set x/y labels and plot title'],"['image/png', 'text/plain']",[],[],['text/plain'],50.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],6,[],[],0,1,1,0,1,0,0.0,0,0,True
91,code,[],"bins = np.linspace(min(london_merged[""Cluster Labels""]), max(london_merged[""Cluster Labels""]), 7)

bins",[],[],[],[],['text/plain'],115.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
92,code,[],"group_names = ['Mixed Social Venues','Hotels and Social Venues','Stores and seafood restaurants','Pubs and Historic places', 'Sports and Athletics','Restaurants and Bars']",[],[],[],[],[],117.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
93,code,[],"london_merged['Cluster-Category'] = pd.cut(london_merged['Cluster Labels'], bins, labels=group_names, include_lowest=True )

london_merged[['Cluster Labels','Cluster-Category']].head()",[],[],[],[],"['text/html', 'text/plain']",118.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
94,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Final Data']]",[],0,[],['# Final Data'],0,0,0,0,0,0,,0,3,False
95,code,[],"london_merged.drop(columns=['Dial Code','OS grid ref','6th Most Common Venue','7th Most Common Venue','8th Most Common Venue','9th Most Common Venue','10th Most Common Venue'],inplace=True)

london_merged.head(20)",[],[],[],[],"['text/html', 'text/plain']",122.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
96,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Creating Chloropleth map to visualize how London is divided in terms of Housing prices and cluster markers on the top']]",[],0,[],['# Creating Chloropleth map to visualize how London is divided in terms of            Housing prices and cluster markers on the top'],0,0,0,0,0,0,,0,21,False
97,code,[],"lnd_geo = r'london_boroughs_proper.geojson'

lnd_map = folium.Map(location = [latitude, longitude], zoom_start = 11)



lnd_map.choropleth(

    geo_data=lnd_geo,

    data=london_merged,

    columns=['London_borough','Avg price'],

    key_on='feature.properties.name',

    fill_color='RdPu', 

    fill_opacity=0.7, 

    line_opacity=0.2,

    legend_name='Average house Prices'

)

# add markers to the map

markers_colors = []

for lat, lon, poi, cluster in zip(london_merged['Latitude'],london_merged['Longitude'], london_merged['Location'], london_merged['Cluster Labels']):

    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)

    folium.CircleMarker(

        [lat, lon],

        radius=3,

        popup=label,

        color=rainbow[cluster-1],

        fill=True,

        fill_color=rainbow[cluster-1],

        fill_opacity=0.7).add_to(lnd_map)

       

# display map

lnd_map","['add markers to the map', 'display map']",[],[],[],"['text/html', 'text/plain']",108.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],28,[],[],0,2,0,0,1,0,0.0,0,0,True
98,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Chloropleth map so to see how the London is clustered']]",[],0,[],['# Chloropleth map so to see how the London is clustered '],0,0,0,0,0,0,,0,11,False
99,code,[],"lnd_geo = r'london_boroughs_proper.geojson'

lnd_map = folium.Map(location = [latitude, longitude], zoom_start = 10)



lnd_map.choropleth(

    geo_data=lnd_geo,

    data=london_merged,

    columns=['London_borough','Cluster Labels'],

    key_on='feature.properties.name',

    fill_color='PuRd', 

    fill_opacity=0.7, 

    line_opacity=0.2,

    legend_name='Cluster Labels'

)



lnd_map",[],[],[],[],"['text/html', 'text/plain']",109.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],15,[],[],0,0,0,0,1,0,0.0,0,0,True
100,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Examining the Clusters']]",[],0,[],['# Examining the Clusters'],0,0,0,0,0,0,,0,4,False
101,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 1']]",[],0,[],['## Cluster 1'],0,0,0,0,0,0,,0,3,False
102,code,[],london_merged[london_merged['Cluster Labels']==0],[],[],[],[],"['text/html', 'text/plain']",123.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
103,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 2']]",[],0,[],['## Cluster 2'],0,0,0,0,0,0,,0,3,False
104,code,[],london_merged[london_merged['Cluster Labels']==1],[],[],[],[],"['text/html', 'text/plain']",124.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
105,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 3']]",[],0,[],['## Cluster 3'],0,0,0,0,0,0,,0,3,False
106,code,[],london_merged[london_merged['Cluster Labels']==2],[],[],[],[],"['text/html', 'text/plain']",125.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
107,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 4']]",[],0,[],['## Cluster 4'],0,0,0,0,0,0,,0,3,False
108,code,[],london_merged[london_merged['Cluster Labels']==3],[],[],[],[],"['text/html', 'text/plain']",126.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
109,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 5']]",[],0,[],['## Cluster 5'],0,0,0,0,0,0,,0,3,False
110,code,[],london_merged[london_merged['Cluster Labels']==4],[],[],[],[],"['text/html', 'text/plain']",127.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
111,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[2, 'Cluster 6']]",[],0,[],['## Cluster 6'],0,0,0,0,0,0,,0,3,False
112,code,[],london_merged[london_merged['Cluster Labels']==5],[],[],[],[],"['text/html', 'text/plain']",128.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
113,markdown,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],"[[1, 'Examining Property prices']]",[],0,[],['# Examining Property prices'],0,0,0,0,0,0,,0,4,False
114,code,[],london_merged[london_merged['Price-Categories']=='High level 2'],[],[],[],[],"['text/html', 'text/plain']",95.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
115,code,[],london_merged[london_merged['Price-Categories']=='High level 1'],[],[],[],[],"['text/html', 'text/plain']",86.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
116,code,[],london_merged[london_merged['Price-Categories']=='Low level 2'],[],[],[],[],"['text/html', 'text/plain']",87.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
117,code,[],london_merged[london_merged['Price-Categories']=='Low level 1'],[],[],[],[],"['text/html', 'text/plain']",96.0,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
118,code,[],,[],[],[],[],[],,mtk12..IBM-Data-science-capstone-project..Capstone-Project.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
0,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[1, 'Neural Style Transfer'], [3, 'Acknowledgement:']]",[],0,"['https://www.deeplearning.ai/deep-learning-specialization/)', 'https://pytorch.org/tutorials/)', ('Deep Learning Course on Coursera', 'https://www.deeplearning.ai/deep-learning-specialization/'), ('Pytorch tutorial', 'https://pytorch.org/tutorials/')]","['# Neural Style Transfer\n', '---\n', '\n', ""This notebook contains a typical machine learning/computer vision problem: **Neural Style Transfer**. With a content image (C) and a style image (S), we are able to generate a image (G) with C's content and S's style. The following images show how a photo can be combined with Van Gogh's painting and generate a new image.\n"", '\n', '<img src=""Image_folder/sea.jpg"" alt=""drawing"" width=""400""/>\n', '<center>Content Image: C</center>  \n', '\n', '<img src=""Image_folder/van.jpg"" alt=""drawing"" width=""300""/>\n', '<center>Style Image: S</center> \n', '<img src=""Image_folder/G_van.jpg"" alt=""drawing"" width=""400""/>\n', '<center>Generated Image: G</center>  \n', '\n', '---\n', '### Acknowledgement:\n', 'Some ideas come from [Deep Learning Course on Coursera](https://www.deeplearning.ai/deep-learning-specialization/) (e.g., the weights of style loss and content loss) and [Pytorch tutorial](https://pytorch.org/tutorials/) (e.g. model loading).']",0,0,0,0,0,0,,0,108,False
1,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '1. Packages and Images Preprocessing']]",[],0,[],['## 1. Packages and Images Preprocessing'],0,0,0,0,0,0,,0,6,False
2,code,[],"import numpy as np

import torch

import torch.nn as nn

import torchvision

from torch.autograd import Variable

import torchvision.transforms as transforms

import torch.nn.functional as F

import torchvision.models as models

from PIL import Image

import matplotlib.pyplot as plt",[],[],[],[],[],23.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],10,[],[],0,0,0,0,0,0,0.0,0,0,True
3,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],"['Helper functions:  \n', '*show*:Show the original image.  \n', '*image_loader*: Preprocess the image.']",0,0,0,0,0,0,,0,10,False
4,code,[],"def show(img):

    """"""Show image with size (width, height, channels).

    """"""

    plt.figure(figsize= (15,10))

    plt.imshow(img)

    plt.show()

    

def image_loader(path):

    """"""Given the path of the image, return the image tensor to feed in the model.

    """"""

    img = Image.open(path).convert('RGB')

    # Resize the image, crop the image, convert the image to tensor.

    loader = transforms.Compose([

                            transforms.Resize(700),

                            transforms.CenterCrop([700,900]),

                            transforms.ToTensor()])

    img = loader(img)

    return img","['Resize the image, crop the image, convert the image to tensor.']",[],[],[],[],24.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],18,[],[],0,1,0,0,0,0,0.0,0,0,True
5,code,[],"C = image_loader('sea.jpg')

S = image_loader('van.jpg')

# Show content image

show(C.permute(1,2,0))

# Show style image

show(S.permute(1,2,0))","['Show content image', 'Show style image']","['image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],25.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],6,[],[],0,2,2,0,0,0,0.0,0,0,True
6,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '2. Define Loss Function']]",[],0,[],['## 2. Define Loss Function'],0,0,0,0,0,0,,0,5,False
7,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],"['The total loss is composed of content loss $J_{content}$ and style loss $J_{style}$.  \n', '**Content Loss:**  \n', '$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\ $$  \n', 'Where $a^{(C)}$ and $a^{(G)}$ are the activation values in some layer.\n', '\n', '**Style Loss:**  \n', '$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2$$\n', '\n', '$$J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)$$\n', 'Where $G^{(G)}_{ij}$ is the value of gram matrix of $G$ at position $i,j$.\n']",0,0,0,0,0,0,,0,75,False
8,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],['**Content Loss:**'],0,0,0,0,0,0,,0,2,False
9,code,[],"def content_loss(a_C, a_G):

    """"""Compute the content loss of the generated image.

    """"""

    # Get the size of activation

    m, n_c, n_h, n_w = a_G.size()

    

    J_content = 1/(4*n_h*n_c*n_w)*(torch.norm(a_C - a_G)**2)

    return J_content",['Get the size of activation'],[],[],[],[],26.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],8,[],[],0,1,0,0,0,0,0.0,0,0,True
10,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],"['**Style Layer Loss:**  \n', 'Compute the gram matrix first and the compute the style loss in one layer.']",0,0,0,0,0,0,,0,17,False
11,code,[],"def gram_matrix(A):

    """"""A of size n_c, n_h*n_w

        Output size n_c,n_c

    """"""

    A_trans = torch.transpose(A,0,1)

    GM = torch.mm(A, A_trans)

    return GM



def style_layer_loss(a_S, a_G):

    """"""Given the hidden layer activations of style and generated image, compute the layer style loss.

    

    """"""

    # get the size of the input tensor

    m, n_c, n_h, n_w = a_G.size()

    

    # resize the tensor to (n_c, n_h*n_w).

    a_G = a_G.view(n_c, n_h*n_w)

    a_S = a_S.view(n_c, n_h*n_w)

    

    GM_S = gram_matrix(a_S)

    GM_G = gram_matrix(a_G)

    

    # conpute the layer style loss

    J_style_layer = 1/(4*(n_c**2)*(n_h*n_w)**2)*(torch.norm(GM_S - GM_G)**2)

    

    return J_style_layer","['get the size of the input tensor', 'resize the tensor to (n_c, n_h*n_w).', 'conpute the layer style loss']",[],[],[],[],27.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],26,[],[],0,3,0,0,0,0,0.0,0,0,True
12,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '3. Load the Model']]",[],0,[],"['## 3. Load the Model  \n', 'Pretrained model VGG19 is used to compute the activation and the loss.']",0,0,0,0,0,0,,0,17,False
13,code,[],"

# Move the model to GPU if possible.

device = torch.device(""cuda:0"" if torch.cuda.is_available else ""cpu"")

# Set the model to evaluation mode.

model = models.vgg19(pretrained = True).features.to(device)

model.eval()","['Move the model to GPU if possible.', 'Set the model to evaluation mode.']",[],[],[],['text/plain'],33.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],6,[],[],0,2,0,0,1,0,0.0,0,0,True
14,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '4. Compute Loss']]",[],0,[],"['## 4. Compute Loss  \n', 'Define layers in the model to compute the activation values.']",0,0,0,0,0,0,,0,14,False
15,code,[],"# Set desired layers to compute loss

style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4','conv_6']

content_layers = ['conv_5']


",['Set desired layers to compute loss'],[],[],[],[],34.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],4,[],[],0,1,0,0,0,0,0.0,0,0,True
16,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],['Build the model for NST and compute the loss.'],0,0,0,0,0,0,,0,9,False
17,code,[],"



class Normalization(nn.Module):

    """"""VGG19 was pretrained with normialized, set a normalization layer for each input image.

    """"""

    def __init__(self, mean, std):

        super().__init__()

        self.mean = torch.tensor(mean).view(-1,1,1)

        self.std = torch.tensor(mean).view(-1,1,1)

    def forward(self, x):

        

        return (x-self.mean)/self.std



def get_loss_model(model,content_image, style_image, input_image,

                   c_layers = content_layers, s_layers = style_layers,

                  mean= torch.tensor([0.485, 0.456, 0.406]), std= torch.tensor([0.229, 0.224, 0.225])):

    """"""Compute content loss and style layer loss given model, input image, content image and style image.

    """"""

    normalization = Normalization(mean, std)

    NST_model = nn.Sequential(normalization)

    J_layer_style = []

    i = 0

    for layer in model.children():

        

        if isinstance(layer, nn.Conv2d):

            i +=1

            name = 'conv_{}'.format(i)

            # Break the loop if it reaches the last layer of style loss

        elif isinstance(layer, nn.ReLU):

            name = 'relu_{}'.format(i)

        elif isinstance(layer, nn.MaxPool2d):

            name = 'max_pool_'.format(i)

        elif isinstance(layer, nn.BatchNorm2d):

            name = 'bn_{}'.format(i)

        else:

            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

            

        NST_model.add_module(name, layer)

        

        # compute the content loss

        if name in c_layers:

            a_C = NST_model(content_image).clone().detach()

            a_G = NST_model(input_image)

            J_content = content_loss(a_C, a_G)

        # coompute the style loss

        J_layer = []

        if name in s_layers:

            a_S = NST_model(style_image).clone().detach()

            a_G = NST_model(input_image)

            J_layer_style.append(style_layer_loss(a_S, a_G))

        

            

        if name == style_layers[-1]:

            break

    

    return J_content, J_layer_style, NST_model



def total_loss(J_content, J_layer_style, coefs=[0.2,0.2,0.2,0.2,0.2], alpha = 10, beta= 40):

    """"""Given weights, compute total loss.

       coefs: the weights in each layer for style loss.

       alpha: the weight of content loss.

       beta: the weight of style loss.

    """"""

    J_style = 0

    for i in range(len(coefs)):

        J_style += coefs[i]*J_layer_style[i]

        

    J_total = alpha*J_content + beta*J_style

    

    return J_total

        ","['Break the loop if it reaches the last layer of style loss', 'compute the content loss', 'coompute the style loss']",[],[],[],[],35.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],71,[],[],0,3,0,0,0,0,0.0,0,0,True
18,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '5. Define the optimizer']]",[],0,[],['## 5. Define the optimizer'],0,0,0,0,0,0,,0,5,False
19,code,[],"# Use Adam optimizer.

def get_optimizer(input_image):

    

    optimizer = torch.optim.Adam([input_image.requires_grad_()], lr = 0.001)

    return optimizer",['Use Adam optimizer.'],[],[],[],[],38.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],5,[],[],0,1,0,0,0,0,0.0,0,0,True
20,markdown,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],"[[2, '6. Run the model']]",[],0,[],['## 6. Run the model'],0,0,0,0,0,0,,0,5,False
21,code,[],"num_epochs = 1000



# Define the content image and style image.

content_image = image_loader('sea.jpg').unsqueeze(0).to(device)

style_image = image_loader('van.jpg').view(content_image.size()).to(device)

#input_image = torch.randn(style_image.data.size())



# Set the input image to be content image.

input_image = content_image



for epoch in range(num_epochs):

    epoch +=1

    input_image = Variable(input_image).to(device)

    

    input_image.data.clamp_(0, 1)   

    optimizer = get_optimizer(input_image)

    optimizer.zero_grad()

    J_content, J_layer_style, NST_model = get_loss_model(model, content_image, style_image, input_image,

                                                         c_layers = content_layers, s_layers = style_layers, 



                  mean= torch.tensor([0.485, 0.456, 0.406]).to(device), 

                  std= torch.tensor([0.229, 0.224, 0.225]).to(device))



    



    loss = total_loss(J_content, J_layer_style, coefs=[0.2,0.2,0.2,0.2,0.2], alpha = 10, beta= 400)

    loss.backward()

    optimizer.step()

    

    # Show the image every 100 epochs.

    if epoch%100 == 0:

        print('epoch:{}, loss:{}'.format(epoch, loss.data))

        img = input_image.to('cpu').clone().detach()

        img = img.squeeze()

        img = img.permute(1,2,0)

        img = np.array(img).clip(0,1)

        show(img)","['Define the content image and style image.', 'input_image = torch.randn(style_image.data.size())', 'Set the input image to be content image.', 'Show the image every 100 epochs.']","['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],39.0,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],37,[],[],0,4,10,0,0,0,0.0,11,0,True
22,code,[],,[],[],[],[],[],,sh2439..pytorch_proj..Computer_Vision..Neural Style Transfer..Neural_Style_Transfer.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
0,code,[],"%load_ext watermark

%watermark -v -p numpy,scipy,sklearn,pandas,matplotlib",[],[],[],[],[],1.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,1,0,True
1,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['**8장 – 차원 축소**'],0,0,0,0,0,0,,0,4,False
2,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['_이 노트북은 8장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다._'],0,0,0,0,0,0,,0,11,False
3,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, '설정']]",[],0,[],['# 설정'],0,0,0,0,0,0,,0,2,False
4,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['파이썬 2와 3을 모두 지원합니다. 공통 모듈을 임포트하고 맷플롯립 그림이 노트북 안에 포함되도록 설정하고 생성한 그림을 저장하기 위한 함수를 준비합니다:'],0,0,0,0,0,0,,0,20,False
5,code,[],"# 파이썬 2와 파이썬 3 지원

from __future__ import division, print_function, unicode_literals



# 공통

import numpy as np

import os



# 일관된 출력을 위해 유사난수 초기화

np.random.seed(42)



# 맷플롯립 설정

%matplotlib inline

import matplotlib

import matplotlib.pyplot as plt

plt.rcParams['axes.labelsize'] = 14

plt.rcParams['xtick.labelsize'] = 12

plt.rcParams['ytick.labelsize'] = 12



# 한글출력

plt.rcParams['font.family'] = 'NanumBarunGothic'

plt.rcParams['axes.unicode_minus'] = False



# 그림을 저장할 폴더

PROJECT_ROOT_DIR = "".""

CHAPTER_ID = ""dim_reduction""



def save_fig(fig_id, tight_layout=True):

    path = os.path.join(PROJECT_ROOT_DIR, ""images"", CHAPTER_ID, fig_id + "".png"")

    if tight_layout:

        plt.tight_layout()

    plt.savefig(path, format='png', dpi=300)","['파이썬 2와 파이썬 3 지원', '공통', '일관된 출력을 위해 유사난수 초기화', '맷플롯립 설정', '한글출력', '그림을 저장할 폴더']",[],[],[],[],2.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],31,[],[],0,6,0,0,0,0,0.0,0,0,True
6,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, '투영 방법']]",[],0,[],"['# 투영 방법\n', '3D 데이터셋을 만듭니다:']",0,0,0,0,0,0,,0,6,False
7,code,[],"np.random.seed(4)

m = 60

w1, w2 = 0.1, 0.3

noise = 0.1



angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5

X = np.empty((m, 3))

X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2

X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2

X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)",[],[],[],[],[],3.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],10,[],[],0,0,0,0,0,0,0.0,0,0,True
8,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, 'SVD 분해를 사용한 PCA']]",[],0,[],['## SVD 분해를 사용한 PCA'],0,0,0,0,0,0,,0,5,False
9,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['노트: `svd()` 함수는 `U`, `s` 그리고 `Vt`를 반환합니다. 여기에서 `Vt`는 행렬 $\\mathbf{V}$의 전치인 $\\mathbf{V}^T$입니다.']",0,0,0,0,0,0,,0,14,False
10,code,[],"X_centered = X - X.mean(axis=0)

U, s, Vt = np.linalg.svd(X_centered)

c1 = Vt.T[:, 0]

c2 = Vt.T[:, 1]",[],[],[],[],[],4.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
11,code,[],"m, n = X.shape



S = np.zeros(X_centered.shape)

S[:n, :n] = np.diag(s)",[],[],[],[],[],5.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
12,code,[],"np.allclose(X_centered, U.dot(S).dot(Vt))",[],[],[],[],['text/plain'],6.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
13,code,[],"W2 = Vt.T[:, :2]

X2D = X_centered.dot(W2)",[],[],[],[],[],7.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
14,code,[],X2D_using_svd = X2D,[],[],[],[],[],8.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
15,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '사이킷런을 사용한 PCA']]",[],0,[],['## 사이킷런을 사용한 PCA'],0,0,0,0,0,0,,0,4,False
16,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['사이킷런에서는 PCA가 아주 간단합니다. 데이터셋에서 평균을 빼는 작업도 대신 처리해 줍니다:'],0,0,0,0,0,0,,0,11,False
17,code,[],"from sklearn.decomposition import PCA



pca = PCA(n_components = 2)

X2D = pca.fit_transform(X)",[],[],[],[],[],9.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
18,code,[],X2D[:5],[],[],[],[],['text/plain'],10.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
19,code,[],X2D_using_svd[:5],[],[],[],[],['text/plain'],11.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
20,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['데이터셋을 조금 다르게해서 PCA를 실행하면 결과가 달라질 것입니다. 일반적으로 달라지는 것은 일부 축이 반대로 바뀌는 정도입니다. 이 예에서 사이킷런의 PCA는 두 축이 반대로 뒤집힌 것외에는 SVD 방식을 사용한 것과 통일한 투영 결과를 만듭니다:'],0,0,0,0,0,0,,0,33,False
21,code,[],"np.allclose(X2D, -X2D_using_svd)",[],[],[],[],['text/plain'],12.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
22,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['평면(PCA 2D 부분공간)에 투영된 3D 포인트를 복원합니다.'],0,0,0,0,0,0,,0,7,False
23,code,[],X3D_inv = pca.inverse_transform(X2D),[],[],[],[],[],13.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
24,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['물론, 투영 단게에서 일부 정보를 잃어버리기 때문에 복원된 3D 포인트가 원본 3D 포인트와 완전히 똑같지는 않습니다:']",0,0,0,0,0,0,,0,16,False
25,code,[],"np.allclose(X3D_inv, X)",[],[],[],[],['text/plain'],14.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
26,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['재구성 오차를 계산합니다:'],0,0,0,0,0,0,,0,3,False
27,code,[],"np.mean(np.sum(np.square(X3D_inv - X), axis=1))",[],[],[],[],['text/plain'],15.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
28,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['SVD 방식의 역변환은 다음과 같습니다:'],0,0,0,0,0,0,,0,5,False
29,code,[],"X3D_inv_using_svd = X2D_using_svd.dot(Vt[:2, :])",[],[],[],[],[],16.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
30,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['사이킷런의 `PCA` 클래스는 자동으로 평균을 뺏던 것을 복원해주기 때문에 두 방식의 재구성 오차가 동일하지는 않습니다. 하지만 평균을 빼면 동일한 재구성을 얻을 수 있습니다:'],0,0,0,0,0,0,,0,23,False
31,code,[],"np.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_)",[],[],[],[],['text/plain'],17.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
32,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['`PCA` 객체를 사용하여 계산된 주성분을 참조할 수 있습니다:'],0,0,0,0,0,0,,0,8,False
33,code,[],pca.components_,[],[],[],[],['text/plain'],18.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
34,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['SVD 방법으로 계산된 처음 두 개의 주성분과 비교해 보겠습니다:'],0,0,0,0,0,0,,0,9,False
35,code,[],Vt[:2],[],[],[],[],['text/plain'],19.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
36,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['축이 뒤집힌 것을 알 수 있습니다.'],0,0,0,0,0,0,,0,6,False
37,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이제 설명된 분산 비율을 확인해 보겠습니다:'],0,0,0,0,0,0,,0,6,False
38,code,[],pca.explained_variance_ratio_,[],[],[],[],['text/plain'],20.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
39,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['첫 번째 차원이 84.2%의 분산을 포함하고 있고 두 번째는 14.6%의 분산을 설명합니다.'],0,0,0,0,0,0,,0,12,False
40,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['2D로 투영했기 때문에 분산의 1.1%을 잃었습니다:'],0,0,0,0,0,0,,0,6,False
41,code,[],1 - pca.explained_variance_ratio_.sum(),[],[],[],[],['text/plain'],21.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
42,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['SVD 방식을 사용했을 때 설명된 분산의 비율을 계산하는 방법은 다음과 같습니다(`s`는 행렬 `S`의 대각 성분입니다):'],0,0,0,0,0,0,,0,15,False
43,code,[],np.square(s) / np.square(s).sum(),[],[],[],[],['text/plain'],22.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
44,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이를 그래프로 멋지게 그려보죠! :)'],0,0,0,0,0,0,,0,5,False
45,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,['http://stackoverflow.com/questions/11140163'],['3D 화살표를 그래기 위한 유틸리티 클래스입니다(http://stackoverflow.com/questions/11140163 에서 복사했습니다)'],0,0,0,0,0,0,,0,8,False
46,code,[],"from matplotlib.patches import FancyArrowPatch

from mpl_toolkits.mplot3d import proj3d



class Arrow3D(FancyArrowPatch):

    def __init__(self, xs, ys, zs, *args, **kwargs):

        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)

        self._verts3d = xs, ys, zs



    def draw(self, renderer):

        xs3d, ys3d, zs3d = self._verts3d

        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)

        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))

        FancyArrowPatch.draw(self, renderer)",[],[],[],[],[],23.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],13,[],[],0,0,0,0,0,0,0.0,0,0,True
47,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['x와 y의 함수로 평면을 표현합니다.'],0,0,0,0,0,0,,0,5,False
48,code,[],"axes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0]



x1s = np.linspace(axes[0], axes[1], 10)

x2s = np.linspace(axes[2], axes[3], 10)

x1, x2 = np.meshgrid(x1s, x2s)



C = pca.components_

R = C.T.dot(C)

z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2])",[],[],[],[],[],24.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,0,0,0,0,0,0.0,0,0,True
49,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['3D 데이터셋, 평면 그리고 이 평면으로의 투영을 그립니다.']",0,0,0,0,0,0,,0,8,False
50,code,[],"from mpl_toolkits.mplot3d import Axes3D



fig = plt.figure(figsize=(6, 3.8))

ax = fig.add_subplot(111, projection='3d')



X3D_above = X[X[:, 2] > X3D_inv[:, 2]]

X3D_below = X[X[:, 2] <= X3D_inv[:, 2]]



ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], ""bo"", alpha=0.5)



ax.plot_surface(x1, x2, z, alpha=0.2, color=""k"")

np.linalg.norm(C, axis=0)

ax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle=""-|>"", color=""k""))

ax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle=""-|>"", color=""k""))

ax.plot([0], [0], [0], ""k."")



for i in range(m):

    if X[i, 2] > X3D_inv[i, 2]:

        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], ""k-"")

    else:

        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], ""k-"", color=""#505050"")

    

ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], ""k+"")

ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], ""k."")

ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], ""bo"")

ax.set_xlabel(""$x_1$"", fontsize=18, labelpad=7)

ax.set_ylabel(""$x_2$"", fontsize=18, labelpad=7)

ax.set_zlabel(""$x_3$"", fontsize=18, labelpad=4)

ax.set_xlim(axes[0:2])

ax.set_ylim(axes[2:4])

ax.set_zlim(axes[4:6])



save_fig(""dataset_3d_plot"")

plt.show()","['505050"")']","['image/png', 'text/plain']",[],[],[],25.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],34,[],[],0,1,1,0,0,0,0.0,0,0,True
51,code,[],"fig = plt.figure()

ax = fig.add_subplot(111, aspect='equal')



ax.plot(X2D[:, 0], X2D[:, 1], ""k+"")

ax.plot(X2D[:, 0], X2D[:, 1], ""k."")

ax.plot([0], [0], ""ko"")

ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')

ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')

ax.set_xlabel(""$z_1$"", fontsize=18)

ax.set_ylabel(""$z_2$"", fontsize=18, rotation=0)

ax.axis([-1.5, 1.3, -1.2, 1.2])

ax.grid(True)

save_fig(""dataset_2d_plot"")",[],"['image/png', 'text/plain']",[],[],[],26.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],13,[],[],0,0,1,0,0,0,0.0,0,0,True
52,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, '매니폴드 학습']]",[],0,[],"['# 매니폴드 학습\n', '스위스 롤:']",0,0,0,0,0,0,,0,5,False
53,code,[],"from sklearn.datasets import make_swiss_roll

X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)",[],[],[],[],[],27.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
54,code,[],"axes = [-11.5, 14, -2, 23, -12, 15]



fig = plt.figure(figsize=(6, 5))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)

ax.view_init(10, -70)

ax.set_xlabel(""$x_1$"", fontsize=18, labelpad=7)

ax.set_ylabel(""$x_2$"", fontsize=18, labelpad=7)

ax.set_zlabel(""$x_3$"", fontsize=18)

ax.set_xlim(axes[0:2])

ax.set_ylim(axes[2:4])

ax.set_zlim(axes[4:6])



save_fig(""swiss_roll_plot"")

plt.show()",[],"['image/png', 'text/plain']",[],[],[],28.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],16,[],[],0,0,1,0,0,0,0.0,0,0,True
55,code,[],"plt.figure(figsize=(11, 4))



plt.subplot(121)

plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)

plt.axis(axes[:4])

plt.xlabel(""$x_1$"", fontsize=18)

plt.ylabel(""$x_2$"", fontsize=18, rotation=0, labelpad=10)

plt.grid(True)



plt.subplot(122)

plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)

plt.axis([4, 15, axes[2], axes[3]])

plt.xlabel(""$z_1$"", fontsize=18)

plt.grid(True)



save_fig(""squished_swiss_roll_plot"")

plt.show()",[],"['image/png', 'text/plain']",[],[],[],29.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],17,[],[],0,0,1,0,0,0,0.0,0,0,True
56,code,[],"from matplotlib import gridspec



axes = [-11.5, 14, -2, 23, -12, 15]



x2s = np.linspace(axes[2], axes[3], 10)

x3s = np.linspace(axes[4], axes[5], 10)

x2, x3 = np.meshgrid(x2s, x3s)



fig = plt.figure(figsize=(6, 5))

ax = plt.subplot(111, projection='3d')



positive_class = X[:, 0] > 5

X_pos = X[positive_class]

X_neg = X[~positive_class]

ax.view_init(10, -70)

ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], ""y^"")

ax.plot_wireframe(5, x2, x3, alpha=0.5)

ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], ""gs"")

ax.set_xlabel(""$x_1$"", fontsize=18, labelpad=7)

ax.set_ylabel(""$x_2$"", fontsize=18, labelpad=7)

ax.set_zlabel(""$x_3$"", fontsize=18)

ax.set_xlim(axes[0:2])

ax.set_ylim(axes[2:4])

ax.set_zlim(axes[4:6])



save_fig(""manifold_decision_boundary_plot1"")

plt.show()



fig = plt.figure(figsize=(5, 4))

ax = plt.subplot(111)



plt.plot(t[positive_class], X[positive_class, 1], ""gs"")

plt.plot(t[~positive_class], X[~positive_class, 1], ""y^"")

plt.axis([4, 15, axes[2], axes[3]])

plt.xlabel(""$z_1$"", fontsize=18)

plt.ylabel(""$z_2$"", fontsize=18, rotation=0, labelpad=7)

plt.grid(True)



save_fig(""manifold_decision_boundary_plot2"")

plt.show()



fig = plt.figure(figsize=(6, 5))

ax = plt.subplot(111, projection='3d')



positive_class = 2 * (t[:] - 4) > X[:, 1]

X_pos = X[positive_class]

X_neg = X[~positive_class]

ax.view_init(10, -70)

ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], ""y^"")

ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], ""gs"")

ax.set_xlabel(""$x_1$"", fontsize=18, labelpad=7)

ax.set_ylabel(""$x_2$"", fontsize=18, labelpad=7)

ax.set_zlabel(""$x_3$"", fontsize=18)

ax.set_xlim(axes[0:2])

ax.set_ylim(axes[2:4])

ax.set_zlim(axes[4:6])



save_fig(""manifold_decision_boundary_plot3"")

plt.show()



fig = plt.figure(figsize=(5, 4))

ax = plt.subplot(111)



plt.plot(t[positive_class], X[positive_class, 1], ""gs"")

plt.plot(t[~positive_class], X[~positive_class, 1], ""y^"")

plt.plot([4, 15], [0, 22], ""b-"", linewidth=2)

plt.axis([4, 15, axes[2], axes[3]])

plt.xlabel(""$z_1$"", fontsize=18)

plt.ylabel(""$z_2$"", fontsize=18, rotation=0, labelpad=7)

plt.grid(True)



save_fig(""manifold_decision_boundary_plot4"")

plt.show()",[],"['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],30.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],73,[],[],0,0,4,0,0,0,0.0,0,0,True
57,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, 'PCA']]",[],0,[],['# PCA'],0,0,0,0,0,0,,0,2,False
58,code,[],"angle = np.pi / 5

stretch = 5

m = 200



np.random.seed(3)

X = np.random.randn(m, 2) / 10

X = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch

X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate



u1 = np.array([np.cos(angle), np.sin(angle)])

u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])

u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])



X_proj1 = X.dot(u1.reshape(-1, 1))

X_proj2 = X.dot(u2.reshape(-1, 1))

X_proj3 = X.dot(u3.reshape(-1, 1))



plt.figure(figsize=(8,4))

plt.subplot2grid((3,2), (0, 0), rowspan=3)

plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], ""k-"", linewidth=1)

plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], ""k--"", linewidth=1)

plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], ""k:"", linewidth=2)

plt.plot(X[:, 0], X[:, 1], ""bo"", alpha=0.5)

plt.axis([-1.4, 1.4, -1.4, 1.4])

plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')

plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')

plt.text(u1[0] + 0.1, u1[1] - 0.05, r""$\mathbf{c_1}$"", fontsize=22)

plt.text(u3[0] + 0.1, u3[1], r""$\mathbf{c_2}$"", fontsize=22)

plt.xlabel(""$x_1$"", fontsize=18)

plt.ylabel(""$x_2$"", fontsize=18, rotation=0)

plt.grid(True)



plt.subplot2grid((3,2), (0, 1))

plt.plot([-2, 2], [0, 0], ""k-"", linewidth=1)

plt.plot(X_proj1[:, 0], np.zeros(m), ""bo"", alpha=0.3)

plt.gca().get_yaxis().set_ticks([])

plt.gca().get_xaxis().set_ticklabels([])

plt.axis([-2, 2, -1, 1])

plt.grid(True)



plt.subplot2grid((3,2), (1, 1))

plt.plot([-2, 2], [0, 0], ""k--"", linewidth=1)

plt.plot(X_proj2[:, 0], np.zeros(m), ""bo"", alpha=0.3)

plt.gca().get_yaxis().set_ticks([])

plt.gca().get_xaxis().set_ticklabels([])

plt.axis([-2, 2, -1, 1])

plt.grid(True)



plt.subplot2grid((3,2), (2, 1))

plt.plot([-2, 2], [0, 0], ""k:"", linewidth=2)

plt.plot(X_proj3[:, 0], np.zeros(m), ""bo"", alpha=0.3)

plt.gca().get_yaxis().set_ticks([])

plt.axis([-2, 2, -1, 1])

plt.xlabel(""$z_1$"", fontsize=18)

plt.grid(True)



save_fig(""pca_best_projection"")

plt.show()","['stretch', 'rotate']","['image/png', 'text/plain']",[],[],[],31.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],58,[],[],0,2,1,0,0,0,0.0,0,0,True
59,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, 'MNIST 압축']]",[],0,[],['# MNIST 압축'],0,0,0,0,0,0,,0,3,False
60,code,[],"from six.moves import urllib

from sklearn.datasets import fetch_mldata

mnist = fetch_mldata('MNIST original')",[],[],[],[],[],32.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
61,code,[],"from sklearn.model_selection import train_test_split



X = mnist[""data""]

y = mnist[""target""]



X_train, X_test, y_train, y_test = train_test_split(X, y)",[],[],[],[],[],33.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],6,[],[],0,0,0,0,0,0,0.0,0,0,True
62,code,[],"pca = PCA()

pca.fit(X_train)

cumsum = np.cumsum(pca.explained_variance_ratio_)

d = np.argmax(cumsum >= 0.95) + 1",[],[],[],[],[],34.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
63,code,[],d,[],[],[],[],['text/plain'],35.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
64,code,[],"pca = PCA(n_components=0.95)

X_reduced = pca.fit_transform(X_train)",[],[],[],[],[],36.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
65,code,[],pca.n_components_,[],[],[],[],['text/plain'],37.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
66,code,[],np.sum(pca.explained_variance_ratio_),[],[],[],[],['text/plain'],38.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
67,code,[],"pca = PCA(n_components = 154)

X_reduced = pca.fit_transform(X_train)

X_recovered = pca.inverse_transform(X_reduced)",[],[],[],[],[],39.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
68,code,[],"def plot_digits(instances, images_per_row=5, **options):

    size = 28

    images_per_row = min(len(instances), images_per_row)

    images = [instance.reshape(size,size) for instance in instances]

    n_rows = (len(instances) - 1) // images_per_row + 1

    row_images = []

    n_empty = n_rows * images_per_row - len(instances)

    images.append(np.zeros((size, size * n_empty)))

    for row in range(n_rows):

        rimages = images[row * images_per_row : (row + 1) * images_per_row]

        row_images.append(np.concatenate(rimages, axis=1))

    image = np.concatenate(row_images, axis=0)

    plt.imshow(image, cmap = matplotlib.cm.binary, **options)

    plt.axis(""off"")",[],[],[],[],[],40.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],14,[],[],0,0,0,0,0,0,0.0,0,0,True
69,code,[],"plt.figure(figsize=(7, 4))

plt.subplot(121)

plot_digits(X_train[::2100])

plt.title(""원본"", fontsize=16)

plt.subplot(122)

plot_digits(X_recovered[::2100])

plt.title(""압축 후 복원"", fontsize=16)



save_fig(""mnist_compression_plot"")",[],"['image/png', 'text/plain']",[],[],[],41.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,0,1,0,0,0,0.0,0,0,True
70,code,[],X_reduced_pca = X_reduced,[],[],[],[],[],42.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
71,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '점진적 PCA']]",[],0,[],['## 점진적 PCA'],0,0,0,0,0,0,,0,3,False
72,code,[],"from sklearn.decomposition import IncrementalPCA



n_batches = 100

inc_pca = IncrementalPCA(n_components=154)

for X_batch in np.array_split(X_train, n_batches):

    print(""."", end="""") # not shown in the book

    inc_pca.partial_fit(X_batch)



X_reduced = inc_pca.transform(X_train)",['not shown in the book'],[],[],[],[],43.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,1,0,0,0,0,0.0,1,0,True
73,code,[],X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced),[],[],[],[],[],44.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
74,code,[],"plt.figure(figsize=(7, 4))

plt.subplot(121)

plot_digits(X_train[::2100])

plt.subplot(122)

plot_digits(X_recovered_inc_pca[::2100])

plt.tight_layout()",[],"['image/png', 'text/plain']",[],[],[],45.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],6,[],[],0,0,1,0,0,0,0.0,0,0,True
75,code,[],X_reduced_inc_pca = X_reduced,[],[],[],[],[],46.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
76,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['일반 PCA와 점진적 PCA로 MNIST 데이터를 변환한 결과를 비교해 보겠습니다. 먼저 평균이 같은지 확인합니다: '],0,0,0,0,0,0,,0,14,False
77,code,[],"np.allclose(pca.mean_, inc_pca.mean_)",[],[],[],[],['text/plain'],47.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
78,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['하지만 결과는 완전히 동일하지 않습니다. 점진적 PCA는 아주 훌륭한 근사치를 제공하지만 완벽하지는 않습니다:'],0,0,0,0,0,0,,0,13,False
79,code,[],"np.allclose(X_reduced_pca, X_reduced_inc_pca)",[],[],[],[],['text/plain'],48.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
80,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[3, '`memmap()` 사용하기']]",[],0,[],['### `memmap()` 사용하기'],0,0,0,0,0,0,,0,3,False
81,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['`memmap()` 구조를 만들고 MNIST 데이터를 복사합니다. 이는 일반적으로 별도의 프로그램에서 먼저 수행됩니다:'],0,0,0,0,0,0,,0,12,False
82,code,[],"filename = ""my_mnist.data""

m, n = X_train.shape



X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))

X_mm[:] = X_train",[],[],[],[],[],49.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
83,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이제 `memmap()` 객체를 삭제하면 파이썬 종결자(finalizer)를 호출해서 데이터를 디스크에 저장하게 됩니다.'],0,0,0,0,0,0,,0,11,False
84,code,[],del X_mm,[],[],[],[],[],50.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
85,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['다음에 다른 프로그램에서 데이터를 로드하여 훈련에 사용합니다:'],0,0,0,0,0,0,,0,7,False
86,code,[],"X_mm = np.memmap(filename, dtype=""float32"", mode=""readonly"", shape=(m, n))



batch_size = m // n_batches

inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)

inc_pca.fit(X_mm)",[],[],[],[],['text/plain'],51.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,0,0,1,0,0.0,0,0,True
87,code,[],"rnd_pca = PCA(n_components=154, svd_solver=""randomized"", random_state=42)

X_reduced = rnd_pca.fit_transform(X_train)",[],[],[],[],[],52.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
88,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '시간 복잡도']]",[],0,[],['## 시간 복잡도'],0,0,0,0,0,0,,0,3,False
89,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['주성분 개수를 바꾸어가며 점진적 PCA와 랜덤 PCA에 비해 일반 PCA 시간을 재어보겠습니다:'],0,0,0,0,0,0,,0,12,False
90,code,[],"import time



for n_components in (2, 10, 154):

    print(""n_components ="", n_components)

    regular_pca = PCA(n_components=n_components)

    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)

    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=""randomized"")



    for pca in (regular_pca, inc_pca, rnd_pca):

        t1 = time.time()

        pca.fit(X_train)

        t2 = time.time()

        print(""    {}: {:.1f} seconds"".format(pca.__class__.__name__, t2 - t1))",[],[],[],[],[],53.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],13,[],[],0,0,0,0,0,0,0.0,1,0,True
91,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이번에는 데이터셋의 크기(샘플의 수)를 바꾸어가며 일반 PCA와 랜덤 PCA를 비교해 보겠습니다:'],0,0,0,0,0,0,,0,11,False
92,code,[],"times_rpca = []

times_pca = []

sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]

for n_samples in sizes:

    X = np.random.randn(n_samples, 5)

    pca = PCA(n_components = 2, svd_solver=""randomized"", random_state=42)

    t1 = time.time()

    pca.fit(X)

    t2 = time.time()

    times_rpca.append(t2 - t1)

    pca = PCA(n_components = 2)

    t1 = time.time()

    pca.fit(X)

    t2 = time.time()

    times_pca.append(t2 - t1)



plt.plot(sizes, times_rpca, ""b-o"", label=""RPCA"")

plt.plot(sizes, times_pca, ""r-s"", label=""PCA"")

plt.xlabel(""n_samples"")

plt.ylabel(""훈련 시간"")

plt.legend(loc=""upper left"")

plt.title(""PCA와 랜덤 PCA의 시간 복잡도"")",[],"['image/png', 'text/plain']",[],[],['text/plain'],54.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],22,[],[],0,0,1,0,1,0,0.0,0,0,True
93,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['이번에는 특성의 개수를 달리하면서 2,000 샘플이 있는 데이터셋에서 성능을 비교해 보겠습니다:']",0,0,0,0,0,0,,0,11,False
94,code,[],"times_rpca = []

times_pca = []

sizes = [1000, 2000, 3000, 4000, 5000, 6000]

for n_features in sizes:

    X = np.random.randn(2000, n_features)

    pca = PCA(n_components = 2, random_state=42, svd_solver=""randomized"")

    t1 = time.time()

    pca.fit(X)

    t2 = time.time()

    times_rpca.append(t2 - t1)

    pca = PCA(n_components = 2)

    t1 = time.time()

    pca.fit(X)

    t2 = time.time()

    times_pca.append(t2 - t1)



plt.plot(sizes, times_rpca, ""b-o"", label=""RPCA"")

plt.plot(sizes, times_pca, ""r-s"", label=""PCA"")

plt.xlabel(""n_features"")

plt.ylabel(""훈련 시간"")

plt.legend(loc=""upper left"")

plt.title(""PCA와 Randomized PCA의 시간 복잡도"")",[],"['image/png', 'text/plain']",[],[],['text/plain'],55.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],22,[],[],0,0,1,0,1,0,0.0,0,0,True
95,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, '커널 PCA']]",[],0,[],['# 커널 PCA'],0,0,0,0,0,0,,0,3,False
96,code,[],"X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)",[],[],[],[],[],56.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
97,code,[],"from sklearn.decomposition import KernelPCA



rbf_pca = KernelPCA(n_components = 2, kernel=""rbf"", gamma=0.04)

X_reduced = rbf_pca.fit_transform(X)",[],[],[],[],[],57.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
98,code,[],"from sklearn.decomposition import KernelPCA



lin_pca = KernelPCA(n_components = 2, kernel=""linear"", fit_inverse_transform=True)

rbf_pca = KernelPCA(n_components = 2, kernel=""rbf"", gamma=0.0433, fit_inverse_transform=True)

sig_pca = KernelPCA(n_components = 2, kernel=""sigmoid"", gamma=0.001, coef0=1, fit_inverse_transform=True)



y = t > 6.9



plt.figure(figsize=(11, 4))

for subplot, pca, title in ((131, lin_pca, ""선형 커널""), (132, rbf_pca, ""RBF 커널, $\gamma=0.04$""), (133, sig_pca, ""시그모이드 커널, $\gamma=10^{-3}, r=1$"")):

    X_reduced = pca.fit_transform(X)

    if subplot == 132:

        X_reduced_rbf = X_reduced

    

    plt.subplot(subplot)

    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], ""gs"")

    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], ""y^"")

    plt.title(title, fontsize=14)

    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)

    plt.xlabel(""$z_1$"", fontsize=18)

    if subplot == 131:

        plt.ylabel(""$z_2$"", fontsize=18, rotation=0)

    plt.grid(True)



save_fig(""kernel_pca_plot"")

plt.show()","['plt.plot(X_reduced[y, 0], X_reduced[y, 1], ""gs"")', 'plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], ""y^"")']","['image/png', 'text/plain']",[],[],[],58.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],26,[],[],0,2,1,0,0,0,0.0,0,0,True
99,code,[],"plt.figure(figsize=(6, 5))



X_inverse = rbf_pca.inverse_transform(X_reduced_rbf)



ax = plt.subplot(111, projection='3d')

ax.view_init(10, -70)

ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker=""x"")

ax.set_xlabel("""")

ax.set_ylabel("""")

ax.set_zlabel("""")

ax.set_xticklabels([])

ax.set_yticklabels([])

ax.set_zticklabels([])



save_fig(""preimage_plot"", tight_layout=False)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],59.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],16,[],[],0,0,1,0,0,0,0.0,0,0,True
100,code,[],"X_reduced = rbf_pca.fit_transform(X)



plt.figure(figsize=(11, 4))

plt.subplot(132)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot, marker=""x"")

plt.xlabel(""$z_1$"", fontsize=18)

plt.ylabel(""$z_2$"", fontsize=18, rotation=0)

plt.grid(True)",[],"['image/png', 'text/plain']",[],[],[],60.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],8,[],[],0,0,1,0,0,0,0.0,0,0,True
101,code,[],"from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline



clf = Pipeline([

        (""kpca"", KernelPCA(n_components=2)),

        (""log_reg"", LogisticRegression())

    ])



param_grid = [{

        ""kpca__gamma"": np.linspace(0.03, 0.05, 10),

        ""kpca__kernel"": [""rbf"", ""sigmoid""]

    }]



grid_search = GridSearchCV(clf, param_grid, cv=3)

grid_search.fit(X, y)",[],[],[],[],['text/plain'],61.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],16,[],[],0,0,0,0,1,0,0.0,0,0,True
102,code,[],print(grid_search.best_params_),[],[],[],[],[],62.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
103,code,[],"rbf_pca = KernelPCA(n_components = 2, kernel=""rbf"", gamma=0.0433,

                    fit_inverse_transform=True)

X_reduced = rbf_pca.fit_transform(X)

X_preimage = rbf_pca.inverse_transform(X_reduced)",[],[],[],[],[],63.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
104,code,[],"from sklearn.metrics import mean_squared_error



mean_squared_error(X, X_preimage)",[],[],[],[],['text/plain'],64.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],3,[],[],0,0,0,0,1,0,0.0,0,0,True
105,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, 'LLE']]",[],0,[],['# LLE'],0,0,0,0,0,0,,0,2,False
106,code,[],"X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)",[],[],[],[],[],65.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
107,code,[],"from sklearn.manifold import LocallyLinearEmbedding



lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)

X_reduced = lle.fit_transform(X)",[],[],[],[],[],66.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
108,code,[],"plt.title(""LLE를 사용하여 펼쳐진 스위스 롤"", fontsize=14)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)

plt.xlabel(""$z_1$"", fontsize=18)

plt.ylabel(""$z_2$"", fontsize=18)

plt.axis([-0.065, 0.055, -0.1, 0.12])

plt.grid(True)



save_fig(""lle_unrolling_plot"")

plt.show()",[],"['image/png', 'text/plain']",[],[],[],67.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,0,1,0,0,0,0.0,0,0,True
109,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, 'MDS, Isomap 그리고 t-SNE']]",[],0,[],"['# MDS, Isomap 그리고 t-SNE']",0,0,0,0,0,0,,0,5,False
110,code,[],"from sklearn.manifold import MDS



mds = MDS(n_components=2, random_state=42)

X_reduced_mds = mds.fit_transform(X)",[],[],[],[],[],68.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
111,code,[],"from sklearn.manifold import Isomap



isomap = Isomap(n_components=2)

X_reduced_isomap = isomap.fit_transform(X)",[],[],[],[],[],69.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
112,code,[],"from sklearn.manifold import TSNE



tsne = TSNE(n_components=2, random_state=42)

X_reduced_tsne = tsne.fit_transform(X)",[],[],[],[],[],70.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
113,code,[],"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis



lda = LinearDiscriminantAnalysis(n_components=2)

X_mnist = mnist[""data""]

y_mnist = mnist[""target""]

lda.fit(X_mnist, y_mnist)

X_reduced_lda = lda.transform(X_mnist)",[],[],[],[],[],71.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],7,[],[],0,0,0,0,0,0,0.0,1,0,True
114,code,[],"titles = [""MDS"", ""Isomap"", ""t-SNE""]



plt.figure(figsize=(11,4))



for subplot, title, X_reduced in zip((131, 132, 133), titles,

                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):

    plt.subplot(subplot)

    plt.title(title, fontsize=14)

    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)

    plt.xlabel(""$z_1$"", fontsize=18)

    if subplot == 131:

        plt.ylabel(""$z_2$"", fontsize=18, rotation=0)

    plt.grid(True)



save_fig(""other_dim_reduction_plot"")

plt.show()",[],"['image/png', 'text/plain']",[],[],[],72.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],16,[],[],0,0,1,0,0,0,0.0,0,0,True
115,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[1, '연습문제 해답']]",[],0,[],['# 연습문제 해답'],0,0,0,0,0,0,,0,3,False
116,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '1. to 8.']]",[],0,[],['## 1. to 8.'],0,0,0,0,0,0,,0,4,False
117,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['부록 A 참조.'],0,0,0,0,0,0,,0,3,False
118,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '9.']]",[],0,[],['## 9.'],0,0,0,0,0,0,,0,2,False
119,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['*문제: (3장에서 소개한) MNIST 데이터셋을 로드하고 훈련 세트와 테스트 세트로 분할합니다(처음 60,000개는 훈련을 위한 샘플이고 나머지 10,000개는 테스트용입니다).*']",0,0,0,0,0,0,,0,18,False
120,code,[],"from sklearn.datasets import fetch_mldata

mnist = fetch_mldata('MNIST original')",[],[],[],[],[],73.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
121,code,[],"X_train = mnist['data'][:60000]

y_train = mnist['target'][:60000]



X_test = mnist['data'][60000:]

y_test = mnist['target'][60000:]",[],[],[],[],[],74.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
122,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['*문제: 이 데이터셋에 랜덤 포레스트 분류기를 훈련시키고 얼마나 오래 걸리는지 시간을 잰 다음, 테스트 세트로 만들어진 모델을 평가합니다.*']",0,0,0,0,0,0,,0,18,False
123,code,[],"from sklearn.ensemble import RandomForestClassifier



rnd_clf = RandomForestClassifier(random_state=42)",[],[],[],[],[],75.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
124,code,[],"import time



t0 = time.time()

rnd_clf.fit(X_train, y_train)

t1 = time.time()",[],[],[],[],[],76.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
125,code,[],"print(""훈련 시간 {:.2f}s"".format(t1 - t0))",[],[],[],[],[],77.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
126,code,[],"from sklearn.metrics import accuracy_score



y_pred = rnd_clf.predict(X_test)

accuracy_score(y_test, y_pred)",[],[],[],[],['text/plain'],78.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,1,0,0.0,0,0,True
127,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['*문제: 그런 다음 PCA를 사용해 설명된 분산이 95%가 되도록 차원을 축소합니다.*'],0,0,0,0,0,0,,0,11,False
128,code,[],"from sklearn.decomposition import PCA



pca = PCA(n_components=0.95)

X_train_reduced = pca.fit_transform(X_train)",[],[],[],[],[],79.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
129,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['*문제: 이 축소된 데이터셋에 새로운 랜덤 포레스트 분류기를 훈련시키고 얼마나 오래 걸리는지 확인합니다. 훈련 속도가 더 빨라졌나요?*'],0,0,0,0,0,0,,0,17,False
130,code,[],"rnd_clf2 = RandomForestClassifier(random_state=42)

t0 = time.time()

rnd_clf2.fit(X_train_reduced, y_train)

t1 = time.time()",[],[],[],[],[],80.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
131,code,[],"print(""훈련 시간 {:.2f}s"".format(t1 - t0))",[],[],[],[],[],81.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
132,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['이런! 훈련이 두 배 이상 느려졌습니다! 어떻게 이럴 수 있죠? 이 장에서 보았듯이 차원 축소는 언제나 훈련 시간을 줄여주지 못합니다. 데이터셋, 모델, 훈련 알고리즘에 따라 달라집니다. 그림 8-6을 참고하세요. 랜덤 포레스트 분류기 대신 소프트맥스 분류기를 적용하면 PCA를 사용해서 훈련 시간을 3배나 줄일 수 있습니다. 잠시 후에 실제로 한번 해보겠습니다. 하지만 먼저 새로운 랜덤 포레스트 분류기의 정밀도를 확인해 보죠.']",0,0,0,0,0,0,,0,58,False
133,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['*문제: 이제 테스트 세트에서 이 분류기를 평가해보세요. 이전 분류기와 비교해서 어떤가요?*'],0,0,0,0,0,0,,0,11,False
134,code,[],"X_test_reduced = pca.transform(X_test)



y_pred = rnd_clf2.predict(X_test_reduced)

accuracy_score(y_test, y_pred)",[],[],[],[],['text/plain'],82.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,1,0,0.0,0,0,True
135,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['차원 축소를 했을 때 유용한 정보를 일부 잃었기 때문에 성능이 조금 감소되는 것이 일반적입니다. 그렇지만 이 경우에는 성능 감소가 좀 심각한 것 같습니다. PCA가 별로 도움이 되지 않네요. 훈련 시간도 느려지고 성능도 감소했습니다. :(\n', '\n', '소프트맥스 회귀를 사용하면 도움이 되는지 확인해 보겠습니다:']",0,0,0,0,0,0,,0,41,False
136,code,[],"from sklearn.linear_model import LogisticRegression



log_clf = LogisticRegression(multi_class=""multinomial"", solver=""lbfgs"", random_state=42)

t0 = time.time()

log_clf.fit(X_train, y_train)

t1 = time.time()",[],[],[],[],[],83.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],6,[],[],0,0,0,0,0,0,0.0,0,0,True
137,code,[],"print(""훈련 시간: {:.2f}s"".format(t1 - t0))",[],[],[],[],[],84.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
138,code,[],"y_pred = log_clf.predict(X_test)

accuracy_score(y_test, y_pred)",[],[],[],[],['text/plain'],85.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
139,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['좋네요. 소프트맥스 회귀는 랜덤 포레스트 분류기보다 이 데이터셋에서 훈련하는데 더 많은 시간이 걸리고 테스트 세트에서의 성능도 더 나쁩니다. 하지만 지금 관심 사항은 아닙니다. PCA가 소프트맥스 회귀에 얼마나 도움이 되는지가 궁금합니다. 축소된 데이터셋에 소프트맥스 회귀 모델을 훈련시켜 보겠습니다:'],0,0,0,0,0,0,,0,37,False
140,code,[],"log_clf2 = LogisticRegression(multi_class=""multinomial"", solver=""lbfgs"", random_state=42)

t0 = time.time()

log_clf2.fit(X_train_reduced, y_train)

t1 = time.time()",[],[],[],[],[],86.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
141,code,[],"print(""훈련 시간: {:.2f}s"".format(t1 - t0))",[],[],[],[],[],87.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,True
142,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['와우! 차원 축소가 속도를 4배나 빠르게 만들었습니다. :) 모델의 정확도를 확인해 보겠습니다:'],0,0,0,0,0,0,,0,12,False
143,code,[],"y_pred = log_clf2.predict(X_test_reduced)

accuracy_score(y_test, y_pred)",[],[],[],[],['text/plain'],88.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
144,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['성능이 조금 감소되었지만 애플리케이션에 따라서 4배 속도 향상의 댓가로 적절한 것 같습니다.'],0,0,0,0,0,0,,0,12,False
145,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['여기서 알 수 있는 것: PCA는 속도를 아주 빠르게 만들어 주지만 항상 그런 것은 아니다!'],0,0,0,0,0,0,,0,15,False
146,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],"[[2, '10.']]",[],0,[],['## 10.'],0,0,0,0,0,0,,0,2,False
147,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['*문제: t-SNE 알고리즘을 사용해 MNIST 데이터셋을 2차원으로 축소시키고 맷플롯립으로 그래프를 그려보세요. 이미지의 타깃 클래스마다 10가지 색깔로 나타낸 산점도를 그릴 수 있습니다.*'],0,0,0,0,0,0,,0,21,False
148,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['MNIST 데이터셋을 (다시) 로드합니다:'],0,0,0,0,0,0,,0,4,False
149,code,[],"from sklearn.datasets import fetch_mldata



mnist = fetch_mldata('MNIST original')",[],[],[],[],[],89.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
150,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['전체 60,000개의 이미지에 차원 축소를 하면 매우 오랜 시간이 걸리므로 10,000개의 이미지만 무작위로 선택하여 사용하겠습니다:']",0,0,0,0,0,0,,0,15,False
151,code,[],"np.random.seed(42)



m = 10000

idx = np.random.permutation(60000)[:m]



X = mnist['data'][idx]

y = mnist['target'][idx]",[],[],[],[],[],90.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],7,[],[],0,0,0,0,0,0,0.0,0,0,True
152,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이제 t-SNE를 사용해 2D로 차원을 축소해 그래프로 나타냅니다:'],0,0,0,0,0,0,,0,8,False
153,code,[],"from sklearn.manifold import TSNE



tsne = TSNE(n_components=2, random_state=42)

X_reduced = tsne.fit_transform(X)",[],[],[],[],[],91.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
154,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['산점도를 그리기 위해 맷플롯립의 `scatter()` 함수를 사용합니다. 각 숫자마다 다른 색깔을 사용합니다:'],0,0,0,0,0,0,,0,12,False
155,code,[],"plt.figure(figsize=(13,10))

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=""jet"")

plt.axis('off')

plt.colorbar()

plt.show()",[],"['image/png', 'text/plain']",[],[],[],92.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,1,0,0,0,0.0,0,0,True
156,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['아름답지 않나요? :) 이 그래프는 어떤 숫자가 다른 것과 구분이 쉬운지 알려 줍니다(가령, 0, 6 등이 잘 구분되어 있습니다). 그리고 어떤 숫자가 구분이 어려운지 알려 줍니다(가령, 4, 9입니다).']",0,0,0,0,0,0,,0,28,False
157,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['많이 겹쳐진 것 같은 숫자 4과 9에 집중해 보겠습니다.'],0,0,0,0,0,0,,0,9,False
158,code,[],"plt.figure(figsize=(9,9))

cmap = matplotlib.cm.get_cmap(""jet"")

for digit in (4, 9):

    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=cmap(digit / 9))

plt.axis('off')

plt.show()",[],"['image/png', 'text/plain']",[],[],[],93.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],6,[],[],0,0,1,0,0,0,0.0,0,0,True
159,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['이 두 개의 숫자에 t-SNE를 실행시켜 더 나은 이미지를 만들 수 있는지 보겠습니다:'],0,0,0,0,0,0,,0,13,False
160,code,[],"idx = (y == 4) | (y == 9)

X_subset = X[idx]

y_subset = y[idx]



tsne_subset = TSNE(n_components=2, random_state=42)

X_subset_reduced = tsne_subset.fit_transform(X_subset)",[],[],[],[],[],94.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],6,[],[],0,0,0,0,0,0,0.0,0,0,True
161,code,[],"plt.figure(figsize=(9,9))

for digit in (4, 9):

    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=cmap(digit / 9))

plt.axis('off')

plt.show()",[],"['image/png', 'text/plain']",[],[],[],95.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],5,[],[],0,0,1,0,0,0,0.0,0,0,True
162,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['훨씬 좋네요. 이제 군집이 덜 겹쳐졌습니다. 하지만 숫자 4가 두 개의 군집으로 나뉘어져 있습니다. 각 군집에 숫자를 몇 개씩 나타내면 훨씬 이해하는기 좋을 것 같습니다. 그렇게 한번 해보죠.'],0,0,0,0,0,0,,0,28,False
163,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['*문제: 또는 샘플의 위치에 각기 다른 색깔의 숫자를 나타낼 수도 있고, 숫자 이미지 자체의 크기를 줄여서 그릴 수도 있습니다(모든 숫자를 다 그리면 그래프가 너무 복잡해지므로 무작위로 선택한 샘플만 그리거나, 인접한 곳에 다른 샘플이 그려져 있지 않은 경우에만 그립니다). 잘 분리된 숫자의 군집을 시각화할 수 있을 것입니다.*']",0,0,0,0,0,0,,0,46,False
164,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,"['http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html),', ('plot_lle_digits', 'http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html')]","['산점도와 색깔있는 숫자를 쓰기위해 `plot_digits()` 함수를 만듭니다. 이 숫자 사이의 거리가 최소가 되도록 합니다. 숫자 이미지가 있다면 대신 이를 사용합니다. 이 코드는 사이킷런의 훌륭한 데모([plot_lle_digits](http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html), 데이터셋은 다릅니다)를 참고했습니다.']",0,0,0,0,0,0,,0,28,False
165,code,[],"from sklearn.preprocessing import MinMaxScaler

from matplotlib.offsetbox import AnnotationBbox, OffsetImage



def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):

    # 입력 특성의 스케일을 0에서 1 사이로 만듭니다.

    X_normalized = MinMaxScaler().fit_transform(X)

    # 그릴 숫자의 좌표 목록을 만듭니다.

    # 반복문 아래에서 `if` 문장을 쓰지 않기 위해 시작할 때 이미 그래프가 그려져 있다고 가정합니다.

    neighbors = np.array([[10., 10.]])

    # 나머지는 이해하기 쉽습니다.

    plt.figure(figsize=figsize)

    cmap = matplotlib.cm.get_cmap(""jet"")

    digits = np.unique(y)

    for digit in digits:

        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=cmap(digit / 9))

    plt.axis(""off"")

    ax = plt.gcf().gca()  # 현재 그래프의 축을 가져옵니다.

    for index, image_coord in enumerate(X_normalized):

        closest_distance = np.linalg.norm(np.array(neighbors) - image_coord, axis=1).min()

        if closest_distance > min_distance:

            neighbors = np.r_[neighbors, [image_coord]]

            if images is None:

                plt.text(image_coord[0], image_coord[1], str(int(y[index])),

                         color=cmap(y[index] / 9), fontdict={""weight"": ""bold"", ""size"": 16})

            else:

                image = images[index].reshape(28, 28)

                imagebox = AnnotationBbox(OffsetImage(image, cmap=""binary""), image_coord)

                ax.add_artist(imagebox)","['입력 특성의 스케일을 0에서 1 사이로 만듭니다.', '그릴 숫자의 좌표 목록을 만듭니다.', '반복문 아래에서 `if` 문장을 쓰지 않기 위해 시작할 때 이미 그래프가 그려져 있다고 가정합니다.', '나머지는 이해하기 쉽습니다.', '현재 그래프의 축을 가져옵니다.']",[],[],[],[],96.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],28,[],[],0,5,0,0,0,0,0.0,0,0,True
166,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['시작해 보죠! 먼저 색깔이 입혀진 숫자를 써 보겠습니다:'],0,0,0,0,0,0,,0,8,False
167,code,[],"plot_digits(X_reduced, y)",[],"['image/png', 'text/plain']",[],[],[],97.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,1,0,0,0,0.0,0,0,True
168,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['꽤 좋습니다. 하지만 아름답지는 않네요. 숫자 이미지를 사용해 보겠습니다:'],0,0,0,0,0,0,,0,9,False
169,code,[],"plot_digits(X_reduced, y, images=X, figsize=(35, 25))",[],"['image/png', 'text/plain']",[],[],[],98.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,1,0,0,0,0.0,0,0,True
170,code,[],"plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))",[],"['image/png', 'text/plain']",[],[],[],99.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],1,[],[],0,0,1,0,0,0,0.0,0,0,True
171,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['*문제: PCA, LLE, MDS 같은 차원 축소 알고리즘을 적용해보고 시각화 결과를 비교해보세요.*']",0,0,0,0,0,0,,0,12,False
172,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['PCA부터 시작해 보죠. 얼마나 오래 걸리는지도 재어 보겠습니다:'],0,0,0,0,0,0,,0,8,False
173,code,[],"from sklearn.decomposition import PCA

import time



t0 = time.time()

X_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)

t1 = time.time()

print(""PCA 시간: {:.1f}s."".format(t1 - t0))

plot_digits(X_pca_reduced, y)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],100.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,0,1,0,0,0,0.0,1,0,True
174,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['와우, PCA가 아주 빠르네요! 몇 개의 군집이 보이지만 너무 겹쳐져 있습니다. LLE를 사용해 보죠:']",0,0,0,0,0,0,,0,14,False
175,code,[],"from sklearn.manifold import LocallyLinearEmbedding



t0 = time.time()

X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)

t1 = time.time()

print(""LLE 시간: {:.1f}s."".format(t1 - t0))

plot_digits(X_lle_reduced, y)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],101.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],8,[],[],0,0,1,0,0,0,0.0,1,0,True
176,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['시간이 좀 걸리고 결과도 아주 좋지는 않습니다. 분산의 95%를 보존하도록 먼저 PCA를 적용하면 어떻게 되는지 보겠습니다:'],0,0,0,0,0,0,,0,16,False
177,code,[],"from sklearn.pipeline import Pipeline



pca_lle = Pipeline([

    (""pca"", PCA(n_components=0.95, random_state=42)),

    (""lle"", LocallyLinearEmbedding(n_components=2, random_state=42)),

])

t0 = time.time()

X_pca_lle_reduced = pca_lle.fit_transform(X)

t1 = time.time()

print(""PCA+LLE 시간: {:.1f}s."".format(t1 - t0))

plot_digits(X_pca_lle_reduced, y)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],102.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],12,[],[],0,0,1,0,0,0,0.0,1,0,True
178,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['결과는 비슷하지만 걸린 시간은 4배나 줄었습니다.'],0,0,0,0,0,0,,0,6,False
179,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['MDS를 시도해 보죠. 10,000개 샘플을 적용하면 이 알고리즘은 너무 오래걸리므로 2,000개만 시도해 보겠습니다:']",0,0,0,0,0,0,,0,13,False
180,code,[],"from sklearn.manifold import MDS



m = 2000

t0 = time.time()

X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])

t1 = time.time()

print(""MDS 시간: {:.1f}s (10,000개가 아니고 2,000 MNIST 이미지에서)."".format(t1 - t0))

plot_digits(X_mds_reduced, y[:m])

plt.show()",[],"['image/png', 'text/plain']",[],[],[],103.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],9,[],[],0,0,1,0,0,0,0.0,1,0,True
181,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['아 이건 좋지 않아 보이네요. 모든 군집이 너무 중복되어 있습니다. 먼저 PCA를 적용하면 빨라질까요?'],0,0,0,0,0,0,,0,14,False
182,code,[],"from sklearn.pipeline import Pipeline



pca_mds = Pipeline([

    (""pca"", PCA(n_components=0.95, random_state=42)),

    (""mds"", MDS(n_components=2, random_state=42)),

])

t0 = time.time()

X_pca_mds_reduced = pca_mds.fit_transform(X[:2000])

t1 = time.time()

print(""PCA+MDS 시간: {:.1f}s (on 2,000 MNIST images)."".format(t1 - t0))

plot_digits(X_pca_mds_reduced, y[:2000])

plt.show()",[],"['image/png', 'text/plain']",[],[],[],104.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],12,[],[],0,0,1,0,0,0,0.0,1,0,True
183,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['같은 결과에 속도도 동일합니다. PCA가 도움이 되지 않네요.'],0,0,0,0,0,0,,0,8,False
184,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['LDA를 시도해 보죠:'],0,0,0,0,0,0,,0,3,False
185,code,[],"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis



t0 = time.time()

X_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)

t1 = time.time()

print(""LDA 시간: {:.1f}s."".format(t1 - t0))

plot_digits(X_lda_reduced, y, figsize=(12,12))

plt.show()",[],"['image/png', 'text/plain']",[],[],[],105.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],8,[],[],0,0,1,0,0,0,0.0,2,0,True
186,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['매우 빨라 처음엔 괜찮아 보이지만 자세히 보면 몇 개의 군집이 심각하게 중복되어 있습니다.'],0,0,0,0,0,0,,0,13,False
187,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['아마 이 비교에서 t-SNE가 승자같네요. 시간을 재어 보진 않았으니 한번 해보죠:'],0,0,0,0,0,0,,0,11,False
188,code,[],"from sklearn.manifold import TSNE



t0 = time.time()

X_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)

t1 = time.time()

print(""t-SNE 시간: {:.1f}s."".format(t1 - t0))

plot_digits(X_tsne_reduced, y)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],106.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],8,[],[],0,0,1,0,0,0,0.0,1,0,True
189,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],['LLE 보다 두 배나 느립니다. 하지만 MDS 보단 훨씬 빠르고 결과물도 아주 좋습니다. PCA가 속도를 높여줄 수 있는지 확인해 보겠습니다:'],0,0,0,0,0,0,,0,20,False
190,code,[],"pca_tsne = Pipeline([

    (""pca"", PCA(n_components=0.95, random_state=42)),

    (""tsne"", TSNE(n_components=2, random_state=42)),

])

t0 = time.time()

X_pca_tsne_reduced = pca_tsne.fit_transform(X)

t1 = time.time()

print(""PCA+t-SNE 시간 {:.1f}s."".format(t1 - t0))

plot_digits(X_pca_tsne_reduced, y)

plt.show()",[],"['image/png', 'text/plain']",[],[],[],107.0,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],10,[],[],0,0,1,0,0,0,0.0,1,0,True
191,markdown,[],,[],[],[],[],[],,hornet4680..boktensorflow..handson-ml-master..08_dimensionality_reduction.ipynb,[],[],[],0,[],"['네, 결과물에 영향을 미치지 않으면서 PCA 속도가 34% 정도 향상되었습니다. t-SNE가 제일 좋네요!']",0,0,0,0,0,0,,0,13,False
0,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],"[[1, 'Final K-fold Model for the Kaggle State Farm Competition']]",[],0,[],['# Final K-fold Model for the Kaggle State Farm Competition'],0,0,0,0,0,0,,0,10,False
1,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['IMPORT AND INTIALIZATIONS'],0,0,0,0,0,0,,0,3,False
2,code,[],"import numpy as np

import random

import cv2

import math

import pickle

import os

import pandas as pd

import datetime

import matplotlib.pyplot as plt



from glob import glob

from PIL import ImageFile 

ImageFile.LOAD_TRUNCATED_IMAGES = True 

from sklearn.model_selection import KFold 

from sklearn.metrics import log_loss,accuracy_score

from sklearn.datasets import load_files

from keras import optimizers

from keras.optimizers import SGD

from keras.applications.resnet50 import ResNet50

from keras.applications.vgg19 import VGG19

from keras.layers import GlobalMaxPooling2D, BatchNormalization,Dropout, Flatten, Dense, Input

from keras.models import Sequential,Model,model_from_json

from keras.preprocessing import image    

from keras.preprocessing.image import ImageDataGenerator

from keras.utils import np_utils

from keras.callbacks import EarlyStopping, Callback,ModelCheckpoint",[],[],[],[],[],1.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],26,[],[],0,0,0,0,0,0,0.0,1,0,True
3,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['PREPROCESSING METHODS'],0,0,0,0,0,0,,0,2,False
4,code,[],"

# Reads images from path and resizes them to 224x224

# Returns resized image

def read_resize_images(path):

    img = cv2.imread(path)

    img_rows, img_cols=224,224

    resized = cv2.resize(img, (img_rows, img_cols))

    return resized



# Reads the drivers_img_list csv file

# Returns driver data, driver classification

def get_driver_data():

    dr = dict()

    clss = dict()

    path ='Data/driver_imgs_list.csv'

    print('Read driver data')

    f = open(path, 'r')

    line = f.readline()

    while (1):

        line = f.readline()

        if line == '':

            break

        arr = line.strip().split(',')

        dr[arr[2]] = arr[0]

        if arr[0] not in clss.keys():

            clss[arr[0]] = [(arr[1], arr[2])]

        else:

            clss[arr[0]].append((arr[1], arr[2]))

    f.close()

    return dr, clss



# Method used to retrieve all Train Dataset images

# Returns data, labels, image names, driver in image, list of unique drivers  

def load_train():

    driver_file=[]

    driver_target=[]

    driver_id=[]

    driver_file_id=[]

    # calls the method that reads the provided CSV file

    driver_data, dr_class = get_driver_data()

    print('driver data and class sample',len(driver_data),len(dr_class))

    for j in range(10):

        print('Load folder c{}'.format(j))

        path = os.path.join( 'Data', 'imgs', 'train', 'c' + str(j), '*.jpg')

        data = glob(path)

        for p in data:

            

            driver_file.append(read_resize_images(p))

            driver_target.append(j)

            base = os.path.basename(p)

            driver_file_id.append(base)

            driver_id.append(driver_data[base])

            

    unique_drivers = sorted(list(set(driver_id)))

    return driver_file,driver_target,driver_file_id,driver_id,unique_drivers



# Saves data into a file, in the specified path 

def cache_data(data, path):

    if os.path.isdir(os.path.dirname(path)):

        file = open(path, 'wb')

        pickle.dump(data, file)

        file.close()



# Loads data from cache file    

# Returns saved data

def restore_data(path):

    data = dict()

    if os.path.isfile(path):

        file = open(path, 'rb')

        data = pickle.load(file)

    return data","['Reads images from path and resizes them to 224x224', 'Returns resized image', 'Reads the drivers_img_list csv file', 'Returns driver data, driver classification', 'Method used to retrieve all Train Dataset images', 'Returns data, labels, image names, driver in image, list of unique drivers', 'calls the method that reads the provided CSV file', 'Saves data into a file, in the specified path', 'Loads data from cache file', 'Returns saved data']",[],[],[],[],2.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],71,[],[],0,10,0,0,0,0,0.0,0,0,True
5,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['PROCESSING METHODS'],0,0,0,0,0,0,,0,2,False
6,code,[],"# Copies the required drivers data and labels into new arrays based on the driver list 

# Returns data, target (both return as uint8 strings)

def get_selected_drivers(train_data, train_target, driver_id, driver_list):

    data = []

    target = []

    

    for i in range(len(train_data)):

        if driver_id[i] in driver_list:

            data.append(train_data[i])

            target.append(train_target[i])

    

    # Delete variables accessing data 

    del train_data

    del train_target

    del driver_id

    del driver_list

    data = np.array(data,dtype=np.uint8)

    target = np.array(target,dtype=np.uint8)

    return data, target



# Copies the required drivers data and labels into new arrays based on the driver list 

# Returns data, target,test index (data returns as float32)

def get_val_selected_drivers(val_data, val_target, driver_id, driver_list):

    data = []

    target = []

    index = []

   

    for i in range(len(val_data)):

        if driver_id[i] in driver_list:

            data.append(val_data[i])

            target.append(val_target[i])

            index.append(i)

            

    # Delete variables accessing data        

    del val_data

    del val_target

    del driver_id

    del driver_list

    data = np.array(data,dtype=np.float32)

    target = np.array(target,dtype=np.uint8)

    index = np.array(index)

    return data, target, index



# Class used to visualize the train vs validation loss plot (per epoch)

class PlotLosses(Callback):

    # Initializes plot on first epoch

    def on_train_begin(self, logs={}):

        self.i = 0

        self.x = []

        self.losses = []

        self.val_losses = []

        

        self.fig = plt.figure()

        

        self.logs = []

        

    # Prints a plot at the end of each epoch

    def on_epoch_end(self, epoch, logs={}):

        

        self.logs.append(logs)

        self.x.append(self.i)

        self.losses.append(logs.get('loss'))

        self.val_losses.append(logs.get('val_loss'))

        self.i += 1

        

        plt.plot(self.x, self.losses, label=""loss"")

        plt.plot(self.x, self.val_losses, label=""val_loss"")

        plt.legend(['train', 'test'], loc='upper left')

        plt.show();



# Splits a list into (specified) parts

def split_list(l,size):

    return [l[i*len(l) // size: (i+1)*len(l) // size] for i in range(size)]



# Converts dictionary to list

def get_val_predictions(train_data, pred_dict):

    val_pred = []

    for i in range(len(train_data)):

        val_pred.append(pred_dict[i])

    

    del train_data

    del pred_dict

    return val_pred





# Merges multiple prediction outputs into one by dividing predictions by its mean

# Returns a list

def merge_prediction_outputs(data, nfolds):

    a = np.array(data[0])

    for i in range(1, nfolds):

        a += np.array(data[i])

    a /= nfolds

    

    return a.tolist()
","['Copies the required drivers data and labels into new arrays based on the driver list', 'Returns data, target (both return as uint8 strings)', 'Delete variables accessing data', 'Copies the required drivers data and labels into new arrays based on the driver list', 'Returns data, target,test index (data returns as float32)', 'Delete variables accessing data', 'Class used to visualize the train vs validation loss plot (per epoch)', 'Initializes plot on first epoch', 'Prints a plot at the end of each epoch', 'Splits a list into (specified) parts', 'Converts dictionary to list', 'Merges multiple prediction outputs into one by dividing predictions by its mean', 'Returns a list']",[],[],[],[],3.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],94,[],[],0,13,0,0,0,0,0.0,0,0,True
7,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['POST-PROCESSING METHODS'],0,0,0,0,0,0,,0,2,False
8,code,[],"# Combines two lists together

# Returns one list

def append_chunk(main, part):

    for p in part:

        main.append(p)

        

    return main



# Method used to retrieve all Test Dataset image paths 

# Splits the images into specified size

# Reads and resizes images of the split

# Returns the chunks  images, image names

def load_test_parts(part,splits):

    path='Data/imgs/test/*'

    data=glob(path)

    driver_file=[]

    driver_id=[]  

    test_chunks=split_list(data,splits)

    

    for p in test_chunks[part]:

        driver_file.append(read_resize_images(p))

        driver_id.append(os.path.basename(p))



    return driver_file,driver_id



# creates the submission file from provided predictions

def create_submission(predictions, test_id, loss,model_name):

    print('Started building csv file')

    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])

    result.insert(loc=0, column='img', value=test_id)

    now = datetime.datetime.now()

    if not os.path.isdir('submissions'):

        os.mkdir('submissions')

    suffix = str(round(loss, 6)) + '_' + str(now.strftime(""%Y-%m-%d-%H-%M""))

    subfile = os.path.join('submissions', 'submission' +model_name+ '_'  +suffix + '.csv')

    result.to_csv(subfile, index=False)

    print(""successfully created submission"")   

    

# Creates Test Dataset predictions

#  Loads the model and weights

#  Loads test images

#  generates predictions

#  Merges predictions of all K-folds into one output file

#  Returns prediction output, test image names

def run_model_submission(model_name,nfolds):    

    

    print('Model: ', model_name)

    

    test_splits=5

    num_fold=0

    

    full_pred=[]

    full_test_id=[]

    # For-loop for each fold

    for i in range(nfolds):

        

        # Loads the model and weights

        model= get_model_arch(model_name)

        num_fold+=1

        print('Fold No. ',num_fold)

        kfold_weights_path = os.path.join('cache', 'weights_kfold_augmented_' + model_name + '_' + str(num_fold) + '.h5')

        model.load_weights(kfold_weights_path)

        

        predictions=[]

        # For loop each test split

        for x in range(test_splits):

            print('iteration: ',x)

           # cache_path=None

            part=x+1

            

            # load test images of the split 

            cache_path= os.path.join('cache','test_224_part'+str(part)+'.dat')



            if not os.path.isfile(cache_path):

                print('building test cache')

                test_files,test_id = load_test_parts(x,test_splits)

                cache_data((test_files,test_id),cache_path)

                print('test cache built')

            else:

                print('Restore test from cache')

                (test_files,test_id)=restore_data(cache_path)



            test_files=np.array(test_files,dtype=np.uint8)    

            test_files = test_files.reshape(test_files.shape[0], img_rows, img_cols,img_channel)

            test_files = test_files.astype('float32')

            

            # Preprocess test images to match train dataset

            test_files/=255

            # Generate Test prediction

            prediction=model.predict(test_files,  batch_size=batch_size)

            

            # Add chunk to main list

            predictions=append_chunk(predictions,prediction)

            if num_fold==1:

                full_test_id=append_chunk(full_test_id,test_id)

            

            del test_files

    

        full_pred.append(predictions)

        

        

    # Merges all predictions into one    

    result=merge_prediction_outputs(full_pred, nfolds)   

       

    return result,full_test_id



# Loops the run_model_submission method by the number of models

# Prediction output of each loop is merged into one

# Creates the final output file by Giving the prediction output to the create_submission method

def test_set_submission(model_list,nfolds,score):

    multi_model=[]

    test_id=None

    model_text=''

    for i in range(len(model_list)):

        result,test_id =run_model_submission(model_list[i],nfolds)

        multi_model.append(result)

        create_submission(result,test_id,score,model_list[i])

        model_text=model_text+'_'+model_list[i]

        

    num_models=len(multi_model)   

    result=merge_prediction_outputs(multi_model, num_models)

    

    create_submission(result,test_id,score,model_text) 




","['Combines two lists together', 'Returns one list', 'Method used to retrieve all Test Dataset image paths', 'Splits the images into specified size', 'Reads and resizes images of the split', 'Returns the chunks  images, image names', 'creates the submission file from provided predictions', 'Creates Test Dataset predictions', 'Loads the model and weights', 'Loads test images', 'generates predictions', 'Merges predictions of all K-folds into one output file', 'Returns prediction output, test image names', 'For-loop for each fold', 'Loads the model and weights', 'For loop each test split', 'cache_path=None', 'load test images of the split', 'Preprocess test images to match train dataset', 'Generate Test prediction', 'Add chunk to main list', 'Merges all predictions into one', 'Loops the run_model_submission method by the number of models', 'Prediction output of each loop is merged into one', 'Creates the final output file by Giving the prediction output to the create_submission method']",[],[],[],[],4.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],125,[],[],0,25,0,0,0,0,0.0,0,0,True
9,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['VISUALIZATION METHODS'],0,0,0,0,0,0,,0,2,False
10,code,[],"

# Creates the sample visualization images of all individual models and including combined model

def visualize_val_images(model_list,all_model_predictions,train_files,train_targets,comb_pred):

    #Name the labels 

    y_labels = ['safe driving', 'texting - right', 'talking on the phone - right', 'texting - left',

                'talking on the phone - left', 'operating the radio', 'drinking', 'reaching behind',

                'hair and makeup', 'talking to passenger']

    # Convert all train images to np array

    train_files=np.array(train_files)

   

    # Create the label/title of the combined model

    # Adds label of models to a new list

    model_label_list=[]

    multi_model_text=''

    for i in range(len(model_list)):

            model_label_list.append(model_list[i])

            if len(model_list)>1:

                multi_model_text+=model_list[i]

                if len(model_list)!= i:

                    multi_model_text+=' '  

    #add the combined predictions to all model predictions for visualization

    if len(model_list)>1:

        model_label_list.append(multi_model_text)

        all_model_predictions.append(comb_pred)

        

    # Set number of sample images    

    np.random.seed(51)

    random_data=np.random.choice(train_files.shape[0], size=10, replace=False)

    # Loops visualization for all models

    for x in range(len(all_model_predictions)):

        

        fig = plt.figure(figsize=(20, 10))

        for i, idx in enumerate(random_data):

            ax = fig.add_subplot(2, 5, i + 1, xticks=[], yticks=[])

            ax.imshow(np.squeeze(train_files[idx]))

            true_idx = np.argmax(train_targets[idx])

            pred_idx = np.argmax(all_model_predictions[x][idx])

            ax.set_title(""{} ({})"".format(y_labels[pred_idx], y_labels[true_idx]),

                         color=(""green"" if pred_idx == true_idx else ""red""))

        fig.suptitle(model_label_list[x], fontsize=25)

        plt.show()

        

 # Creates the validation loss plot of all models   

def visualize_val_score(model_list,val_scores,nfolds):

    plt.figure(figsize=(9,4))

    model_label_list=[]

    multi_model_text=''

    # Plots the model validation loss for each fold

    # Creates combined model text

    # Adds model labels to new list

    for i in range(len(model_list)):

        plt.plot(range(1,nfolds+1),val_scores[i])

        model_label_list.append(model_list[i])

        if len(model_list)>1:

                multi_model_text+=model_list[i]

                if len(model_list)!= i:

                    multi_model_text+=' '        

    # Adds the combined model label to labels list

    # Plots the combined model

    if len(model_list)>1:

        model_label_list.append(multi_model_text)

        avg_val_score=[]

        for i in range(nfolds):

            avg_score=0

            for x in range(len(model_list)):

                avg_score+=val_scores[x][i]

            avg_score/=len(model_list)

            avg_val_score.append(avg_score)



     

        plt.plot(range(1,nfolds+1),avg_val_score)    



    plt.title('Model Validation Loss of All Folds')

    plt.ylabel('Loss')

    plt.xlabel('Folds')

    

    plt.legend(model_label_list, loc='upper left')

    plt.show()
","['Creates the sample visualization images of all individual models and including combined model', 'Name the labels', 'Convert all train images to np array', 'Create the label/title of the combined model', 'Adds label of models to a new list', 'add the combined predictions to all model predictions for visualization', 'Set number of sample images', 'Loops visualization for all models', 'Creates the validation loss plot of all models', 'Plots the model validation loss for each fold', 'Creates combined model text', 'Adds model labels to new list', 'Adds the combined model label to labels list', 'Plots the combined model']",[],[],[],[],5.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],78,[],[],0,14,0,0,0,0,0.0,0,0,True
11,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['ARCHITECTURE METHODS'],0,0,0,0,0,0,,0,2,False
12,code,[],"# Creates the VGG19 model architecture 

# Returns compiled model architecture

def VGG19_arch():

    #Calls the Keras application model with pre trained weights

    base_model = VGG19(input_shape=(224,224,3),weights='imagenet', include_top=False)



    #Additional model layers

    x = base_model.output

    x = GlobalMaxPooling2D()(x)

    x = Dense(512, activation='relu')(x)

    predictions = Dense(10, activation='softmax')(x)  

    

    model = Model(inputs=base_model.input, outputs=predictions)

    #Model compilation

    model.compile(optimizer=SGD(lr=0.0009,momentum=0.9,nesterov=True),loss='categorical_crossentropy')

    

    return model



# Creates the ResNet50 Model architecture

# Returns compiled model architecture

def ResNet50_arch():

    #Calls the Keras application model with pre trained weights

    base_model = ResNet50(input_shape=(224,224,3),weights='imagenet', include_top=False)

    

    #Additional model layers

    x = base_model.output

    x = GlobalMaxPooling2D()(x)

    x = Dense(512, activation='relu')(x)

    x = BatchNormalization()(x)

    predictions = Dense(10, activation='softmax')(x)  

   

    model = Model(inputs=base_model.input, outputs=predictions)

    #Model compilation

    model.compile(optimizer=SGD(lr=0.001, momentum=0.9,nesterov=True), loss='categorical_crossentropy')

   

    return model

 

# Simplifies the calling of a model by accessing the model equivalent of the string

# returns model architecture

def get_model_arch(model_name):

    model_arch=None

    if model_name=='VGG19':

        model_arch=VGG19_arch()

    elif model_name=='ResNet50':

        model_arch=ResNet50_arch()

    else:

        print('No model with such name')

    return model_arch","['Creates the VGG19 model architecture', 'Returns compiled model architecture', 'Calls the Keras application model with pre trained weights', 'Additional model layers', 'Model compilation', 'Creates the ResNet50 Model architecture', 'Returns compiled model architecture', 'Calls the Keras application model with pre trained weights', 'Additional model layers', 'Model compilation', 'Simplifies the calling of a model by accessing the model equivalent of the string', 'returns model architecture']",[],[],[],[],6.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],48,[],[],0,12,0,0,0,0,0.0,0,0,True
13,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['TRAINING PARAMETERS'],0,0,0,0,0,0,,0,2,False
14,code,[],"# Some parameter intializations



img_rows=224

img_cols=224

img_channel=3  





batch_size=32

nfolds=10

epochs=20



# list specifying the models we want to train

model_list=[

      'VGG19'

              ,

    'ResNet50'

   

   

           ]

# Variable that specifies if we want to make a submission output or not

build_submission=True","['Some parameter intializations', 'list specifying the models we want to train', 'Variable that specifies if we want to make a submission output or not']",[],[],[],[],7.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],21,[],[],0,3,0,0,0,0,0.0,0,0,True
15,markdown,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],['MODEL TRAINING AND TESTING SEGMENT'],0,0,0,0,0,0,,0,5,False
16,code,[],"# Method that compiles all methods

# Loads dataset

# Preprocesses data

# Trains model

# tests the model

# Generates validation predictions score

# Returns score, train images,train targets/labels, validation score



def build_model(model_name,nfolds,epochs):

    print('Model: ', model_name)

    

    #intializes the fixed seed/random state

    random_state = 51

   

    #Loads the train data from file or cache

    cache_path= os.path.join('cache','train_224.dat')

    if not os.path.isfile(cache_path):

        print('building train cache')

        train_files,train_targets,train_id, driver_id, unique_drivers = load_train()

        cache_data((train_files,train_targets,train_id, driver_id, unique_drivers),cache_path)

        print('train cache built')           

    else:

        print('Restore train from cache')

        (train_files,train_targets,train_id, driver_id, unique_drivers)=restore_data(cache_path)

    

    #intialize the dictionary that will store the validation predictions

    val_predictions = dict()

    val_score=[]

    # splits unique list into folds

    kf = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)

    num_fold = 0

    # drivers are split into train and validation sets

    for train_drivers, test_drivers in kf.split(unique_drivers):

        #Loads model architecture

        model = get_model_arch(model_name)

        # Gets the drivers and data based on the K-fold

        # Loads the data and labels

        unique_list_train = [unique_drivers[i] for i in train_drivers]

        X_train, Y_train  = get_selected_drivers(train_files, train_targets, driver_id, unique_list_train)

        unique_list_valid = [unique_drivers[i] for i in test_drivers]

        X_valid, Y_valid, test_index = get_val_selected_drivers(train_files, train_targets, driver_id, unique_list_valid)



        num_fold += 1

        print('Start KFold number {} of {}'.format(num_fold, nfolds))

        print('Train drivers: ', unique_list_train)

        print('Test drivers: ', unique_list_valid)

        

        # Divide each data point by 255 to normalize our data 

        X_valid/=255

        

        # Intialize Data augmentations

        train_datagen=ImageDataGenerator(

          rescale=1./255,

           zoom_range=0.4,

           width_shift_range=0.4,

           height_shift_range=0.4, 

           rotation_range=0.2

            )



        # Convert our labels to categorical using one-hot encode

        Y_train=np_utils.to_categorical(Y_train,10)

        Y_valid=np_utils.to_categorical(Y_valid,10)

        

        # Initialize weight path    

        if not os.path.isdir(os.path.join('cache')):

            os.mkdir(os.path.join('cache'))  

            

        kfold_weights_path = os.path.join('cache', 'weights_kfold_augmented_'+ model_name + '_' +  str(num_fold) + '.h5')

        

        # Check if weights already exist 

        if not os.path.isfile(kfold_weights_path) :

            

            # Initialize callbacks

            plot_losses = PlotLosses()

            callbacks = [

                EarlyStopping(monitor='val_loss', patience=8, verbose=0),

                plot_losses,

                ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=2),

                

            ]

            

            #Convert our train data to float32

            X_train=X_train.astype('float32')

            

            print('--finished pre processing--')

            

            # Augment and train the model

            history=model.fit_generator(train_datagen.flow(X_train,Y_train,batch_size=batch_size,seed=random_state),

                            validation_data=(X_valid,Y_valid),

                    epochs=epochs ,use_multiprocessing=False,steps_per_epoch=len(X_train)/batch_size,

                            callbacks=callbacks,verbose=0) 

        #load weights

        if os.path.isfile(kfold_weights_path):

            model.load_weights(kfold_weights_path)



        del X_train

        

        # Generate validation data predictions and score

        pred_result = model.predict(X_valid, batch_size=batch_size, verbose=0)

        score = log_loss(Y_valid, pred_result)

        print('Score log_loss: ', score)

        # Store validation score

        val_score.append(score)

        # Store validation prediction results in the dictionary

        # Orders predictions to match the index

        for i in range(len(test_index)):

            val_predictions[test_index[i]] = pred_result[i]

        

        # Release variables accessing data

        del test_index

        del train_datagen

        del model

        del pred_result

        del X_valid

    

    print('train length: ', len(train_files))    

    

    score_sum=sum(val_score)

    score_sum/=nfolds

    print('Validation Score Average ', score_sum)

    # Converts the val_predictions dictionary to a list

    val_predictions = get_val_predictions(train_files, val_predictions)

    # Get complete dataset validation score

    train_targets=np_utils.to_categorical(train_targets,10)

    score = log_loss(train_targets, val_predictions)

    #Score model

    print('(validation score) Final log_loss: {}, nfolds: {} epoch: {}'.format(score, nfolds, epochs))

    return val_predictions,train_files,train_targets,val_score





# Method that loops the build_model method to the number of models

# Combines predictions of multiple models

# Calls the visualization methods

# Calls the method to build the Test Dataset predictions and Submission file

def train_selected_models(model_list,nfolds,epochs,build_submission):

    multi_model=[]

    train_targets=None

    train_files=None

    model_val_scores=[]

    # Execute each model- using build_model

    # Save return values

    for i in range(len(model_list)):

        result,train_files,train_targets,val_score=build_model(model_list[i],nfolds,epochs)

        multi_model.append(result)

        model_val_scores.append(val_score)

        

    num_models=len(multi_model)

    print(""***FINAL MODEL RESULTS/SUMMARY***"") 

    

    for i in range(len(model_list)):

        score=log_loss(train_targets,multi_model[i])

        print('{} Final log_loss: {}, nfolds: {} epoch: {}'.format(model_list[i],score, nfolds, epochs))

        #score=accuracy_score(train_targets,multi_model[i])

       # print('{} Accuracy: {}'.format(model_list[i],score))

    #Merge multiple model results and score the results

    result=merge_prediction_outputs(multi_model, num_models)

    score=log_loss(train_targets,result)

    print(""Multi Model Log_loss Validation Score: {} "".format(score)) 

   # score=accuracy_score(train_targets,result)

   # print('Multi Model Accuracy: {}'.format(score))

    

    #Creates the scores plot 

    visualize_val_score(model_list,model_val_scores,nfolds)

    #Creates sample images

    visualize_val_images(model_list,multi_model,train_files,train_targets,result)

    del train_files

    #If true, it build the submission results

    if build_submission == True:

        test_set_submission(model_list,nfolds,0.22)

        



    

# The one line to run it all ~unless memory get exhausted before building results!!

train_selected_models(model_list,nfolds,epochs,build_submission)","['Method that compiles all methods', 'Loads dataset', 'Preprocesses data', 'Trains model', 'tests the model', 'Generates validation predictions score', 'Returns score, train images,train targets/labels, validation score', 'intializes the fixed seed/random state', 'Loads the train data from file or cache', 'intialize the dictionary that will store the validation predictions', 'splits unique list into folds', 'drivers are split into train and validation sets', 'Loads model architecture', 'Gets the drivers and data based on the K-fold', 'Loads the data and labels', 'Divide each data point by 255 to normalize our data', 'Intialize Data augmentations', 'Convert our labels to categorical using one-hot encode', 'Initialize weight path', 'Check if weights already exist', 'Initialize callbacks', 'Convert our train data to float32', 'Augment and train the model', 'load weights', 'Generate validation data predictions and score', 'Store validation score', 'Store validation prediction results in the dictionary', 'Orders predictions to match the index', 'Release variables accessing data', 'Converts the val_predictions dictionary to a list', 'Get complete dataset validation score', 'Score model', 'Method that loops the build_model method to the number of models', 'Combines predictions of multiple models', 'Calls the visualization methods', 'Calls the method to build the Test Dataset predictions and Submission file', 'Execute each model- using build_model', 'Save return values', 'score=accuracy_score(train_targets,multi_model[i])', ""print('{} Accuracy: {}'.format(model_list[i],score))"", 'Merge multiple model results and score the results', 'score=accuracy_score(train_targets,result)', ""print('Multi Model Accuracy: {}'.format(score))"", 'Creates the scores plot', 'Creates sample images', 'If true, it build the submission results', 'The one line to run it all ~unless memory get exhausted before building results!!']","['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",['MemoryError'],[''],[],9.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],174,[],[],0,47,24,1,0,0,0.0,22,0,True
17,code,[],"# Used to build the submission test

# BACKUP ONLY: Only use this if memory gets exhausted

if build_submission == True:

    test_set_submission(model_list,nfolds,0.22)","['Used to build the submission test', 'BACKUP ONLY: Only use this if memory gets exhausted']",[],[],[],[],8.0,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],4,[],[],0,2,0,0,0,0,0.0,1,0,True
18,code,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
19,code,[],,[],[],[],[],[],,jartantupjar..7-Distracted-Driver-Detection..Solution 3 - K-fold Implementation.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
0,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Household Electricity Consumption Analysis and segregation, using k-Means Clustering'], [2, 'Report by: Spandan Gandhi'], [2, 'Practical Data Science Tutorial (15-688)']]",[],0,[],"['# Household Electricity Consumption Analysis and segregation, using k-Means Clustering\n', '\n', '## Report by: Spandan Gandhi\n', '## Practical Data Science Tutorial (15-688)']",0,0,0,0,0,0,,0,21,False
1,code,[],"import pandas as pd

import matplotlib.pyplot as plt

import numpy as np

from sklearn.cluster import KMeans

import datetime as dt

import matplotlib.patches as mpatches

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from IPython.display import HTML",[],[],[],[],[],1.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],9,[],[],0,0,0,0,0,0,0.0,0,0,True
2,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"[('Introduction', '#Introduction'), ('Application', '#Application'), ('References', '#References')]","['<h1 id=""tocheading"">Table of Contents</h1>\n', '<div id=""toc""></div>\n', '\n', '1.[Introduction](#Introduction)\n', '\n', '2.[Downloading Data File](#Downloading Data File)\n', '\n', '3.[Data Cleaning and Analysis](#Data Cleaning and Analysis)\n', '\n', '4.[Data Processing](#Data Processing)\n', '\n', '5.[K-Means Clustering](#K-Means Clustering)\n', '   \n', '6.[Application](#Application)\n', '\n', '7.[References](#References)\n']",0,0,0,0,0,0,,0,27,False
3,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Introduction']]",[],0,[],"[""<a id='Introduction'></a>\n"", '# Introduction']",0,0,0,0,0,0,,0,4,False
4,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['Clustering is the process of classifying a set of objects (here data points) into different groups, such that the objects in the same group are more similar to each other in some particular sense than those in other groups. These different groups are called clusters and mean of each of this cluster is called centroid. The property on basis of which the data is classified depends on the type of data being classified. Clustering belongs to unsupervised learning problems, since the data that are being classified are unlabeled, i.e. it is not known beforehand, which data point belongs to which cluster. There are mainly two major methods of clustering, namely, Partitioning Relocation Clustering and Hierarchical Clustering. The k-means clustering, which is used in this project is a type of Partitioning Relocation Clustering. It is widely used in market segmentation, computer vision, geostatistics, image classification and astronomy. It is also used as a preprocessing step for other algorithms, for example to find a starting configuration.']",0,0,0,0,0,0,,0,164,False
5,code,[],"HTML('<iframe src=https://i.imgur.com/S65Sk9c.jpg width=2000 height=600></iframe>')

#<img src ='https://i.imgur.com/S65Sk9c.jpg', width=2000, height=4000>","[""<img src ='https://i.imgur.com/S65Sk9c.jpg', width=2000, height=4000>""]",[],[],[],"['text/html', 'text/plain']",2.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],2,[],[],0,1,0,0,1,0,0.0,0,0,True
6,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,['https://data.nrel.gov/submissions/69).'],['This project uses the k-Means clustering to classify 200 households based on their average hourly electricity consumption in the year 2010. The data was taken from NREL data source (https://data.nrel.gov/submissions/69). The households were randomly selected among the ones available in the 2010 RECS data set for the Midwest region of the United States.  The data is in form of a power consumed in Watts(W) by each of the 200 households with a resolution of 10 minutes.'],0,0,0,0,0,0,,0,76,False
7,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Downloading Data File']]",[],0,[],"[""<a id='Downloading Data File'></a>\n"", '# Downloading Data File\n']",0,0,0,0,0,0,,0,8,False
8,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://github.com/join?source=header-home', 'https://desktop.github.com/', 'https://github.com/SpandanGandhi/PDS_tutorial.git', 'https://help.github.com/desktop/guides/getting-started-with-github-desktop/']","['- **If you do not have a github acount,** please follow the following instructions on the following link to create one in 2 minutes https://github.com/join?source=header-home\n', '- You can **use github desktop app** to handle your github accouunt, if you are not familiar with github coding. https://desktop.github.com/\n', ""- Then go to following link https://github.com/SpandanGandhi/PDS_tutorial.git and click on **'clone or download'** option and opt for **'Download ZIP'** option.\n"", ""- Extract files from the zip folder and use the **'Consumption.csv'** file as data.\n"", '- Ensure to **store the csv file in the same folder where you have saved the jupyter notebook.**\n', '- If you are still unable to download the file, read the following instructions on how to download and use **github desktop app.** https://help.github.com/desktop/guides/getting-started-with-github-desktop/']",0,0,0,0,0,0,,0,122,False
9,code,[],"data=pd.read_csv('Consumption.csv', delimiter=None, header='infer')

data.head()",[],[],[],[],"['text/html', 'text/plain']",3.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
10,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['http://ieeexplore.ieee.org/document/5356176/)', ('Data: Impact of uncoordinated plug-in electric vehicle charging on residential power demand - supplementary data', 'http://ieeexplore.ieee.org/document/5356176/')]",['[Data: Impact of uncoordinated plug-in electric vehicle charging on residential power demand - supplementary data](http://ieeexplore.ieee.org/document/5356176/)\n'],0,0,0,0,0,0,,0,15,False
11,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Data Cleaning and Analysis']]",[],0,[],"[""<a id='Data Cleaning and Analysis'></a>\n"", '# Data Cleaning and Analysis\n', '\n', 'Here the data is scrubbed of any anomalies and the resultant dataset is graphically visualized']",0,0,0,0,0,0,,0,25,False
12,code,[],"#Converting time to timestamp ( a datetime object)

data['Timestamp']=pd.to_datetime(data['Time'])



# Ensuring there are no temporal gaps

plt.figure(num=1, figsize=(10,5))

data.Timestamp.plot() #Plotting the newly added timestamp column

plt.xlabel('Reading Count')

plt.ylabel('Date')

plt.show()



# As seen in the figure above, time vs index curve is a continuous staight line, 

# assuring that there are no temporal gaps in the data.
","['Converting time to timestamp ( a datetime object)', 'Ensuring there are no temporal gaps', 'Plotting the newly added timestamp column', 'As seen in the figure above, time vs index curve is a continuous staight line,', 'assuring that there are no temporal gaps in the data.']","['image/png', 'text/plain']",[],[],[],4.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],12,[],[],0,5,1,0,0,0,0.0,0,0,True
13,code,[],"# Conforming that all readings are taken at an interval of 10 minutes, 

# the minimum and maximum differences of all the consecutive timestamps are

# taken, as follows:



# Getting the minimum difference between two consecutive timestamps:

print(""The minimum difference between any two consecutive timestamps is: "" +\

      str(np.min(np.diff(data['Timestamp']))))



# Getting the maximum difference between two consecutive timestamps:

print(""The maximum difference between any two consecutive timestamps is: "" +\

      str(np.max(np.diff(data['Timestamp']))) )



print ('Since maximum difference = minimum difference, all intervals are fixed at 10 minutes')","['Conforming that all readings are taken at an interval of 10 minutes,', 'the minimum and maximum differences of all the consecutive timestamps are', 'taken, as follows:', 'Getting the minimum difference between two consecutive timestamps:', 'Getting the maximum difference between two consecutive timestamps:']",[],[],[],[],5.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],13,[],[],0,5,0,0,0,0,0.0,1,0,True
14,code,[],"# Ensuring there are no NAN values



a=list(data.isnull().sum(axis=0)==1)

print(a.count(True)) # Number of NAN values

print('\n Thus, no NAN valus are detected.')","['Ensuring there are no NAN values', 'Number of NAN values']",[],[],[],[],6.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],5,[],[],0,2,0,0,0,0,0.0,1,0,True
15,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)', ('Isnull documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html')]",['[Isnull documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n'],0,0,0,0,0,0,,0,2,False
16,code,[],"# Dropping 'Time' column



data=data.drop(['Time'],axis=1)

data.head()","[""Dropping 'Time' column""]",[],[],[],"['text/html', 'text/plain']",7.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,1,0,0,1,0,0.0,0,0,True
17,code,[],"# Resetting the index of the dataframe

data2=data.set_index('Timestamp', drop=True, append=False, inplace=False, verify_integrity=False)

# Changing the data resolution from 10 minutes to 1 hour. 

# Since the power consumed in a hour, would be the sum of power consumed every six consecutive chunks of 

# ten minumtes, (.sum()) is used to resample the data

data2=data2.resample('H').sum()

data2.head()","['Resetting the index of the dataframe', 'Changing the data resolution from 10 minutes to 1 hour.', 'Since the power consumed in a hour, would be the sum of power consumed every six consecutive chunks of', 'ten minumtes, (.sum()) is used to resample the data']",[],[],[],"['text/html', 'text/plain']",8.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],7,[],[],0,4,0,0,1,0,0.0,0,0,True
18,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html)', ('Resample documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html')]",['[Resample documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html)'],0,0,0,0,0,0,,0,2,False
19,code,[],"# Adding a Hour column, containing Hour and Minute part of each timestamp

data2['Hour'] = pd.to_datetime(data2.index, format='%H:%M').time

data2['Hour'] = data2.Hour.apply(lambda x: x.strftime('%H:%M'))

# data2['Date'] = data2.index.date

data2.head()","['Adding a Hour column, containing Hour and Minute part of each timestamp', ""data2['Date'] = data2.index.date""]",[],[],[],"['text/html', 'text/plain']",9.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],5,[],[],0,2,0,0,1,0,0.0,0,0,True
20,code,[],"Hourly=data2.groupby(['Hour']).mean()

Hourly.head()",[],[],[],[],"['text/html', 'text/plain']",10.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
21,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://docs.python.org/3/library/datetime.html)', ('Datetime documentation', 'https://docs.python.org/3/library/datetime.html'), 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)', ('Group by documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html')]","['\n', '[Datetime documentation](https://docs.python.org/3/library/datetime.html)\n', '\n', '[Group by documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)']",0,0,0,0,0,0,,0,5,False
22,code,[],"# Plotting Hourly raw Data



plt.figure(num=2, figsize=(15,8))

plt.plot(Hourly, color='c', alpha=0.1, linewidth='3')

plt.plot(Hourly.mean(axis=1),color='k', alpha=1, linewidth='4')

#plt.ylabel('Date')

plt.ylabel('Average Hourly Consumption (W)', fontsize = 15)

plt.xticks(Hourly.index)

plt.xlabel('Time of the Day', fontsize = 15)

leg= mpatches.Patch(color='black', label='Mean Average Horly Consumption')

plt.legend(handles=[leg])

plt.show()","['Plotting Hourly raw Data', ""plt.ylabel('Date')""]","['image/png', 'text/plain']",[],[],[],11.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],12,[],[],0,2,1,0,0,0,0.0,0,0,True
23,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Data Processing']]",[],0,[],"[""<a id='Data Processing'></a>\n"", '\n', '# Data Processing']",0,0,0,0,0,0,,0,6,False
24,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['Seasons play an important role in determining the electricity consumption patterns. For example, in winters, houses with electric heaters would consume a significantly higher amount of electricity in the night compared to nights in summers. To account for these seasonal variations the data is split up into four seasons as follows:\n', '\n', '- Summer: June, July, August\n', '- Fall: September, October, November\n', '- Winter: December, January, February\n', '- Spring: March, April, May']",0,0,0,0,0,0,,0,71,False
25,code,[],"# Adding a column 'Month' to the original dataframe containing month of the corresponding dataframe

data['Month'] = data['Timestamp'].dt.month



# Making a new dataframe 'Summer' with data cooresponding to summer months

summer = (data.Month >=6) & (data.Month <=8) # Since summer has months from July (6) to August (8)

Summer_df=data[summer].reset_index(drop=True)

Summer_df=Summer_df.drop('Month',axis=1)



# Making a new dataframe 'Fall' with data cooresponding to fall months

fall = (data.Month >=9) & (data.Month <=11) # Since fall has months from September (9) to November (11)

Fall_df=data[fall].reset_index(drop=True)

Fall_df=Fall_df.drop('Month',axis=1)



# Making a new dataframe 'Winter' with data cooresponding to winter months

winter = (data.Month <=2) | (data.Month ==12) # Since winter has months from January (1) to February (2) and month of december(12)

Winter_df=data[winter].reset_index(drop=True)

Winter_df=Winter_df.drop('Month',axis=1)



# Making a new dataframe 'Spring' with data cooresponding to spring months

spring = (data.Month >=3) & (data.Month <=5) # Since spring has months from March (3) to May (5)

Spring_df=data[spring].reset_index(drop=True)

Spring_df=Spring_df.drop('Month',axis=1)



# Checking whether the assignment of data into the four dataframes is correct or not

print(len(Spring_df)) 

print(len(Summer_df))  

print(len(Fall_df))

print(len(Winter_df))



# Assuring data is assigned correctly to the four new **seasonal** dataframe.

len(Spring_df)+len(Summer_df)+len(Fall_df)+len(Winter_df)==len(data) 
","[""Adding a column 'Month' to the original dataframe containing month of the corresponding dataframe"", ""Making a new dataframe 'Summer' with data cooresponding to summer months"", 'Since summer has months from July (6) to August (8)', ""Making a new dataframe 'Fall' with data cooresponding to fall months"", 'Since fall has months from September (9) to November (11)', ""Making a new dataframe 'Winter' with data cooresponding to winter months"", 'Since winter has months from January (1) to February (2) and month of december(12)', ""Making a new dataframe 'Spring' with data cooresponding to spring months"", 'Since spring has months from March (3) to May (5)', 'Checking whether the assignment of data into the four dataframes is correct or not', 'Assuring data is assigned correctly to the four new **seasonal** dataframe.']",[],[],[],['text/plain'],12.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],31,[],[],0,11,0,0,1,0,0.0,1,0,True
26,code,[],Summer_df.head(),[],[],[],[],"['text/html', 'text/plain']",13.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
27,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],['The **process ** function is created to normalizes the data. Each **Hourly** usage dataframe was normalized by its largest value so that all home profiles would fall within **0 to 1** scale.'],0,0,0,0,0,0,,0,32,False
28,code,[],"def process(df):

    df.name = '%s'%df

    # Changing the resolution of the data to '1 hour'.

    df=df.set_index('Timestamp', drop=True, append=False, inplace=False, verify_integrity=False)

    df=df.resample('H').sum()



    # Adding a new 'Hour' column to get the hour and minutes of all corresponding timestamps

    df['Hour'] = pd.to_datetime(df.index, format='%H:%M').time

    df['Hour'] = df.Hour.apply(lambda x: x.strftime('%H:%M'))

    df['Date'] = df.index.date



    # Grouping the dataframe by hour to get 24 hours of the day as index (0 to 23)

    Hourly=df.groupby(['Hour']).mean()



    # Normalizing the data by dividing all hourly electric consumptions of the given 

    # household, with largest houly consumption.

    Hourly_normalized=Hourly.apply(lambda x: (x / (np.max(x))))

    

    return Hourly_normalized","[""Changing the resolution of the data to '1 hour'."", ""Adding a new 'Hour' column to get the hour and minutes of all corresponding timestamps"", 'Grouping the dataframe by hour to get 24 hours of the day as index (0 to 23)', 'Normalizing the data by dividing all hourly electric consumptions of the given', 'household, with largest houly consumption.']",[],[],[],[],14.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],19,[],[],0,5,0,0,0,0,0.0,0,0,True
29,code,[],"Summer = process(Summer_df)

Summer.name = 'Summer'

Winter = process(Winter_df)

Winter.name = 'Winter'

Fall = process(Fall_df)

Fall.name = 'Fall'

Spring = process(Spring_df)

Spring.name = 'Spring'



Summer.head()",[],[],[],[],"['text/html', 'text/plain']",15.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],10,[],[],0,0,0,0,1,0,0.0,0,0,True
30,code,[],"# Visualizing the data



def visualize (df):

    plt.figure(num=2, figsize=(15,8))

    plt.plot(df, color='c', alpha=0.1, linewidth='3')

    plt.plot(df.mean(axis=1),color='k', alpha=1, linewidth='4')

    plt.title(df.name, fontsize = 20)

    plt.ylabel('Normalized Average Hourly Consumption', fontsize = 15)

    plt.xticks(df.index)

    plt.xlabel('Time of the Day', fontsize = 15)

    l = mpatches.Patch(color='black', label='Mean Normalized Average Horly Consumption')

    plt.legend(handles=[l])

    plt.show()

    ",['Visualizing the data'],[],[],[],[],16.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],14,[],[],0,1,0,0,0,0,0.0,0,0,True
31,code,[],"visualize(Fall)

visualize(Summer)

visualize(Winter)

visualize(Spring)",[],"['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],17.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,0,4,0,0,0,0.0,0,0,True
32,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'K-Means Clustering']]",[],0,[],"[""<a id='K-Means Clustering'></a>\n"", '# K-Means Clustering']",0,0,0,0,0,0,,0,6,False
33,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'How does K-Means Clustering Works?']]",[],0,[],['## How does K-Means Clustering Works?'],0,0,0,0,0,0,,0,6,False
34,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['The k-Means Algorithm is an iterative process. It first selects random k centroids, where k is the number of centroids given as the input to the algorithm. It then groups the data points into k clusters, assigning each data points to the nearest centroid to it. It then recalculates the mean of data points assigned to a single cluster and assigns that mean as the new centroid. Like this k new centroids are obtained. After this the process is reiterated until there is convergence of the new previous centroid. The process is explained more clearly with the following example and steps.\n', 'Assume that there is a 2-D data available as follows that needs to be clustered:\n']",0,0,0,0,0,0,,0,116,False
35,code,[],HTML('<iframe src=http://opencv-python-tutroals.readthedocs.io/en/latest/_images/testdata.jpg width=500 height=400></iframe>'),[],[],[],[],"['text/html', 'text/plain']",18.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
36,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['**Step 1:** Determine the number of cluster to be obtained. This step is one of the trickiest steps for the algorithm. Since the data is unlabeled, one has to input the number of groups (or clusters) desired in the output. Since the variability in the data is unknown, it becomes difficult determine number of clusters required. The optimal number of clusters is somehow subjective and depends on the method used for measuring similarities and the parameters used for partitioning. The three most widely used methods to determine number of clusters are:\n', '-\t Elbow method\n', '-\t Average silhouette method\n', '-\t Gap statistic method\n', 'The elbow method is used in this project and is explained in greater detail below. For now, assume that number of clusters required were two.\n', '\n', '**Step 2:** First two centroids  **_C1_** and **_C2_** are chosen randomly.\n', '\n', '**Step 3:** - Then the distance from each data point to both centroids is calculated. If a point is closer to **_C1_**, then that data is labelled with ‘0’. If it is closer to **_C2_**, then labelled as ‘1’. In our case, we will color all ‘0’ labelled with red, and ‘1’ labelled with blue.\n', '\n', '$$ \\min\\limits_{c_i\\in{C}} dist(c_i, x)^2 $$\n', '\n', 'Here $c_i$ is C1 and C2, while _dist_ represents the euclidean distance between all datapoints (x) and centroids C1 and C2.\n']",0,0,0,0,0,0,,0,219,False
37,code,[],HTML('<iframe src=http://opencv-python-tutroals.readthedocs.io/en/latest/_images/initial_labelling.jpg width=500 height=400></iframe>'),[],[],[],[],"['text/html', 'text/plain']",19.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
38,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['**Step 4:** Next mean of all blue points and all red points is calculated separately. The mean of red point is assigned as the new **_C1_** and mean of blue points is assigned as new **_C2_**. \n', '\n', 'Let $S_i$ be the set of data points assigned of the $i^{th}$ cluster, then\n', '\n', '$$ c_i = \\frac{1}{|S_i|}\\sum\\limits_{x_i\\in{S_i}} x_i $$\n', '\n', 'Here $c_i$ is the newly assigned $i^{th}$ cluster and $x_i$ are all the data points in $S_i$, while $|S_i|$ is the number of data points in set $S_i$\n']",0,0,0,0,0,0,,0,84,False
39,code,[],HTML('<iframe src=http://opencv-python-tutroals.readthedocs.io/en/latest/_images/update_centroid.jpg width=500 height=400></iframe>'),[],[],[],[],"['text/html', 'text/plain']",20.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
40,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['**Step 5:** Repeat step 3 with new centroids and label data to ‘0’ and ‘1’. So we get result as below :\n', '\n', '**Step 6:** Step - 3 and Step - 4 are iterated until both centroids are converged to fixed points. (Or it may be stopped depending on the criteria we provide, like maximum number of iterations, or a specific accuracy is reached etc.) These points are such that sum of distances between data points and their corresponding centroids are minimum.']",0,0,0,0,0,0,,0,81,False
41,code,[],HTML('<iframe src=http://opencv-python-tutroals.readthedocs.io/en/latest/_images/final_clusters.jpg width=500 height=400></iframe>'),[],[],[],[],"['text/html', 'text/plain']",21.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
42,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'Selecting the number of clusters for the NREL Data']]",[],0,[],['## Selecting the number of clusters for the NREL Data'],0,0,0,0,0,0,,0,10,False
43,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],['The elbow method is used here to determine the number of clusters here.'],0,0,0,0,0,0,,0,13,False
44,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['The Elbow method computes within cluster sum of squares (WSS), which is the sum of square of distances between the centroid and all of data points in that centroid. The total WSS measures the compactness of the clustering and we want it to be as small as possible. Steps include:\n', '\n', '- Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 15 clusters.\n', '- For each k, calculate the total within-cluster sum of square (wss).\n', '- Plot the curve of wss according to the number of clusters k.\n', '- The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n']",0,0,0,0,0,0,,0,119,False
45,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)', ('Methods to determine optimal number of clusters', 'http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/')]",['[Methods to determine optimal number of clusters](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)'],0,0,0,0,0,0,,0,7,False
46,code,[],"S=Summer

error_summer=[]

for k in range(1,15):

    cluster1 = KMeans(n_clusters = k)

    cluster1.fit_predict(S[S.columns[0:]])

    error_summer.append(cluster1.inertia_)



clusters_summer = pd.DataFrame( { ""num_clusters"":range(1,15), ""WSS"": error_summer } )



plt.plot( clusters_summer.num_clusters, clusters_summer.WSS, marker = ""o"" )

plt.xlabel('Number of clusters')

plt.ylabel('WSS')

plt.title('Summer')

plt.show()



clusters_summer",[],"['image/png', 'text/plain']",[],[],"['text/html', 'text/plain']",22.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],16,[],[],0,0,1,0,1,0,0.0,0,0,True
47,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['As we can see from the graph and table the WSS is not significantly improved as number of clusters are increased beyond **3**, thus number of clusters desired are chosen to be 3.']",0,0,0,0,0,0,,0,33,False
48,code,[],"def elbow(df):

    error=[]

    for k in range(1,15):

        cluster = KMeans(n_clusters = k)

        cluster.fit_predict(df[df.columns[0:]])

        error.append(cluster.inertia_)



    clusters = pd.DataFrame( { ""num_clusters"":range(1,15), ""cluster_errors"": error } )



    plt.plot( clusters.num_clusters, clusters.cluster_errors, marker = ""o"" )

    plt.xlabel('Number of clusters')

    plt.ylabel('WSS')

    plt.title(df.name)



    plt.show()",[],[],[],[],[],23.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],15,[],[],0,0,0,0,0,0,0.0,0,0,True
49,code,[],"elbow(Summer)

elbow(Winter)

elbow(Fall)

elbow(Spring)",[],"['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],24.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,0,4,0,0,0,0.0,0,0,True
50,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['After observing the WSS v/s number of clusters graphs for all the seseaonal dataframes, the number of clusters chosen for each of the dataframe is **3**.']",0,0,0,0,0,0,,0,26,False
51,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'Applying k-Means Clustering using Sci-kit Learn on the NREL dataset']]",[],0,[],['## Applying k-Means Clustering using Sci-kit Learn on the NREL dataset'],0,0,0,0,0,0,,0,11,False
52,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],['The **k-Means** class function is used from **sci-kit learn** module to perform clustering on all the four **Seasonal** dataframes. The **n_clusters** attributes takes the input as an integer which is the number of clusters required in the output.'],0,0,0,0,0,0,,0,38,False
53,code,[],"def cluster(x):

    y=x.T # Since we need to assign households to clusters and not hour of the day, the dataframe is transposed

    cluster = KMeans(n_clusters = 3)

    # Adding a new column 'cluster_number' containing cluster assignment (0, 1 or 2) for each household.

    y['cluster_number']=cluster.fit_predict(y[y.columns[0:]])

    y.name = x.name

    return y","['Since we need to assign households to clusters and not hour of the day, the dataframe is transposed', ""Adding a new column 'cluster_number' containing cluster assignment (0, 1 or 2) for each household.""]",[],[],[],[],25.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],7,[],[],0,2,0,0,0,0,0.0,0,0,True
54,code,[],"s_cl = cluster(Summer)

w_cl = cluster(Winter)

f_cl = cluster(Fall)

sp_cl = cluster(Spring)",[],[],[],[],[],26.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
55,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'Clustering Results for the NREL dataset']]",[],0,[],['## Clustering Results for the NREL dataset'],0,0,0,0,0,0,,0,7,False
56,code,[],"# The following **Assignment** dataframe gives the cluster 

# assignment for each household for each season:



Assignment_df = pd.DataFrame(data=None, index=s_cl.index, columns=None, dtype=None, copy=False)

Assignment_df['Summer'] = s_cl['cluster_number']

Assignment_df['Winter'] = w_cl['cluster_number']

Assignment_df['Fall'] = f_cl['cluster_number']

Assignment_df['Spring'] = sp_cl['cluster_number']

Assignment_df.head()","['The following **Assignment** dataframe gives the cluster', 'assignment for each household for each season:']",[],[],[],"['text/html', 'text/plain']",27.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],9,[],[],0,2,0,0,1,0,0.0,0,0,True
57,code,[],"# To plot the clustered dataset of each seasonal dataframe the 

# following function **cluster_visualization** is formed. The 

# plots (one for each season) consists of mean normalized power 

# consumption for each household and a curve (darker color) 

# showing the average of mean mormalized power consumption over

# all the households.



def cluster_visulization(x):

    

    x_0 = x[(x.cluster_number==0)].reset_index(drop=True)

    x_0 = x_0.drop(['cluster_number'],axis=1)

    x_1 = x[(x.cluster_number==1)].reset_index(drop=True)

    x_1 = x_1.drop(['cluster_number'],axis=1)

    x_2 = x[(x.cluster_number==2)].reset_index(drop=True)

    x_2 = x_2.drop(['cluster_number'],axis=1)

    

    plt.figure(figsize=(15,8))

    plt.xlabel('Time of the Day', fontsize = 15)

    plt.ylabel('Normalized Average Power', fontsize = 15)

    plt.title(x.name, fontsize = 20)

    plt.plot((x_0).mean(), color='r')

    plt.plot(x_0.T, color = 'r', alpha = 0.03)

    plt.plot((x_1).mean(), color='b')

    plt.plot(x_1.T, color = 'b', alpha = 0.03)

    plt.plot((x_2).mean(), color='g')

    plt.plot(x_2.T, color = 'g', alpha = 0.05)

    

    c = mpatches.Patch(color='red', label='Cluster 1')

    d = mpatches.Patch(color='blue', label='Cluster 2')

    e = mpatches.Patch(color='green', label='Cluster 3')

    

    plt.legend(handles=[c,d,e])

    

    plt.show()     ","['To plot the clustered dataset of each seasonal dataframe the', 'following function **cluster_visualization** is formed. The', 'plots (one for each season) consists of mean normalized power', 'consumption for each household and a curve (darker color)', 'showing the average of mean mormalized power consumption over', 'all the households.']",[],[],[],[],28.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],34,[],[],0,6,0,0,0,0,0.0,0,0,True
58,code,[],"cluster_visulization(s_cl)

cluster_visulization(w_cl)

cluster_visulization(f_cl)

cluster_visulization(sp_cl)",[],"['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],29.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,0,4,0,0,0,0.0,0,0,True
59,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'Results and Interpretation']]",[],0,[],['## Results and Interpretation'],0,0,0,0,0,0,,0,4,False
60,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['The general trend in the electricity consumption is observed to be a peak in consumption during evening, possibly due to presence of people in household which would lead to use more appliances. Further there is continuous decrease in consumption of electricity in the night, possibly due to people going to sleep. There is a short rise in consumption in the morning around 6:00, when people in household get ready to leave for work or school. \n', '-\tFor the summer season’s data clustering, it is observed that households in cluster 1 on an average have a highest consumption for most of the day, except for 1:00 to 5:00, when cluster 2 has slightly higher consumption. Households assigned to cluster 3 shows a dip in electricity consumption during the afternoon, i.e. 12:00 to 5:00, possibly due to absence of any people in the home during that time.\n', '-\tFor the winter Cluster 1 and cluster 3 households show a two-peak energy consumption, while cluster 2 households show a pretty much stable use throughout the day. It is possible that there are people using appliances in households during afternoon which leads to comparatively higher consumption in cluster 2 compared to other two. All the households reach peak consumption during evening, however compared to summer time the consumption during afternoon is much higher, possibly due to use of electrical heaters. \n', '-\t For fall and spring, the trends in electricity consumption are pretty much the same. Also, the cluster allotment in these two seasons are similar. One cluster shows a stable consumption from 12:00 in afternoon to 9:00 in night, while the other two clusters show two different peaks in daily consumption, mainly in the morning around 6:00 and in evening around 7:00.\n']",0,0,0,0,0,0,,0,288,False
61,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://www.datascience.com/blog/k-means-clustering)', ('More about K-means', 'https://www.datascience.com/blog/k-means-clustering'), 'http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)', ('k-Means documentation', 'http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html')]","['\n', '[More about K-means](https://www.datascience.com/blog/k-means-clustering)\n', '\n', '[k-Means documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)']",0,0,0,0,0,0,,0,5,False
62,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[2, 'Principal Component Analysis for Data Visualiztion']]",[],0,[],['## Principal Component Analysis for Data Visualiztion'],0,0,0,0,0,0,,0,7,False
63,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['It would be helpful to visualize this clustered data to understand how well the data is grouped. In our case the data is 24-dimensional, each dimension representing the one hour of the day, making visualization challeging. Thus, PCA is used to reduce that 24-dimensional data into 2D so that we can plot and hopefully understand the data better. It should be noted that after dimensionality reduction, there usually isn’t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation. ']",0,0,0,0,0,0,,0,88,False
64,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['To compute principal components the **PCA** and **Standard Scalar** functions are used from the **sci-kit learn** module. First the **cluster_number** labelled data frame is separated into **features** and **target**, where features has all the columns except the labels (which is the cluster_number column) and target is the column containing the labels. After this the features dataframe is transformed into a **principal** dataframe containing the 2 principal components, using the **fit_transform** attribute of the pca function. \n', '\n', 'Before plotting this 2-D data, one must check the accuracy of the transformation. This is important as while when convert 24 dimensional space to 2 dimensional space, there is loss of some of the variance (information). By using the attribute **explained_variance_ratio**, we get a tuple with two numbers. The first number gives the variance (information) contained in first principal component while the second number gives the variance of the second principal component. The sum of the two components gives the total percentage of information conserved during the transformation. As a rule of thumb the PCA results are considered to be reliable only if this resultant sum is above 0.85, as loss of more than 15% information, leads to inaccurate results.']",0,0,0,0,0,0,,0,195,False
65,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)', ('Example of PCA', 'https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60')]",['[Example of PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)'],0,0,0,0,0,0,,0,3,False
66,code,[],"# Calculating PCA accuracy



def pca_accuracy (df):

    x=df.iloc[:,:-2].values

    y=df.iloc[:,-1].values

    pca=PCA(n_components=2)

    principal_components=pca.fit_transform(x)

    principal_df=pd.DataFrame(data=principal_components, columns = ['Component 1', 'Component 2'])

    z = pca.explained_variance_ratio_

    return (z[0]+z[1])",['Calculating PCA accuracy'],[],[],[],[],30.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],10,[],[],0,1,0,0,0,0,0.0,0,0,True
67,code,[],"print('Explained Variance Ratio for Summer season is', (pca_accuracy(s_cl)))

print('Explained Variance Ratio for Winter season is', (pca_accuracy(w_cl)))

print('Explained Variance Ratio for Fall season is', (pca_accuracy(f_cl)))

print('Explained Variance Ratio for Spring season is',(pca_accuracy(sp_cl)))",[],[],[],[],[],31.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,1,0,True
68,code,[],"# Since explained varience ratio is above 0.85 for only Winter,

# PCA is perforned for only this season.



x_component=w_cl.iloc[:,:-2].values

y_component=w_cl.iloc[:,-1].values

pca=PCA(n_components=2)

principal_components=pca.fit_transform(x_component)

principal_df=pd.DataFrame(data=principal_components, columns = ['Component 1', 'Component 2'])

y_component=pd.Series(y_component.T)

finalDf = pd.concat([principal_df, y_component], axis = 1, ignore_index=False)

finalDf=finalDf.rename(columns={0: 'target'})

finalDf.head()","['Since explained varience ratio is above 0.85 for only Winter,', 'PCA is perforned for only this season.']",[],[],[],"['text/html', 'text/plain']",32.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],12,[],[],0,2,0,0,1,0,0.0,0,0,True
69,code,[],"# Plotting final_df



plt.figure(figsize=(8,8))

plt.xlabel('Principal Component 1', fontsize = 15)

plt.ylabel('Principal Component 2', fontsize = 15)

plt.title('2 component PCA', fontsize = 20)

targets = [0, 1, 2]

colors = ['r', 'b', 'g']

for target, color in zip(targets,colors):

    indicesToKeep = finalDf['target'] == target

    plt.scatter(finalDf.loc[indicesToKeep, 'Component 1']

               , finalDf.loc[indicesToKeep, 'Component 2']

               , c = color

               , s = 50)

plt.legend(targets)

plt.grid()

plt.show()",['Plotting final_df'],"['image/png', 'text/plain']",[],[],[],33.0,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],17,[],[],0,1,1,0,0,0,0.0,0,0,True
70,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],['As expected the clusters with lowest euclidean distance to a given centroid are assigned to that centroid.'],0,0,0,0,0,0,,0,17,False
71,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'Application']]",[],0,[],"[""<a id='Application'></a>\n"", '# Application']",0,0,0,0,0,0,,0,4,False
72,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,[],"['Apart from making electricity consumption forecasts, this kind of household electricity consumption clustering has a huge significance in Demand Response program initiatives. Demand response refers to shifting electricity utilization from peak hours to low consumption time through time based price variation. By keeping the rates higher during peak electricity consumption and lower during lower consumption times, utilities can force consumers to utilize electricity during time of lower use, shifting the burden during peak consumption. Electric utilities can save millions of dollars and KW of energy through such shift in consumption.\n', '\n', ""Clustering of households based on their electricity consumption trends would help utilities identify which houses would prove to be responsive to such price variations and which houses won't. It is observed that households with peak electricity consumption in evening and morning are more responsive to demand response initiatives, compared to household that already have a nearly constant consumption throughout the afternoon and evening. Clustering is the easiest way to segregate households in such groups, thus helping utility companies decide the electricity prices to be kept during different time of the day, such that there is significant energy and monetary savings, with minimal increase in prices.""]",0,0,0,0,0,0,,0,195,False
73,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['http://ieeexplore.ieee.org/document/6693793/)', ('**Demand Response:** Kwac, Jungsuk, June Flora, and Ram Rajagopal. ""Household energy consumption segmentation using hourly data."" IEEE Transactions on Smart Grid 5.1 (2014): 420-430.', 'http://ieeexplore.ieee.org/document/6693793/')]","['[**Demand Response:** Kwac, Jungsuk, June Flora, and Ram Rajagopal. ""Household energy consumption segmentation using hourly data."" IEEE Transactions on Smart Grid 5.1 (2014): 420-430.](http://ieeexplore.ieee.org/document/6693793/)']",0,0,0,0,0,0,,0,24,False
74,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],"[[1, 'References']]",[],0,[],"[""<a id='References'></a>\n"", '# References']",0,0,0,0,0,0,,0,4,False
75,markdown,[],,[],[],[],[],[],,practicaldatascience..practicaldatascience.github.io..tutorial_final..223..PDS_Tutorial_final_2.ipynb,[],[],[],0,"['http://ieeexplore.ieee.org/document/5356176/)', ('Data: Impact of uncoordinated plug-in electric vehicle charging on residential power demand - supplementary data', 'http://ieeexplore.ieee.org/document/5356176/'), 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)', ('Isnull documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html'), 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html)', ('Resample documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html'), 'https://docs.python.org/3/library/datetime.html)', ('Datetime documentation', 'https://docs.python.org/3/library/datetime.html'), 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)', ('Group by documentation', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html'), 'http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)', ('Methods to determine optimal number of clusters', 'http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/'), 'https://www.datascience.com/blog/k-means-clustering)', ('More about K-means', 'https://www.datascience.com/blog/k-means-clustering'), 'http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)', ('k-Means documentation', 'http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html'), 'https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)', ('Example of PCA', 'https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60'), 'http://ieeexplore.ieee.org/document/6693793/)', ('**Demand Response:** Kwac, Jungsuk, June Flora, and Ram Rajagopal. ""Household energy consumption segmentation using hourly data."" IEEE Transactions on Smart Grid 5.1 (2014): 420-430.', 'http://ieeexplore.ieee.org/document/6693793/')]","['[Data: Impact of uncoordinated plug-in electric vehicle charging on residential power demand - supplementary data](http://ieeexplore.ieee.org/document/5356176/)\n', '\n', '[Isnull documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n', '\n', '[Resample documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html)\n', '\n', '[Datetime documentation](https://docs.python.org/3/library/datetime.html)\n', '\n', '[Group by documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)\n', '\n', '[Methods to determine optimal number of clusters](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)\n', '\n', '[More about K-means](https://www.datascience.com/blog/k-means-clustering)\n', '\n', '[k-Means documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n', '\n', '[Example of PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n', '\n', '[**Demand Response:** Kwac, Jungsuk, June Flora, and Ram Rajagopal. ""Household energy consumption segmentation using hourly data."" IEEE Transactions on Smart Grid 5.1 (2014): 420-430.](http://ieeexplore.ieee.org/document/6693793/)']",0,0,0,0,0,0,,0,63,False
0,code,[],"%load_ext autoreload

%autoreload 2

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

import scVI

import tensorflow as tf



from R_interop import SIMLR

from sklearn.decomposition import PCA





from helper import *

from benchmarking import *

% matplotlib inline",[],[],[],[],[],1.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],16,[],[],0,0,0,0,0,0,0.0,0,0,True
1,code,[],"data_path = ""/home/ubuntu/single-cell-scVI/data/Klein/""",[],[],[],[],[],2.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
2,markdown,[],,[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],"[[1, 'import and format']]",[],0,[],['# import and format'],0,0,0,0,0,0,,0,4,False
3,code,[],"X = pd.read_csv(data_path + ""bBM.raw_umifm_counts.csv"")

X.index = X[""cell_id""]

print X.shape

# remove this library to avoid dealing with batch effects

X.drop(X.index[X[""library_id""] == ""basal_bm1""], inplace=True)

print X.shape",['remove this library to avoid dealing with batch effects'],[],[],[],[],3.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],6,[],[],0,1,0,0,0,0,0.0,1,0,True
4,code,[],"Y = pd.read_csv(data_path + ""bBM.spring_and_pba.csv"")

Y.index = Y[""cell_id""]

gene_filter_list = np.loadtxt(data_path + ""bBM.filtered_gene_list.paper.txt"", dtype=np.str)

data = X.merge(Y, how=""inner"")

expression_data = data[gene_filter_list]

layout = data[[""x_spring"", ""y_spring""]]

meta = data[[""Potential"", ""Pr_Er"", ""Pr_Gr"", ""Pr_Ly"", ""Pr_DC"", ""Pr_Mk"", ""Pr_Mo"", ""Pr_Ba""]]

expression_data = expression_data.as_matrix()

expression_data = expression_data[:, np.mean(expression_data, axis=0) > 0.05]

expression_data = expression_data[:, np.std(expression_data, axis=0) / np.mean(expression_data, axis=0) > 2]

layout = layout.as_matrix()",[],[],[],[],[],4.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],11,[],[],0,0,0,0,0,0,0.0,0,0,True
5,code,[],"Z = pd.read_csv(data_path + ""bBM.filtered_normalized_counts.csv"")

Z.index = Z[""cell_id""]

Z.drop(Z.index[Z[""library_id""] == ""basal_bm1""], inplace=True)

gene_filter_list = np.loadtxt(data_path + ""bBM.filtered_gene_list.paper.txt"", dtype=np.str)

Y = pd.read_csv(data_path + ""bBM.spring_and_pba.csv"")

Y.index = Y[""cell_id""]

data = Z.merge(Y, how=""inner"")

expression_data = data[gene_filter_list]

layout = data[[""x_spring"", ""y_spring""]]

meta = data[[""Potential"", ""Pr_Er"", ""Pr_Gr"", ""Pr_Ly"", ""Pr_DC"", ""Pr_Mk"", ""Pr_Mo"", ""Pr_Ba""]]

expression_data = expression_data.as_matrix()

layout = layout.as_matrix()",[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],12,[],[],0,0,0,0,0,0,0.0,0,0,True
6,code,[],"selected = np.std(expression_data, axis=0).argsort()[-700:][::-1]

expression_data = expression_data[:, selected]",[],[],[],[],[],5.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
7,code,[],"print expression_data.shape[0], "" cells with "", expression_data.shape[1], "" genes""",[],[],[],[],[],6.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,1,0,False
8,code,[],"X_zero, i, j, ix = dropout(expression_data)

np.save(data_path + ""imputation/X_zero.npy"", X_zero)

np.save(data_path + ""imputation/i.npy"", i)

np.save(data_path + ""imputation/j.npy"", j)

np.save(data_path + ""imputation/ix.npy"", ix)",[],[],[],[],[],17.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
9,code,[],"X_zero, i, j, ix = \

        np.load(data_path + ""imputation/X_zero.npy""), np.load(data_path + ""imputation/i.npy""),\

        np.load(data_path + ""imputation/j.npy""), np.load(data_path + ""imputation/ix.npy"")",[],[],[],[],[],7.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
10,code,[],"mean_MAGIC = np.load(data_path + ""imputation/X_zero_MAGIC.npy"")

print(""MAGIC"", imputation_error(mean_MAGIC, expression_data, X_zero, i, j, ix))",[],[],[],[],[],8.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,1,0,True
11,code,[],"log_library_size = np.log(np.sum(expression_data, axis=1))

zero_amount = np.sum(expression_data == 0, axis=1)

mean, var = np.mean(log_library_size), np.var(log_library_size)",[],[],[],[],[],9.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
12,code,[],"#lr 0.0004

#latent 10



learning_rate = 0.004

epsilon = 0.01



tf.reset_default_graph()

expression = tf.placeholder(tf.float32, (None, expression_data.shape[1]), name='x')

kl_scalar = tf.placeholder(tf.float32, (), name='kl_scalar')

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)

training_phase = tf.placeholder(tf.bool, (), name='training_phase')



model = scVI.scVIModel(expression=expression, kl_scale=kl_scalar, \

                         optimize_algo=optimizer, phase=training_phase, \

                          library_size_mean=mean, library_size_var=var, n_latent=60)



# Session creation

sess = tf.Session()","['lr 0.0004', 'latent 10', 'Session creation']",[],[],[],[],27.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],18,[],[],0,3,0,0,0,0,0.0,1,0,True
13,code,[],"sess.run(tf.global_variables_initializer())

res = train_model(model, (expression_data, expression_data), sess, 120, kl=0)",[],[],[],[],[],28.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
14,code,[],meta.head(),[],[],[],[],"['text/html', 'text/plain']",13.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
15,code,[],"sess.run(tf.global_variables_initializer())

res = train_model(model, (X_zero, expression_data), sess, 150)

eval_imputed_data(model, (X_zero, i, j, ix), expression_data, sess)",[],[],[],[],[],23.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,1,0,True
16,code,[],"from sklearn.manifold import TSNE

from sklearn.neighbors import kneighbors_graph

import networkx as nx



def show_viz(latent, name, algo=None, labels=None, clusters_cmap=7, cmap=""tab10"", return_layout=False):

    

    if clusters_cmap == 0:

        cmap = plt.get_cmap(cmap)

    else:

        cmap = plt.get_cmap(cmap, clusters_cmap)

        

    if labels is None:

        labels = c_train

        

    if algo == ""tSNE"":

        layout = TSNE().fit_transform(latent)

    elif algo == ""kNN"":

        ad = kneighbors_graph(latent, 5, include_self=False)

        graph = nx.from_scipy_sparse_matrix(ad)

        lag_node = 0.9

        pos = nx.spring_layout(graph, k=np.sqrt(1.0/(lag_node*ad.shape[0])), iterations=200)

        layout =  np.concatenate([x[:, np.newaxis] for x in pos.values()], axis=1).T   

    else:

        layout = latent

        

        

    plt.figure(figsize=(10, 10))

    cax = plt.scatter(layout[:, 0], layout[:, 1], c=labels, \

                                   cmap=cmap, edgecolors='none')

    plt.axis(""off"")

    if algo is None:

        st = ""tSNE""

    else:

        st = algo

    plt.tight_layout()

    if return_layout:

        return layout",[],[],[],[],[],35.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],37,[],[],0,0,0,0,0,0,0.0,0,0,True
17,code,[],"def logit(p):

    p = np.copy(p.as_matrix())

    p[p == 0] = np.min(p[p > 0])

    p[p == 1] = np.max(p[p < 1])

    return np.log(p / (1-p))

# show_viz(layout, ""PBA"", algo=None, labels=logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \

#              clusters_cmap=0, cmap=""viridis"")

#cbar = plt.colorbar(cax, ticks=[-40, 0, 40], orientation='horizontal')

#cbar.ax.set_xticklabels(['Erythroblasts', 'Other', 'Granulocytes'])  # horizontal colorbar

#plt.savefig(""PBA_scale.pdf"", dpi=300)

potential_scaled = np.log(1 + np.max(meta.iloc[:, 0])-meta.iloc[:, 0])

show_viz(layout, ""PBA"", algo=None, labels=potential_scaled , \

             clusters_cmap=0, cmap=""viridis"")","['show_viz(layout, ""PBA"", algo=None, labels=logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \\', 'clusters_cmap=0, cmap=""viridis"")', ""cbar = plt.colorbar(cax, ticks=[-40, 0, 40], orientation='horizontal')"", ""cbar.ax.set_xticklabels(['Erythroblasts', 'Other', 'Granulocytes'])    horizontal colorbar"", 'plt.savefig(""PBA_scale.pdf"", dpi=300)']","['image/png', 'text/plain']",[],[],[],16.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],13,[],[],0,5,1,0,0,0,0.0,0,0,True
18,code,[],"def alpha_cmap(cmap, min_alpha=-1, max_alpha=1):

    cm = plt.get_cmap(cmap)

    cm._init()

    alphas = np.abs(np.linspace(0.0, 1.0, cm._lut[:,-1].shape[0]))

    cm._lut[:,-1] = alphas

    return cm",[],[],[],[],[],13.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],6,[],[],0,0,0,0,0,0,0.0,0,0,True
19,code,[],"import matplotlib.patches as mpatches

import matplotlib.pyplot as plt



def plot_layout(layout=None):

    if layout is not None:

        plt.figure(figsize=(10, 10))

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 1], \

                                       cmap=alpha_cmap(""Reds""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 2], \

                                       cmap=alpha_cmap(""Blues""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 3], \

                                       cmap=alpha_cmap(""Greens""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 4], \

                                       cmap=alpha_cmap(""Purples""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 5], \

                                       cmap=alpha_cmap(""Oranges""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 6], \

                                       cmap=alpha_cmap(""spring""), edgecolors='none')

        plt.scatter(layout[:, 0], layout[:, 1], c=meta.iloc[:, 7], \

                                       cmap=alpha_cmap(""copper""), edgecolors='none')

    else:

        plt.figure(figsize=(5, 5))

        patch_1 = mpatches.Patch(color=plt.get_cmap(""Reds"")(.8), label='Erythroid')

        patch_2 = mpatches.Patch(color=plt.get_cmap(""Blues"")(.8), label='Granulocytic Neutrophil')

        patch_3 = mpatches.Patch(color=plt.get_cmap(""Greens"")(.8), label='Lymphocytic')

        patch_4 = mpatches.Patch(color=plt.get_cmap(""Purples"")(.8), label='Dendritic')

        patch_5 = mpatches.Patch(color=plt.get_cmap(""Oranges"")(.8), label='Megakaryocytic')

        patch_6 = mpatches.Patch(color=plt.get_cmap(""spring"")(.8), label='Monocytic')

        patch_7 = mpatches.Patch(color=plt.get_cmap(""copper"")(.8), label='Basophilic')



        plt.legend(handles=[patch_1, patch_2, patch_3, patch_4, patch_5, patch_6, patch_7])

        

    plt.axis(""off"")",[],[],[],[],[],14.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],33,[],[],0,0,0,0,0,0,0.0,0,0,True
20,code,[],meta.columns,[],[],[],[],['text/plain'],219.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
21,code,[],"plot_layout()

plt.savefig(""Klein_full_legend.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],221.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
22,code,[],"plot_layout(layout)

plt.savefig(""layout_klein_kNN.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],161.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
23,code,[],"simlr = SIMLR.SIMLR(n_clusters=10)

%time simlr.estimate_clusters_numbers(expression_data)",[],[],[],[],[],72.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,1,0,True
24,code,[],"simlr.estimated_clusters

# SIMLR find 5 clusters with two different heuristics",['SIMLR find 5 clusters with two different heuristics'],[],[],[],['text/plain'],73.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,1,0,0,1,0,0.0,0,0,True
25,code,[],"simlr = SIMLR.SIMLR(n_clusters=5)

%time simlr.fit_transform(expression_data)",[],[],[],[],['text/plain'],20.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,1,0,True
26,markdown,[],,[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],"[[2, 'Latent Space tSNE']]",[],0,[],['## Latent Space tSNE'],0,0,0,0,0,0,,0,4,False
27,code,[],"latent_scVI = eval_latent(model, expression_data, sess)

layout_scVI = show_viz(latent_scVI, ""scVI+"", algo=""kNN"", labels = potential_scaled, \

         clusters_cmap=0, cmap=""viridis"", return_layout=True)

#plt.savefig(""scVIX0_klein_kNN.pdf"", dpi=300)","['plt.savefig(""scVIX0_klein_kNN.pdf"", dpi=300)']","['image/png', 'text/plain']",[],[],[],36.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],4,[],[],0,1,1,0,0,0,0.0,0,0,True
28,code,[],"plot_layout(layout_scVI)

plt.savefig(""scVIX0_klein_kNN.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],37.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
29,code,[],"latent_pc = PCA(n_components=10).fit_transform(np.log(1 + expression_data))

show_viz(latent_pc, \

          ""PCA100"", algo=""kNN"", labels=logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \

         clusters_cmap=0, cmap=""viridis"")

plt.savefig(""figures/PCA_klein_kNN.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],89.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],5,[],[],0,0,1,0,0,0,0.0,0,0,True
30,code,[],"latent_pc = PCA(n_components=60).fit_transform(np.log(1 + expression_data))

layout_pc = show_viz(latent_pc, \

          ""PCA100"", algo=""kNN"", labels=potential_scaled, \

         clusters_cmap=0, cmap=""viridis"", return_layout=True)",[],"['image/png', 'text/plain']",[],[],[],151.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],4,[],[],0,0,1,0,0,0,0.0,0,0,True
31,code,[],"plot_layout(layout_pc)

plt.savefig(""PCA60_klein_kNN.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],162.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
32,code,[],"print model.n_latent

latent_scVI = eval_latent(model, expression_data, sess)

show_viz(latent_scVI, ""scVI10"", algo=""kNN"", labels = logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \

         clusters_cmap=0, cmap=""viridis"", return_layout=True)

plt.savefig(""figures/scVI_klein_kNN.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],97.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],5,[],[],0,0,1,0,0,0,0.0,1,0,True
33,code,[],meta.columns,[],[],[],[],['text/plain'],27.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
34,code,[],"show_viz(layout, ""scVI10"", labels = meta.iloc[:, 6], \

         clusters_cmap=0, cmap=""viridis"", return_layout=True)

plt.colorbar()",[],"['image/png', 'text/plain']",[],[],['text/plain'],123.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,0,1,0,1,0,0.0,0,0,True
35,code,[],"show_viz(simlr.ydata, ""SIMLR"", algo=None, labels=logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \

         clusters_cmap=0, cmap=""viridis"")

#plt.savefig(""SIMLR_klein_tSNE.pdf"", dpi=300)","['plt.savefig(""SIMLR_klein_tSNE.pdf"", dpi=300)']","['image/png', 'text/plain']",[],[],[],76.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,1,1,0,0,0,0.0,0,0,True
36,code,[],"show_viz(simlr.F, ""SIMLR"", algo=""kNN"", labels=logit(meta.iloc[:, 2]) - logit(meta.iloc[:, 1]), \

         clusters_cmap=0, cmap=""viridis"")

#plt.savefig(""SIMLR_klein_kNN.pdf"", dpi=300)","['plt.savefig(""SIMLR_klein_kNN.pdf"", dpi=300)']","['image/png', 'text/plain']",[],[],[],77.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],3,[],[],0,1,1,0,0,0,0.0,0,0,True
37,markdown,[],,[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],"[[4, 'distance matrix']]",[],0,[],['#### distance matrix'],0,0,0,0,0,0,,0,3,False
38,code,[],"pca = PCA(n_components=10)

% time latent_pc10 = pca.fit_transform(np.log(1 + expression_data))",[],[],[],[],[],100.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,1,0,True
39,code,[],from matplotlib import gridspec,[],[],[],[],[],101.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
40,code,[],"def visualize_distance(latent, labels, algorithm):

    order_latent = np.vstack([x for _, x in sorted(zip(labels,latent), key=lambda pair: pair[0])])

    order_label = np.vstack([y for y, x in sorted(zip(labels,latent), key=lambda pair: pair[0])])

    distance = scipy.spatial.distance_matrix(order_latent, order_latent)  

    

    fig = plt.figure(figsize=(10, 10)) 

    gs = gridspec.GridSpec(2, 2, width_ratios=[1, 70], height_ratios=[1, 70])

    gs.update(wspace=0.05, hspace=0.05)

    ax0 = plt.subplot(gs[1])

    ax0.imshow(order_label.T, cmap='viridis', interpolation='none', aspect=100)

    ax0.axis('off')

    ax1 = plt.subplot(gs[3], sharex=ax0)

    ax1.imshow(distance, cmap='hot', interpolation='none')

    ax1.axis('off')

    ax2 = plt.subplot(gs[2], sharey=ax1)

    ax2.imshow(order_label, cmap='viridis', interpolation='none', aspect=1/100.)

    ax2.axis('off')

    #plt.tight_layout()

    fig.suptitle(""Cell-Cell Similarity matrix on the ""+ algorithm + "" latent space"", fontsize=16, y=0.92)",['plt.tight_layout()'],[],[],[],[],102.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],19,[],[],0,1,0,0,0,0,0.0,0,0,True
41,code,[],"visualize_distance(latent_pc, logit(meta.iloc[:, 2]), ""PC"")

plt.savefig(""PCA_klein_distance.pdf"")",[],"['image/png', 'text/plain']",[],[],[],103.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
42,code,[],"visualize_distance(simlr.F, logit(meta.iloc[:, 2]), ""SIMLR"")

plt.savefig(""SIMLR_klein_distance.pdf"", dpi=300)",[],"['image/png', 'text/plain']",[],[],[],107.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
43,code,[],"visualize_distance(latent_scVI, logit(meta.iloc[:, 2]), ""scVI"")

plt.savefig(""scVI_klein_distance.pdf"")",[],"['image/png', 'text/plain']",[],[],[],106.0,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],2,[],[],0,0,1,0,0,0,0.0,0,0,True
44,markdown,[],,[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],"[[1, 'Imputation']]",[],0,[],['# Imputation'],0,0,0,0,0,0,,0,2,False
45,code,[],,[],[],[],[],[],,romain-lopez..scVI-reproducibility..HEMATO-filtered.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
0,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, 'Prepearing data']]",[],0,[],['## Prepearing data'],0,0,0,0,0,0,,0,3,False
1,code,[],"import xgboost

import numpy as np

import pandas as pd

import shap



# load JS visualization code to notebook

shap.initjs() ",['load JS visualization code to notebook'],"['text/html', 'text/plain']",[],[],[],1.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],7,[],[],0,1,1,0,0,0,0.0,1,0,True
2,code,[],"data_features = pd.read_hdf('/home/olgako/data/data_features_JetHT.hdf5', ""data"")

labels = 1-pd.read_hdf('/home/olgako/data/labels_JetHT.hdf5', 'labels')

sublabels = pd.read_hdf('//data/cms2010/data_not_f_JetHT.hdf5', ""data"")",[],[],[],[],[],2.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],3,[],[],0,0,0,0,0,0,0.0,0,0,True
3,code,[],"data_features.shape, sublabels.shape",[],[],[],[],['text/plain'],3.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
4,code,[],np.where(sublabels['new_json'] != labels),[],[],[],[],['text/plain'],4.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
5,code,[],"from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_curve, auc, roc_auc_score



import time",[],[],[],[],[],5.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],4,[],[],0,0,0,0,0,0,0.0,0,0,True
6,code,[],"indx_train = np.arange(data_features.shape[0]-int(data_features.shape[0]/5), dtype='int32')

indx_test = np.arange(data_features.shape[0]-int(data_features.shape[0]/5),data_features.shape[0], dtype='int32')



#indx_train, indx_test = train_test_split(np.arange(data.shape[0], dtype='int32'), stratify=labels, test_size=0.1, random_state = 1)","[""indx_train, indx_test = train_test_split(np.arange(data.shape[0], dtype='int32'), stratify=labels, test_size=0.1, random_state = 1)""]",[],[],[],[],6.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],4,[],[],0,1,0,0,0,0,0.0,0,0,True
7,code,[],"num_good = np.sum(labels)

num_bad = len(labels)-np.sum(labels)



weights = 0.5 / np.where(labels == 0.0, num_good, num_bad)

weights *= len(labels)",[],[],[],[],[],7.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],5,[],[],0,0,0,0,0,0,0.0,0,0,True
8,code,[],"y_train = np.array(labels.iloc[indx_train], 'float32')

#y_val = np.array(labels.iloc[indx_val], 'float32')

y_test = np.array(labels.iloc[indx_test], 'float32')



X_train = np.array(data_features.iloc[indx_train], 'float32')

#X_val = np.array(data_features.iloc[indx_val], 'float32')

X_test = np.array(data_features.iloc[indx_test], 'float32')



weights_train = weights[indx_train]

#weights_val = weights[indx_val]

weights_test = weights[indx_test]



ids_train = sublabels[['runId', 'lumiId']].iloc[indx_train]

ids_test = sublabels[['runId', 'lumiId']].iloc[indx_test]","[""y_val = np.array(labels.iloc[indx_val], 'float32')"", ""X_val = np.array(data_features.iloc[indx_val], 'float32')"", 'weights_val = weights[indx_val]']",[],[],[],[],8.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],14,[],[],0,3,0,0,0,0,0.0,0,0,True
9,code,[],feature_names = data_features.columns,[],[],[],[],[],9.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
10,code,[],"Muon_features = [s for s in feature_names if (s[:3] == 'qMu')]# and (s[3:7] != 'Cosm')]

Pho_features = [s for s in feature_names if s[:4] == 'qPho']

Cal_features = [s for s in feature_names if s[:4] == 'qCal']

PF_features = [s for s in feature_names if s[:3] == 'qPF']



channels_features = dict()



channels_features['muons'] = Muon_features

channels_features['photons'] = Pho_features

channels_features['PF'] = PF_features

channels_features['calo'] = Cal_features



[ (g, len(fs)) for g, fs in channels_features.items() ]","[""and (s[3:7] != 'Cosm')]""]",[],[],[],"['text/html', 'text/plain']",10.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],13,[],[],0,1,0,0,1,0,0.0,0,0,True
11,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, 'train any classifier']]",[],0,[],['## train any classifier'],0,0,0,0,0,0,,0,4,False
12,code,[],"import xgboost as xgb

xgb_clf  = xgb.XGBClassifier(n_estimators=80, max_depth=10, eta=0.1, n_jobs=-1, random_state=111, silent=False)

xgb_clf.fit(X_train, y_train)",[],[],[],[],['text/plain'],11.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],3,[],[],0,0,0,0,1,0,0.0,0,0,True
13,code,[],model = xgb_clf,[],[],[],[],[],12.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,0,0,0.0,0,0,True
14,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, 'some runs with given cause of anomaly']]",[],0,[],['## some runs with given cause of anomaly'],0,0,0,0,0,0,,0,8,False
15,code,[],"dict_causes = {

 273150:"" [[61, 64], [66, 75]],     	Ecal excluded"",

 273296:"" [[1, 94]],			Pixe excluded"",

 273426:"" [[1, 59], [64, 64], [66, 69]],inefficeincy in EE"",

 273445:"" [[5, 6]],			tracker off  seen by many systems"",

 274142:"" [[99, 100]],			pixel and tracker off"",

 274157:"" [[103, 534]],		EB minus has region with low efficiency"",

 274282:"" [[86, 88]],			tracker is off --> particle flow object are affected"",

 275326:"" [[1, 169]],			sistrip excl"",

 275757:"" [[104, 122]],		low DCS: HBHE. nothing on PF plot"",

 275758:"" [[1, 4]],			part of hcal not available --> JetMET reconstruction suffers"",

 275764:"" [[1, 31]],			pixel off"",

 275766:"" [[1, 23], [25, 60]],		lower tracker efficiency"",

 275768:"" [[1, 79]],			lower tracker efficiency"",

 275769:"" [[1, 22]],			pixel not in Data Acquisition"",

 275781:"" [[1, 29]],			EB+16, +17, +18 are excluded due to a problem with VME crateS206h"",

 275783:"" [[1, 1634]],			strip EXCL"",

 275838:"" [[1, 51]],			strip EXCL"",

 275922:"" [[4, 6]],			low stat"",

 276064:"" [[1, 22]],			hot trigger tower seen in EE-07 (high occupancy and Et TP) which cause  ~100% deadtime"",

 276071:"" [[1, 22]],			strip EXCL"",

 276095:"" [[1, 5]],			low stat"",

 276237:"" [[1, 5]],			ECAL, HCAL, PIXEL excluded"",

 276453:"" [[1, 8], [10, 125]],		EB-17 (FED 626): was excluded (because of cooling failure)"",

 276455:"" [[1, 401]],			strip EXCL"",

 276456:"" [[1, 182]],			strip EXCL"",

 276457:"" [[1, 19]],			sistrip not in DAQ"",

 277217:"" [[12, 14]],			Short collision run with strip in error in DCS and pixel in error in DAQ. For physically meaningful lumisections 33-47, the total rate is zero. L1T flags marked as bad due to this"",

 277933:"" [[1, 42]],			ECAL EE+09 FED 648 removed from all LS in the run because it was causing 100% dead time. EE+09 was not masked, so all LS in this run are bad"",

 278309:"" [[1, 10]],			EE-04 FED 607 TT disabled in LS [1-10] according to express dataset (LS# 10)"",

 278821:"" [[1, 33], [36, 37]],		FED652 in error, EE+04 is off"",

 279028:"" [[1, 17]],			strip EXCL"",

 279995:"" [[1, 8]],			hcal water colling issues"",

 280002:"" [[1, 111]],			ecal excluded"",

 280006:"" [[1, 68]],			EB-11 token ring missing"",

 280007:"" [[1, 36]],			Low voltage channel broken in EB"",

 280239:"" [[1, 9]],			strip EXCL"",

 280241:"" [[1, 11]],			strip EXCL"",

 281663:"" [[61, 172]],			Timing shifted by 5ns due to TCDS* problem "",

 281674:"" [[1, 45]],			Timing shifted by 5ns due to TCDS* problem"",

 281680:"" [[1, 31]],			Timing shifted by 5ns due to TCDS* problem"",

 281974:"" [[79, 85]],			pixel High voltage OFF"",

 282408:"" [[80, 191]],			strip EXCL"",

 282707:"" [[79, 81]],			pixel High voltage OFF"",

 282796:"" [[80, 82]],			pixel High Voltage off"",

 282921:"" [[1, 152]]		        strip EXCL""



    

}","['10)"",']",[],[],[],[],14.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],49,[],[],0,1,0,0,0,0,0.0,0,0,True
16,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, 'SHAP to explain predictions']]",[],0,[],['## SHAP to explain predictions'],0,0,0,0,0,0,,0,5,False
17,code,[],"shap_values_test = shap.TreeExplainer(model).shap_values(X_test)

shap_values_train = shap.TreeExplainer(model).shap_values(X_train)",[],[],[],[],[],16.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],2,[],[],0,0,0,0,0,0,0.0,0,0,True
18,code,[],"min_num_neg = 2000

for i in range(shap_values_train.shape[0]):

    cur = len(shap_values_train[i,:-1][shap_values_train[i,:-1]<0])

    if cur < min_num_neg:

        min_num_neg = cur

for i in range(shap_values_test.shape[0]):

    cur = len(shap_values_train[i,:-1][shap_values_test[i,:-1]<0])

    if cur < min_num_neg:

        min_num_neg = cur

        

print(min_num_neg)",[],[],[],[],[],17.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],11,[],[],0,0,0,0,0,0,0.0,1,0,True
19,code,[],"for run in dict_causes.keys():

    if run in sublabels['runId'].values:

        print('')

        print (run, dict_causes[run])

        print('')

        

        if run in ids_train['runId'].values:

            print ('this run from TRAIN set')

            shap_values = shap_values_train

            search_for_ind = ids_train

            X = X_train

            

        else:

            print ('this run from TEST set')

            shap_values = shap_values_test

            search_for_ind = ids_test

            X = X_test

            

        

        inds = np.where(np.array(search_for_ind)==run)[0]



        print (""sum of features' influences from all lumis, 20 the most important"")

        print('')

        sum_shap = np.sum(shap_values[inds,:-1], axis=0)

        print (feature_names[np.argsort(sum_shap, axis=0)][:20])

        print('')

        print(""the most common features with big shap values"")

        print('')

        top_sh_values = np.argsort(shap_values[inds,:-1], axis=1)[:,:50].T.reshape(-1)

        unique, first_index, counts = np.unique(

            top_sh_values,

        return_counts=True, return_index=True)

        print(feature_names[top_sh_values[np.sort(first_index[counts == len(inds)])]])

        print('')

        #random example 

        #shap.force_plot(shap_values[np.random.choice(inds,1),:], feature_names)

        shap.summary_plot(shap_values[inds,:], features=X[inds, :], feature_names=feature_names)

        

        channels_shap = pd.DataFrame()



        for i in shap_values[inds,:-1]:



            pred_for_channnel = {}

            for channel in channels_features:

                current_sum = 0

                for feature in channels_features[channel]:

                    current_sum += i[list(feature_names).index(feature)]

                pred_for_channnel[channel] = current_sum

            channels_shap = channels_shap.append(pred_for_channnel, ignore_index=True)



        print ('shap for channel for lumi')

        print (channels_shap)

        print("" "")

        print ('shap for channel for run')

        print(channels_shap.sum())

        print("" "")

        print(""_""*100)

        

    else:

        print (str(run) +' is not found')","['random example', 'shap.force_plot(shap_values[np.random.choice(inds,1),:], feature_names)']","['image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain', 'image/png', 'text/plain']",[],[],[],18.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],60,[],[],0,2,14,0,0,0,0.0,15,0,True
20,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, 'some examples for one sample seperately']]",[],0,[],['## some examples for one sample seperately'],0,0,0,0,0,0,,0,7,False
21,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""275768"": [[1, 79]], lower tracker efficiency']]",[],0,[],"['##  ""275768"": [[1, 79]],\t\t\tlower tracker efficiency']",0,0,0,0,0,0,,0,7,False
22,code,[],"inds = np.where(np.array(ids_train)==275768)[0]

inds",[],[],[],[],['text/plain'],19.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
23,code,[],"shap.force_plot(shap_values_train[583,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",20.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
24,code,[],"shap.force_plot(shap_values_train[617,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",21.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
25,code,[],"shap.force_plot(shap_values_train[656,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",22.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
26,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""279995"": [[1, 8]], hcal water colling issues']]",[],0,[],"['##  ""279995"": [[1, 8]],\t\t\thcal water colling issues']",0,0,0,0,0,0,,0,8,False
27,code,[],np.where(np.array(ids_train)==279995),[],[],[],[],['text/plain'],23.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
28,code,[],"shap.force_plot(shap_values_train[108963,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",24.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
29,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""275764"": [[1, 31]], pixel off']]",[],0,[],"['##  ""275764"": [[1, 31]],\t\t\tpixel off']",0,0,0,0,0,0,,0,6,False
30,code,[],"inds = np.where(np.array(ids_train)==275764)[0]

inds",[],[],[],[],['text/plain'],25.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],2,[],[],0,0,0,0,1,0,0.0,0,0,True
31,code,[],"shap.force_plot(shap_values_train[500,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",26.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
32,code,[],"shap.force_plot(shap_values_train[523,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",27.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
33,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""275758"": [[1, 4]], part of hcal not available --> JetMET reconstruction suffers']]",[],0,[],"['##  ""275758"": [[1, 4]],\t\t\tpart of hcal not available --> JetMET reconstruction suffers\n']",0,0,0,0,0,0,,0,13,False
34,code,[],np.where(np.array(ids_train)==275758),[],[],[],[],['text/plain'],28.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
35,code,[],"shap.force_plot(shap_values_train[476,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",29.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
36,code,[],"shap.force_plot(shap_values_train[478,:], feature_names[:])",[],[],[],[],"['text/html', 'text/plain']",30.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
37,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""275757"": [[104, 122]], low DCS: HBHE. nothing on PF plot.']]",[],0,[],"['##  ""275757"": [[104, 122]],\t\tlow DCS: HBHE. nothing on PF plot.']",0,0,0,0,0,0,,0,11,False
38,code,[],np.where(np.array(ids_train)==275757)[0],[],[],[],[],['text/plain'],31.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
39,code,[],"shap.force_plot(shap_values_train[459,:], feature_names)",[],[],[],[],"['text/html', 'text/plain']",32.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
40,code,[],"shap.force_plot(shap_values_train[460,:], feature_names)",[],[],[],[],"['text/html', 'text/plain']",33.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
41,code,[],"shap.force_plot(shap_values_train[461,:], feature_names)",[],[],[],[],"['text/html', 'text/plain']",34.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
42,code,[],"shap.force_plot(shap_values_train[475,:], feature_names)",[],[],[],[],"['text/html', 'text/plain']",35.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
43,markdown,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],"[[2, '""280007"": [[1, 36]], Low voltage channel broken in EB-']]",[],0,[],"['##  ""280007"": [[1, 36]],\t\t\tLow voltage channel broken in EB-']",0,0,0,0,0,0,,0,10,False
44,code,[],np.where(np.array(ids_train)==280007),[],[],[],[],['text/plain'],36.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
45,code,[],"shap.force_plot(shap_values_train[109030,:], feature_names=feature_names)",[],[],[],[],"['text/html', 'text/plain']",37.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
46,code,[],"shap.force_plot(shap_values_train[109031,:], feature_names=feature_names)",[],[],[],[],"['text/html', 'text/plain']",38.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
47,code,[],"shap.force_plot(shap_values_train[109032,:], feature_names=feature_names)",[],[],[],[],"['text/html', 'text/plain']",39.0,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],1,[],[],0,0,0,0,1,0,0.0,0,0,True
48,code,[],,[],[],[],[],[],,yandexdataschool..cms-dqm..notebooks..shap-new.ipynb,[],[],[],0,[],[],0,0,0,0,0,0,0.0,0,0,True
