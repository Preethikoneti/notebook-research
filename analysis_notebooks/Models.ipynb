{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ML Framework Use &lt;](Frameworks.ipynb) | [&gt; Visualizations](Visualizations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What types of ML models are people using?\n",
    "All results within the context of machine learning with sklearn.\n",
    "\n",
    "\n",
    "## Results Summary:\n",
    "- Supervised learning is much more common than unsupervised learning.\n",
    "- Within supervised models, linear models are the most common, followed by ensemble methods.\n",
    "- Within unsupervised models, decomposition is the most popular, followed by clustering.\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_temp = load_data.load_notebooks()\n",
    "repos_temp = load_data.load_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load aggregated dataframe. Code used to create it is in [aggregate.py](aggregate.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_uses_df_temp = load_data.load_framework_uses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tidy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only looking at Python notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks = notebooks_temp.copy()[notebooks_temp.lang_name == 'python'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:,} ({1}%) of notebooks not in ipynb checkpoints were written in Python. The remaining {2}% have been removed.\".format(\n",
    "    len(notebooks),\n",
    "    round(100*len(notebooks)/len(notebooks_temp), 2),\n",
    "    round(100 - 100*len(notebooks)/len(notebooks_temp), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update repos and aggregated dataframe to reflect notebooks in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = repos_temp.copy()[repos_temp.repo_id.isin(notebooks.repo_id)].reset_index(drop=True)\n",
    "framework_uses_df = framework_uses_df_temp.copy()[framework_uses_df_temp.file.isin(notebooks.file)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(framework_uses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete temp dataframes to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del notebooks_temp\n",
    "del framework_uses_df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Manipulate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs unsupervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_type = {\n",
    "    'sklearn.linear_model': 'S',\n",
    "    'sklearn.discriminant_analysis': 'S',\n",
    "    'sklearn.kernel_ridge': 'S',\n",
    "    'sklearn.svm': 'S',\n",
    "    'sklearn.SGDClassifier': 'S',\n",
    "    'sklearn.neighbors': 'S',\n",
    "    'sklearn.gaussian_process': 'S',\n",
    "    'sklearn.cross_decomposition': 'S',\n",
    "    'sklearn.naive_bayes': 'S',\n",
    "    'sklearn.tree': 'S',\n",
    "    'sklearn.ensemble': 'S',\n",
    "    'sklearn.multiclass': 'S',\n",
    "    'sklearn.feature_selection': 'S',\n",
    "    'sklearn.semi_supervised': 'S',\n",
    "    'sklearn.isotonic': 'S',\n",
    "    'sklearn.calibration': 'S',\n",
    "    'sklearn.neural_network': 'S',\n",
    "    \n",
    "    'sklearn.mixture': 'U',\n",
    "    'sklearn.manifold': 'U',\n",
    "    'sklearn.cluster': 'U',\n",
    "    'sklearn.decomposition': 'U',\n",
    "    'sklearn.covariance': 'U',\n",
    "    #'sklearn.neural_network.BernoulliRBM': 'U'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_supervised = []\n",
    "all_unsupervised = []\n",
    "sklearn_uses = []\n",
    "for framework_uses in framework_uses_df.uses:\n",
    "    nb_sklearn_uses = []\n",
    "    for framework in framework_uses:\n",
    "        if framework == 'sklearn':\n",
    "            for use in framework_uses[framework]:\n",
    "                major = '.'.join(use.split('.')[:2])\n",
    "                if major in model_to_type:\n",
    "                    t = model_to_type[major]\n",
    "                    if t == 'S':\n",
    "                        if use.startswith('sklearn.neural_network.BernoulliRBM'):\n",
    "                            all_unsupervised.append(major)\n",
    "                            nb_sklearn_uses.append(major)\n",
    "                        else:\n",
    "                            all_supervised.append(major)\n",
    "                            nb_sklearn_uses.append(major)\n",
    "                    else:\n",
    "                        all_unsupervised.append(major)\n",
    "                        nb_sklearn_uses.append(major)\n",
    "    sklearn_uses.append(nb_sklearn_uses)\n",
    "    \n",
    "framework_uses_df['sklearn_models'] = sklearn_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "for model in set(load_data.flatten(framework_uses_df.sklearn_models)):\n",
    "    framework_uses_df[model] = [model in m for m in framework_uses_df.sklearn_models]\n",
    "    print(model, datetime.datetime.now() - start)\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_s_counts = pd.Series(all_supervised).value_counts().reset_index().rename(\n",
    "    columns = {'index':'model',0:'count'}\n",
    ")\n",
    "all_u_counts = pd.Series(all_unsupervised).value_counts().reset_index().rename(\n",
    "    columns = {'index':'model',0:'count'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model use over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 seconds\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "framework_uses_time_df = framework_uses_df.merge(\n",
    "    notebooks[['file','repo_id']], on = 'file'\n",
    ").merge(\n",
    "    repos[['repo_id','pushed_at']], on = 'repo_id'\n",
    ")\n",
    "\n",
    "framework_uses_time_df['pushed_at'] = pd.to_datetime(framework_uses_time_df['pushed_at'])\n",
    "framework_uses_time_df['month'] = [c.month for c in framework_uses_time_df['pushed_at']]\n",
    "framework_uses_time_df['year'] = [c.year for c in framework_uses_time_df['pushed_at']]\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "top_models = [\n",
    "    'linear_model',\n",
    "    'ensemble',\n",
    "    'tree',\n",
    "    'svm',\n",
    "    'neighbors',\n",
    "    'decomposition',\n",
    "    'cluster'\n",
    "]\n",
    "\n",
    "for m in top_models:\n",
    "    framework_uses_time_df[m] = [\n",
    "        'sklearn' in u and m in [\n",
    "            s.split('.')[1] for s in u['sklearn'] if '.' in s\n",
    "        ]\n",
    "        for u in framework_uses_time_df['uses']\n",
    "    ]\n",
    "    print(m, datetime.datetime.now() - start)\n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_uses_nb_time_df = framework_uses_time_df.groupby(\n",
    "    ['month','year']\n",
    ")[top_models].sum().reset_index().merge(\n",
    "    framework_uses_time_df.groupby(\n",
    "        ['month','year']\n",
    "    )['file'].count().reset_index().rename(\n",
    "        columns = {'file':'total'}\n",
    "    )\n",
    ").sort_values(['year','month'])\n",
    "\n",
    "framework_uses_nb_time_df['order'] = list(range(len(framework_uses_nb_time_df)))\n",
    "framework_uses_nb_time_df['label'] = [\n",
    "        datetime.date(int(row['year']), int(row['month']), 1).strftime('%b %Y') \n",
    "        if row['month'] == 1 else ''\n",
    "        for _, row in framework_uses_nb_time_df.iterrows()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualizations and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_uses = sum([sum([f.startswith('sklearn') \n",
    "                    for f in framework]) \n",
    "               for framework in framework_uses_df.uses])\n",
    "tf_uses = sum([sum([f.startswith('tensorflow') \n",
    "                    for f in framework]) \n",
    "               for framework in framework_uses_df.uses])\n",
    "keras_uses = sum([sum([f.startswith('keras') \n",
    "                    for f in framework]) \n",
    "               for framework in framework_uses_df.uses])\n",
    "\n",
    "all_uses = sk_uses + tf_uses + keras_uses\n",
    "\n",
    "print(\"There are: \\n{0} ({1}%) notebooks using SciKitLearn \\n{4} ({5}%) notebooks using Keras \\n{2} ({3}%) notebooks using Tensorflow\".format(\n",
    "    sk_uses, round(100*sk_uses/all_uses, 2), tf_uses, round(100*tf_uses/all_uses, 2), keras_uses, round(100*keras_uses/all_uses, 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKitLearn is the most popular machine learning framework for Jupyter notebooks, as we saw in exploring [frameworks](Frameworks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing supervised and unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on SciKitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKitLearn has good documentation of all available models, making it easy to distinguish what model a user is deploying. On the other hand, Keras and Tensorflow have *parts* of models, called layers, that users can use to build different types of models. Keras is primarily for neural networks, and neural networks are primarily supervised learning. However, there are instances of using keras for unsupervised learning, and its very difficult to parse that information from the code itself. Tensorflow is even more difficult because it can use lots of different APIs (for instance, tensorflow can actually use keras). For these reasons, and because the majority of machine learning uses are SciKitLearn, I have decided to focus on SciKitLearn models for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [sklearn supervised](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- [sklearn unsupervised](https://scikit-learn.org/stable/unsupervised_learning.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "x = [' '.join(m.split('.')[1].title().split('_')) for m in all_s_counts['model']]\n",
    "x_pos = np.arange(len(x))\n",
    "height = all_s_counts['count']\n",
    "ax0 = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
    "ax0.bar(x_pos, height, color = 'teal')\n",
    "plt.title('Supervised')\n",
    "plt.xticks(x_pos, x, rotation = 70, ha = 'right')\n",
    "plt.ylim(0, 400000)\n",
    "plt.yticks(range(0, 400001, 50000), ['{:,}'.format(i) for i in range(0, 400001, 50000)])\n",
    "plt.ylabel('Number of Notebooks')\n",
    "\n",
    "\n",
    "\n",
    "ax1 = plt.subplot2grid((1, 3), (0, 2))\n",
    "x = [' '.join(m.split('.')[1].title().split('_')) for m in all_u_counts['model']]\n",
    "x_pos = np.arange(len(x))\n",
    "height = all_u_counts['count']\n",
    "ax1.bar(x_pos, height, color = 'teal')\n",
    "plt.title('Unsupervised')\n",
    "plt.xticks(x_pos, x, rotation = 70, ha = 'right')\n",
    "plt.ylim(0, 400000)\n",
    "plt.yticks(range(0, 400001, 50000), ['' for i in range(0, 400001, 50000)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is much more common than unsupervised learning. I'll go over the contents of the most common model groups. For a full list, view the sklearn [documentation](https://scikit-learn.org/stable/supervised_learning.html). \n",
    "\n",
    "Within supervised learning, linear models are the most common. This includes regular least squares regression, ridge and lasso regression, elastic net, logistic regression, stochastic gradient descent.\n",
    "\n",
    "Closely following linear models are ensemble methods, which includes bagging, random forests, adaboost, gradient tree boosting, voting classifiers, and voting regressors.\n",
    "\n",
    "Within unsupervised learning, decomposition is the most popular. This includes PCA, singular value decomposition, dictionary learning, factor analysis, independent component analysis, non-negative matrix factorization, and latent dirichlet allocation.\n",
    "\n",
    "Following decomposition is clustering, which incorperates k means, mean shift, hierarchical, dbscan, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are some sklearn models likely to be used in the same notebooks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = framework_uses_df[set(load_data.flatten(framework_uses_df.sklearn_models))].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (7,7))\n",
    "plt.pcolor(corr, cmap = 'Blues')\n",
    "plt.yticks(np.arange(0.5, len(corr.index), 1), [c.split('.')[1] for c in corr.index])\n",
    "plt.xticks(np.arange(0.5, len(corr.columns), 1), [c.split('.')[1] for c in corr.columns], rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strongest association is between tree and ensemble (r = 0.39), which makes sense as these can be used together. The next strongest is linear model and ensemble (r = 0.33) followed by svm and ensemble (r = 0.29), svm and neighbors (r = 0.28), and tree and neigbors (r = 0.27)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model type over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, since = 2012, color = None):\n",
    "    df = framework_uses_nb_time_df[framework_uses_nb_time_df.year >= since]\n",
    "    x_pos = df.order\n",
    "    x = df.label\n",
    "    y = df[model] / df.total\n",
    "\n",
    "    if color != None:\n",
    "        plt.plot(x_pos, y, label = model.title(), color = color)\n",
    "    else:\n",
    "        plt.plot(x_pos, y, label = model.title())\n",
    "    plt.xticks(x_pos, x, rotation = 70)\n",
    "    plt.title('Use of Scikit-Learn {0}'.format(' '.join(\n",
    "        model.title().split('_')\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model('linear_model', since = 2014, color= 'teal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\n",
    "    'linear_model',\n",
    "    'ensemble',\n",
    "    'tree',\n",
    "    'svm',\n",
    "    'neighbors'\n",
    "]:\n",
    "    plot_model(m, since = 2014)\n",
    "    \n",
    "plt.title('Use of Scikit-Learn')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ML Framework Use &lt;](Frameworks.ipynb) | [&gt; Visualizations](Visualizations.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
