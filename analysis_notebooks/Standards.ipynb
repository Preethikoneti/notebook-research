{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use of Special Features &lt;](Magic.ipynb)| [&gt; Data Sceince Workflow](Time.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Standards\n",
    "\n",
    "In this notebook, I am looking at if notebooks follow the intention of the creators of Project Jupyter. That is, whether the notebook:\n",
    "- is a computational narrative\n",
    "- is collaborative\n",
    "- is reproducible\n",
    "\n",
    "## Results Summary:\n",
    "- 6.82% of notebooks attempt to access a file with a full path.\n",
    "- 1.8% of notebooks are still untitled.\n",
    "- There is no apperant association between the contents of a notebook and the attention its repostiory recieves.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import load_data\n",
    "import datetime\n",
    "import re\n",
    "import scipy.stats as st\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebooks loaded in 0:00:23.740910\n",
      "Repos loaded in 0:00:04.058064\n"
     ]
    }
   ],
   "source": [
    "notebooks = load_data.load_notebooks()\n",
    "repos = load_data.load_repos()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell types loaded in 0:00:04.968306\n",
      "Cell order loaded in 0:00:39.087439\n",
      "Statuses loaded in 0:00:01.013576\n",
      "Errors loaded in 0:00:11.210184\n",
      "Educational status loaded in 0:00:00.002591\n",
      "Collaboration statuses loaded in 0:00:00.028552\n",
      "Code loaded in 0:01:05.415815\n",
      "Notebook imports loaded in 0:01:06.929515\n",
      "Cell stats loaded in 0:00:01.834526\n",
      "Comments loaded in 0:00:01.207842\n"
     ]
    }
   ],
   "source": [
    "cell_types_df = load_data.load_cell_types()\n",
    "cell_order_df = load_data.load_cell_order()\n",
    "statuses_df = load_data.load_statuses()\n",
    "errors_df = load_data.load_errors()\n",
    "edu_statuses_df = load_data.load_edu_status()\n",
    "collab_status_df = load_data.load_collab_status()\n",
    "code_df = load_data.load_code()\n",
    "nb_imports = load_data.load_nb_imports()\n",
    "cell_stats_df = load_data.load_cell_stats()\n",
    "comments_df = load_data.load_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Manipulate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables we already have:\n",
    "- cells run in_order\n",
    "- function definitions in order\n",
    "- imports in order\n",
    "- variable definitions in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does a notebook attempt to access a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths(code):\n",
    "    return (list(re.findall('/Users/[a-zA-Z_/./]+', str(code))) +      # Mac\n",
    "           list(re.findall('C:/[a-zA-Z_/./]+', str(code))) +           # Windows\n",
    "           list(re.findall('~/[a-zA-Z_/./]+', str(code))) +            # Linux/Mac\n",
    "           list(re.findall('/home/[a-zA-Z_/./]+', str(code))))         # Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_paths = pd.Series([\n",
    "    find_paths('\\n'.join([i for i in c if type(i) == str])) \n",
    "    for c in code_df.code\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df['full_path'] = [len(l) > 0 for l in local_paths]\n",
    "if 'full_path' not in notebooks.columns:\n",
    "    notebooks = notebooks.merge(code_df[['file','full_path']], on = 'file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add num errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df['num_errors'] = [len(e) for e in errors_df.error_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Narrative\n",
    "\n",
    "Variables we already have:\n",
    "- number of comments\n",
    "\n",
    "Is a notebook named well? If an owner kept the default 'Untitled123.ipynb' name, it's not very descriptive of the notebook's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks['good_name'] = [\n",
    "    not str(n).lower().startswith('untitled') \n",
    "    for n in notebooks.name\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add whether the repository has a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos['has_desc'] = [1 if d == True else 0 for d in  ~repos.repo_description.isna().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words markdown to lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_stats_df['ratio_wl'] = cell_stats_df['num_words']/cell_stats_df['lines_of_code']\n",
    "cell_stats_df = cell_stats_df[cell_stats_df.ratio_wl < math.inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables we already have:\n",
    "- number of cells\n",
    "\n",
    "Add imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_imports['num_imports'] = [len(im) for im in nb_imports.imports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['pandas', 'numpy', 'sklearn', 'tensorflow', 'keras', 'matplotlib', 'seaborn']:\n",
    "    nb_imports[p] = [p in [i[0] for i in im] for im in nb_imports.imports]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all \"Standards\" Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_features = ['subscribers_count','watchers_count','forks_count','open_issues_count']\n",
    "\n",
    "standard_df = notebooks\\\n",
    "    .merge(collab_status_df.rename(columns={'status':'collab'}), on = 'repo_id')\\\n",
    "    .merge(errors_df, on = 'file')\\\n",
    "    .merge(cell_types_df, on = 'file')\\\n",
    "    .merge(cell_order_df[['file','in_order']], on = 'file')\\\n",
    "    .merge(statuses_df, on = 'file')\\\n",
    "    .merge(repos[['repo_id','has_desc'] + attention_features], on = 'repo_id')\\\n",
    "    .merge(nb_imports[['file','num_imports', 'pandas', 'numpy', \n",
    "                       'sklearn', 'tensorflow', 'keras', \n",
    "                       'matplotlib', 'seaborn']], on = 'file')\\\n",
    "    .merge(cell_stats_df[['file','ratio_wl']], on = 'file')\\\n",
    "    .merge(comments_df, on = 'file')[[\n",
    "        'file','repo_id','collab','num_errors','good_name','ratio_wl', 'num_comments',\n",
    "        'in_order','function','import','variable','syntax', 'full_path',\n",
    "        'has_desc','num_cells','num_imports', 'pandas', 'numpy', 'sklearn', \n",
    "        'tensorflow', 'keras', 'matplotlib', 'seaborn'\n",
    "    ] + attention_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df = standard_df[standard_df.num_cells > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Visualizations & Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, there are 18.47 comments per notebook (1.0 comments per cell).\n"
     ]
    }
   ],
   "source": [
    "print(\"On average, there are {0} comments per notebook ({1} comments per cell).\".format(\n",
    "    round(standard_df['num_comments'].mean(), 2),\n",
    "    round((standard_df['num_comments']/standard_df['num_cells']).mean(), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown to Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, there are 6.36 words of markdown for each line of code.\n"
     ]
    }
   ],
   "source": [
    "print(\"On average, there are {0} words of markdown for each line of code.\".format(\n",
    "    round(standard_df['ratio_wl'].mean(), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99% of notebooks were not renamed from their default 'Untitled' name.\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}% of notebooks were not renamed from their default 'Untitled' name.\".format(\n",
    "    round(100*sum(standard_df.good_name == False)/len(standard_df), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.82% of notebooks attempt to access a file with a full local path.\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}% of notebooks attempt to access a file with a full local path.\".format(\n",
    "    round(100*len(local_paths[[i!=[] for i in local_paths]])/len(local_paths), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can a notebook be run top to bottom?\n",
    "\n",
    "Variables related to reproducibility are:\n",
    "- function and variable definition order\n",
    "- package import order\n",
    "- cell execution order\n",
    "- whether the notebook attempts to access a file with a full path\n",
    "- errors\n",
    "\n",
    "Hypothetically, if everything is in order, there are no errors, and no full path is used, a notebook should be able to run top to bottom without having to edit or rearrange any code. What proportion of notebooks are reproducible, given this definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.85% of notebooks could hypothetically be run through top to bottom.\n"
     ]
    }
   ],
   "source": [
    "cant_run_through = (\n",
    "    standard_df['function'] +            # if functions defined in order, function = 0\n",
    "    standard_df['import'] +              # if packages imported in order, import = 0\n",
    "    standard_df['variable'] +            # if vairables defined in order, variable = 0\n",
    "    standard_df['full_path'] +           # if full path is not used, full_path = 0\n",
    "    standard_df['num_errors'] +          # we want num_errors = 0\n",
    "    (standard_df['in_order'] == False)   # if cells run in order, in_order = True\n",
    ")\n",
    "print(\"{0}% of notebooks could hypothetically be run through top to bottom.\".format(\n",
    "    round(100 - 100*sum(cant_run_through)/len(cant_run_through), 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the attention of a repository be predicted by the contents of a notebook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter standards dataframe\n",
    "Adjust variables by length of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df['num_errors_per_cell'] = standard_df['num_errors'] / standard_df['num_cells']\n",
    "standard_df['num_imports_per_cell'] = standard_df['num_imports'] / standard_df['num_cells']\n",
    "standard_df['num_comments_per_cell'] = standard_df['num_comments'] / standard_df['num_cells']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = ['good_name','in_order','full_path','has_desc', \n",
    "              'pandas', 'numpy', 'sklearn', 'tensorflow', \n",
    "              'keras', 'matplotlib', 'seaborn']\n",
    "for var in indicators:\n",
    "    standard_df[var] = [1 if s else 0 for s in standard_df[var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in standard_df['collab'].unique():\n",
    "    standard_df['collab_'+level] = [1 if c==level else 0 for c in standard_df['collab']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop notebooks with no lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df = standard_df[standard_df.ratio_wl < math.inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all numericalcolumns with Z score\n",
    "num_features = [\n",
    "    'num_errors_per_cell',\n",
    "    'ratio_wl','num_comments_per_cell',\n",
    "    'num_imports_per_cell'\n",
    "]\n",
    "for col in num_features:\n",
    "    standard_df[col] = (standard_df[col] - standard_df[col].mean()) / standard_df[col].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention metric 1: product of four related measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = [1]*len(repos)\n",
    "for f in attention_features:\n",
    "    repos[f] = (repos[f] - repos[f].mean()) / repos[f].std()\n",
    "    attention *= repos[f]\n",
    "repos['attention'] = attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'attention' not in standard_df.columns:\n",
    "    standard_df = standard_df.merge(\n",
    "        repos[['repo_id','attention']], on = 'repo_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention metric 2: first principal component of four related measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.52% of the variance in attention is explained by the first principal component.\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 1)\n",
    "attention_pca = pca.fit_transform(repos[attention_features])\n",
    "print(\"{0}% of the variance in attention is explained by the first principal component.\".format(\n",
    "    round(100*pca.explained_variance_ratio_[0], 2)\n",
    "))\n",
    "\n",
    "repos['attention_pca'] = attention_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribers_count : 0.5384214608254829\n",
      "watchers_count : 0.560261328120955\n",
      "forks_count : 0.5494918902409348\n",
      "open_issues_count : 0.3070313294993551\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(attention_features[i],':',pca.components_[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important factor for describing attention is watchers count. Watchers, subscribers, and forks have similar importances (coefficient around 0.55), but issues is much less important with a coefficient at 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'attention_pca' not in standard_df.columns:\n",
    "    standard_df = standard_df.merge(\n",
    "        repos[['repo_id','attention_pca']], on = 'repo_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention metric 3: sum of four related measures\n",
    "approximate measure of the number of views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_sum = [0]*len(repos)\n",
    "for f in attention_features:\n",
    "    attention_sum += repos[f]\n",
    "repos['attention_sum'] = attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'attention_sum' not in standard_df.columns:\n",
    "    standard_df = standard_df.merge(\n",
    "        repos[['repo_id','attention_sum']], on = 'repo_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'num_errors_per_cell','good_name','ratio_wl', 'num_comments_per_cell',\n",
    "    'in_order','function','import','variable', 'full_path',\n",
    "    'num_cells', 'pandas', 'numpy', 'sklearn', \n",
    "    'tensorflow', 'keras', 'matplotlib', 'seaborn'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are attention features associated with narrative, reproducibility, and content\n",
    "- $H_o$: all features are dependent on eachother\n",
    "- $H_a$: attention features are independent of narrative, reproducibility, and content features. \n",
    "\n",
    "Under alternative hypothesis\n",
    "$\\Sigma$ = $\\Sigma_a$ = \n",
    "\n",
    "\n",
    "\n",
    "[$\\Sigma_{attention}$, 0]\n",
    "\n",
    "[0, $\\Sigma_{other\\_features}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998571210589533"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(standard_df[features + attention_features].corr()) / (\n",
    "    np.linalg.det(standard_df[features].corr()) *\n",
    "    np.linalg.det(standard_df[attention_features].corr())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null alternative hypothesis, $\\Sigma$ = $\\Sigma_a$ so we expect the U statistic, the ratio of their determinants, to equal 1. The U statistic here of 0.999 is very close to 1, so provides strong evidence that the attention features are independent of narrative, reproducibility, and content features. This means it's unlikely we will be able to predict attention.\n",
    "\n",
    "This was a surprising finding because in [Collaboration.ipynb](Collaboration.ipynb) I found that collaborative notebooks tend to have a higher markdown to code ratio and are more likely to have descriptions. However, this was differentiating between Collaborative (at least one fork or issue), Watched (at least one stargazer or watcher), and Isolated (no views). The lack of association here might be because of how spread out the data is. Some collaborative repositories have only one fork, while others have thousands. In [Collaboration.ipynb](Collaboration.ipynb), I lumped all of those repositories together, but here I *want* to distinguish between a little bit of attention and a lot of attention (e.g. one versus thousands of forks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = standard_df[features]\n",
    "x_2 = standard_df[features]\n",
    "x_3 = standard_df[features]\n",
    "\n",
    "y_1 = standard_df['attention']\n",
    "y_2 = standard_df['attention_pca']\n",
    "y_3 = standard_df['attention_sum']\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(x_1, y_1, test_size=0.2, random_state=123)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(x_2, y_2, test_size=0.2, random_state=123)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(x_3, y_3, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention measure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 2.0933491652330716e-06\n",
      "Test R^2: 1.589981419858333e-07\n"
     ]
    }
   ],
   "source": [
    "reg1 = LinearRegression().fit(X1_train, y1_train)\n",
    "print('Train R^2:',reg1.score(X1_train, y1_train))\n",
    "print('Test R^2:',reg1.score(X1_test, y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention measure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: -531115747.19483113\n",
      "Test R^2: -314387799.64059305\n"
     ]
    }
   ],
   "source": [
    "re2 = LinearRegression().fit(X2_train, y2_train)\n",
    "print('Train R^2:',reg1.score(X2_train, y2_train))\n",
    "print('Test R^2:',reg1.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention measure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 2.0933491652330716e-06\n",
      "Test R^2: 1.589981419858333e-07\n"
     ]
    }
   ],
   "source": [
    "re3 = LinearRegression().fit(X3_train, y3_train)\n",
    "print('Train R^2:',reg1.score(X3_train, y3_train))\n",
    "print('Test R^2:',reg1.score(X3_test, y3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the linear regression attempts supports the conclusion that attention is independent of the content of the notebook. All $R^2$ values are extremely low, even on the training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use of Special Features &lt;](Magic.ipynb)| [&gt; Data Sceince Workflow](Time.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
